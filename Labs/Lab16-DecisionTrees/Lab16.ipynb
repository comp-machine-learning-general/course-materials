{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 16\n",
    "\n",
    "Today, we will continue our journey through the classification task within supervised learning. Today we will explore some of the ingredients of decision trees before learning how to implement them in `sklearn`. Today's goals are: \n",
    "\n",
    "0. Explain why SVMs and decision trees are different approaches to classification \n",
    "1. Develop intuition for how to build decision trees\n",
    "2. Learn how to build and fit decision trees in `sklearn`\n",
    "\n",
    "\n",
    "![A comedic decision tree on deciding whether to major in cs.](csmajor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports for Today\n",
    "\n",
    "We begin as usual, importing the packages and data that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import block\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "# from sklearn.externals.six import StringIO \n",
    "from IPython.display import Image \n",
    "from pydot import graph_from_dot_data\n",
    "from six import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a conda package!\n",
    "\n",
    "If you got a (non-deprecation) error, you may need to exit jupyter and add `pydot` to your conda environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "We define some premade helper functions that will use to build your decision trees. These functions are heavily adapted from [\"Decision Tree Algorithm in Python From Scratch\"](https://towardsdatascience.com/decision-tree-algorithm-in-python-from-scratch-8c43f0e40173) by Eligijus Bujokas on TowardsDataScience.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index(data_pd: pd.DataFrame, class_var: str) -> float:\n",
    "    \"\"\"\n",
    "    Given the observations of a binary class and the name of the binary class column\n",
    "    calculate the gini index\n",
    "    \"\"\"\n",
    "    # count classes 0 and 1\n",
    "    count_A = np.sum(data_pd[class_var] == 0)\n",
    "    count_B = np.sum(data_pd[class_var])\n",
    "\n",
    "    # get the total observations\n",
    "    n = count_A + count_B\n",
    "\n",
    "    # If n is 0 then we return the lowest possible gini impurity\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Getting the probability to see each of the classes\n",
    "    p1 = count_A / n\n",
    "    p2 = count_B / n\n",
    "\n",
    "    # Calculating gini\n",
    "    gini = 1 - (p1 ** 2 + p2 ** 2)\n",
    "\n",
    "    # Returning the gini impurity\n",
    "    return gini\n",
    "\n",
    "def info_gain(data_pd: pd.DataFrame, class_var: str, feature: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates how much info we gain from a split compared to info at the current node\n",
    "    \"\"\"\n",
    "    # compute the base gini impurity (at the current node)\n",
    "    gini_base = gini_index(data_pd, class_var)\n",
    "\n",
    "    # split on the feature\n",
    "    node_left, node_right = split_bool(data_pd, feature)\n",
    "\n",
    "    # count datapoints in each split and the whole dataset\n",
    "    n_left = node_left.shape[0]\n",
    "    n_right = node_left.shape[0]\n",
    "    n = n_left + n_right\n",
    "\n",
    "    # get left and right gini index\n",
    "    gini_left = gini_index(node_left, class_var)\n",
    "    gini_right = gini_index(node_right, class_var)\n",
    "\n",
    "    # calculate weight for each node\n",
    "    # according to proportion of data it contains from parent node\n",
    "    w_left = n_left / n\n",
    "    w_right = n_right / n\n",
    "\n",
    "    # calculated weighted gini index\n",
    "    w_gini = w_left * gini_left + w_right * gini_right\n",
    "\n",
    "    # calculate the gain of this split\n",
    "    gini_gain = gini_base - w_gini\n",
    "\n",
    "    # return the best feature\n",
    "    return gini_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For function testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Today's data\n",
    "\n",
    "Today's data is adapted from a [dog breed trait dataset](https://www.kaggle.com/datasets/yonkotoshiro/dogs-breeds) posted by Al Chernyshev on Kaggle, as web scraped from breed profiles on https://dogtime.com. Each row represents a breed or breed mix. The dataset contains binary, numerical, and categorical data. For the first part of the lab, we use a subset of the available features to work with only binary variables.\n",
    "\n",
    "Our goal is to classify dogs as \"Good\" or \"Not Good\" for Novice Owners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Data\n",
    "dog_pd = pd.read_csv(\"lab16data-binary.csv\", sep = \",\", index_col = \"Breed Name\")\n",
    "\n",
    "print(dog_pd.shape)\n",
    "\n",
    "dog_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From SVMs to Decision Trees\n",
    "\n",
    "In our discussion of SVMs, we did not attempt to interpret nor to explain _why_ one data point gets labeled. In SVM, we have a boundary and we might be able to twist ourselves into an \"explanation.\" However, we may want a transparent set of questions that lead us to our classifications.\n",
    "\n",
    "* In SVM, we could say a dog is in group A because it lies above the line `class = 4 + 3*Easy-to-Train - 7*High-Energy`.\n",
    "* In Decision Trees, we would say that this dog is in group A because it is easy to train and has high energy.\n",
    "\n",
    "In the SVM example, it can be hard to interpret to the average person \"above the line\" with some equation. While in comparison, the answering of yes/no questions for the decision tree provides a straight-forward way to understand *why* a dog is or is not in one group. This is the intuition behind **decision trees.**\n",
    "\n",
    "Decision trees are trees where each **branch** represents a decision based on a specific variable. Consider the following situation: We're a novice dog owner who wants to get a dog. We decide to classify dog breeds as \"good\" or \"not good\" for novice owners.\n",
    "\n",
    "Our goal is to build something like this infographic that tells us how to decide whether or not a dog breed is good for novice owners: \n",
    "\n",
    "![dogBreeds.png](dogBreeds.png)\n",
    "\n",
    "At the top **node,** we represent our first decision. Then based on whether a dog is easy to train, we either examine the dog's energy levels or the dog's kid-friendliness. \n",
    "\n",
    "The bottom nodes of a decision tree are called **leaves.** Each leaf is the group of data points that followed the series of decisions encoded in that branch. (A _leaf_ is a node with no branches under it.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Best Way to Split the Data\n",
    "\n",
    "The central question of decision trees is: \"What decision will give me the best split of my data?\" To address this question we need to 1) understand how to split the data, and 2) how to evaluate the \"goodness\" of a split. \n",
    "\n",
    "In our example about dogs, currently, we don't know whether a dog's size, energy, or some other characteristic will help us best sort dogs into \"good\" or \"not good\" for novice owners.\n",
    "\n",
    "We start by trying to split the data into two groups based on one variable, then repeat this decision for each other variable in the dataset.\n",
    "\n",
    "Let's start by splitting our dog breeds based on whether they are \"Easy to Train\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe that \"Easy to Train\" is a boolean variable - either True or False\n",
    "dog_pd['Easy To Train'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split our data into two dataframes: easy/not easy to train\n",
    "easy_to_train = dog_pd[dog_pd['Easy To Train']]\n",
    "hard_to_train = dog_pd[~dog_pd['Easy To Train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check our output shapes\n",
    "print(dog_pd.shape)\n",
    "print(easy_to_train.???)\n",
    "print(???.???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our later convenience, let's functionalize the process of splitting a dataframe based on a binary variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split on a boolean/binary variable\n",
    "## should look very similar to your code in the block above\n",
    "def split_bool(data_pd, column_name):\n",
    "    \"\"\"Returns two pandas dataframes:\n",
    "    one where the specified variable is true,\n",
    "    and the other where the specified variable is false\"\"\"\n",
    "    node_left = data_pd[data_pd[???]]\n",
    "    node_right = ???\n",
    "    \n",
    "    return node_left, node_right\n",
    "\n",
    "easy_to_train, hard_to_train = split_bool(dog_pd, 'Easy To Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For function testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check our output shapes that use the above function \n",
    "# Do they match the above?\n",
    "\n",
    "print(dog_pd.shape)\n",
    "print(easy_to_train.shape)\n",
    "print(hard_to_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another example, let's repeat our split on the variable \"Kid-Friendly\" to split dog breeds by whether or not they are good with kids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kid_friendly, not_kid_friendly = split_bool(???, ???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check our output shapes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we define the \"Best\" Split?\n",
    "\n",
    "We need a way to measure whether \"Easy to Train\" or \"Kid-Friendly\" is better for splitting up dogs into groups of \"Good\" or \"Not Good\" for Novice Owners.\n",
    "\n",
    "***Note:*** *In this section, we will use the term \"purity\" as it is term that is often used in the literature. We—your instructor and pedagogical partner—do not like this term, but feel that you should hear it due to its existence in the decision tree space.*\n",
    "\n",
    "In an ideal scenario, splitting on one of these variables would let us send all the dogs who are \"Good\" for Novice Owners to one leaf of the tree, and send all the dogs who are \"Not Good\" for Novice Owners to the other leaf of the tree. But in our imperfect world, we'll usually end up with *impure leaves*—leaves which are not perfectly separated by our target classification. We aim to quantify the \"purity\", also known as the \"homogeneity\", of each node on our tree.\n",
    "\n",
    "![A visual diagram showing the spectrum from most pure to least pure.](homogeneity.png)\n",
    "\n",
    "There are multiple ways to measure the homogeneity of a node, including:\n",
    "* Gini Impurity Index\n",
    "* Entropy\n",
    "* Missclassification\n",
    "\n",
    "In the helper functions at the top of the lab, we have an implementation of the Gini Impurity Index. For a given leaf's group of data, `gini_leaf()` returns a number between 0 and 1, where 1 means the data is completely homogeneous (all the same class) and 0 means the data as not homogeneous at all (there's the same amount of data of both classes at the node)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find gini impurity index for each of our split groups\n",
    "## syntax: gini_index(leaf_data, class_var)\n",
    "easy_to_train_gini = gini_index(easy_to_train, 'Good For Novice Owners')\n",
    "hard_to_train_gini = gini_index(hard_to_train, ???)\n",
    "\n",
    "kid_friendly_gini = gini_index(???, ???)\n",
    "not_kid_friendly_gini = gini_index(???, ???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print our gini values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Among the \"Easy to Train\" and \"Kid-Friendly\" variables, do one decision's nodes seem to have lower Gini Impurities than the other decision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Thoughts Here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our chosen metric of homogeneity, information gain (`info_gain()`) quantifies how much we learn from a given decision. (We learn more if we can split the data into more homogenous nodes.) If you want to read more about the calculations behind the Gini Index and Information Gain, see the resources at the end of the lab.\n",
    "\n",
    "For now, we'll run with applying `info_gain()` to our decisions, and we'll split on the variable that lets us gain the most information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply info_gain to each split\n",
    "## syntax is: info_gain(data_decide, decision_var, class_var)\n",
    "info_easy_to_train = info_gain(dog_pd, 'Easy To Train', 'Good For Novice Owners')\n",
    "info_kid_friendly = info_gain(dog_pd, ???, 'Good For Novice Owners')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print our info gain values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Among \"Easy To Train\" and \"Kid-Friendly\", which variable gives us more information for classifying dog breeds as \"Good for Novice Owners\"? Does this correspond with the node which had lower Gini Impurities above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Thoughts Here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable whose split gives us the most information is the best first split in our decision tree. In other words, splitting on this variable is the best first decision at the top of our decision tree.\n",
    "\n",
    "Let's build a function which tests all possible features to split on and returns the best one.\n",
    "\n",
    "### A Note on Types in Python:\n",
    "\n",
    "The functions in this lab specify input and output types in the function declarations. The function declaration  \n",
    "`def best_split(data_pd: pd.DataFrame, class_var: str) -> float:`  \n",
    "is equivalent to  \n",
    "`def best_split(data_pd, class_var):`  \n",
    "but gives us the additional information that `data_pd` is a Pandas DataFrame, `class_var` is a string, and the return from the function is a float.\n",
    "\n",
    "***We do not expect you to code with types in Python for this course,*** but this lab aims to give you some exposure in case you encounter them in a future context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_split(data_pd: pd.DataFrame, class_var: str, exclude_features: list = []) -> float:\n",
    "    \"\"\"\n",
    "    Returns the name of the best feature to split on at this node.\n",
    "    If the current node contains the most info (all splits lose information), return None.\n",
    "    \"\"\"\n",
    "    # compute the base gini index (at the current node)\n",
    "    gini_base = gini_index(data_pd, class_var)\n",
    "\n",
    "    # initialize max_gain and best_feature\n",
    "    max_gain = 0\n",
    "    best_feature = None\n",
    "\n",
    "    # create list of features of data_pd not including class_var\n",
    "    features = ???\n",
    "    \n",
    "    # will be useful later - can skip for now\n",
    "    # remove features we're excluding\n",
    "    # (already made decision on this feature)\n",
    "    features = [f for f in features if f not in exclude_features]\n",
    "\n",
    "    # test a split on each feature\n",
    "    for ??? in ???:\n",
    "        info = info_gain(???, ???, ???)\n",
    "\n",
    "        # check whether this is the greatest gain we've seen so far\n",
    "        # and thus the best split we've seen so far\n",
    "        if ??? > max_gain:\n",
    "            best_feature = feature\n",
    "            max_gain = info\n",
    "\n",
    "    # return the best feature\n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block for testing your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_split(dog_pd, \"Good For Novice Owners\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for the best split if we exclude 'Kid-Friendly'\n",
    "best_split(dog_pd, \"Good For Novice Owners\", exclude_features=['Kid-Friendly'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Note on Splitting Non-Binary Data\n",
    "\n",
    "Very few real data sets contain only binary variables. Whether a variable is categorical, ordinal, or numerical, the presence of more than two possible values of the variable in the data set means that there are multiple ways to split the data set into two groups.\n",
    "\n",
    "For example:\n",
    "* For a categorical variable with levels ABC, there are three potential ways to split the data into two groups: AB|C, AC|B, or BC|A.\n",
    "* For an ordinal variable with rankings 1-5, there are four potential decision boundaries: var <= 1, 2, 3, or 4 (Note that <= 5 is not a valid boundary, as the condition will be true for all data and thus we divide the data into only one group.)\n",
    "* For a numerical variable with values 2.3, 2.6, 3.1, 3.7, and 4.1, we can form decision boundaries between each value of the data similar to the ordinal variable above, or based on moving averages of subsets of the data.\n",
    "\n",
    "Note that the number of possible decisions on a variable grows rapidly as the number of distinct values of the variable increase.\n",
    "\n",
    "We will not code decisions on non-binary data from scratch, but it’s important to remember that when we talk about finding the “best possible decision” at each level of a decision tree, we’re often considering multiple decisions for each variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question (optional, but recommended):**  \n",
    "If a dataset contains n variables, we’ve established that we have more than n options to consider for the “best possible decision.” In the worst case, what is the order of decisions we need to consider?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Thoughts Here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Tree of Decisions\n",
    "After we decide on the best first decision in our tree, we have two nodes of data. We consider each node individually, and repeat the whole process again to see which of the remaining potential decisions is the next best split for that node. Often, the next best decision is different on each side of the tree.\n",
    "\n",
    "In other words, we take the nodes that result from our first decision, and we recursively build a decision tree from each node.\n",
    "\n",
    "**Question:** When we have a recursive process, we need both the recursive step and a base case (aka stopping condition). What are potential base cases for building a decision tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Thoughts Here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start writing the **pseudocode** for a recursive decision tree algorithm. Our goal at each node is to:\n",
    "\n",
    "0. (Stop if we reach the base case)\n",
    "1. Determine which split/decision gives us the most information\n",
    "2. Make the split on that variable\n",
    "3. Continue recursively on each of the resulting two nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initial decision tree algorithm PSEUDOCODE\n",
    "## this doesn't need to run\n",
    "## just sketch out how you would follow the steps above\n",
    "\n",
    "# you may need to pass additional parameters to this function call\n",
    "# depending on your stopping condition\n",
    "def decision_tree(node_data: pd.DataFrame, class_var: str):\n",
    "    # 0. stop at the base case\n",
    "    \n",
    "    # 1. determine which decision gives us the most information\n",
    "    \n",
    "    # 2. make the split according to the best decision\n",
    "    \n",
    "    # 3. continue recursively on each of the resulting two nodes\n",
    "    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Stopping Conditions\n",
    "Common stopping conditions for a decision tree algorithm include, but are not limited to:\n",
    "\n",
    "* When the tree has reached *n* levels of depth\n",
    "* When none of the remaining possible decisions/splits at a node provide more information than the info we already have at the node\n",
    "* When we run out of potential decisions\\*\\*\n",
    "* When our splits result in leaves smaller than size *n*\n",
    "\n",
    "\\*\\*Note that for binary data, we run out of potential decisions when we've split on every variable. For non-binary data, we could make multiple splits on a given variable between different levels of the numbers or among different combinations of categories—thus, we have many, many possible decisions to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Why would it be valuable to have shorter trees (Trees with depth <= *n*)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Thoughts Here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Hint: Think about overfitting...)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Decision Tree\n",
    "Now, let's turn your decision tree into working code. We'll be implementing a simplified version of the decision tree in [this post](https://towardsdatascience.com/decision-tree-algorithm-in-python-from-scratch-8c43f0e40173). For the scope of this lab, we'll just build the structure of the decision tree—we will not add functionality to produce a classification from the tree. See the source post for an example of how to classify observations using your decision tree.\n",
    "\n",
    "The outline provided is based on a max depth of 2 as a stopping condition. As a challenge, try to add another stopping condition as described in the above section.\n",
    "\n",
    "As a reminder, here is our ***recursive*** decision tree algorithm:\n",
    "\n",
    "0. (Stop if we reach the base case)\n",
    "1. Determine which split/decision gives us the most information\n",
    "2. Make the split on that variable\n",
    "3. Continue recursively on each of the resulting two nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build a decision tree from a node of data\n",
    "def build_decision_tree(node_data: pd.DataFrame, class_var: str, depth: int = 0, exclude_features: list = []) -> None:\n",
    "    # 0. stop at the base case\n",
    "    max_depth = 2\n",
    "    if depth >= max_depth:\n",
    "        return\n",
    "    \n",
    "    # 1. determine which decision gives us the most information\n",
    "    best_feature = ???\n",
    "    print(f\"{'>'*(depth+1)}Splitting {node_data.shape[0]} data points on {best_feature}\")\n",
    "    \n",
    "    # 2a. if best_feature == None, don't split further\n",
    "    if best_feature == None:\n",
    "        print(f\"{'>'*(depth+1)}No best next split.\")\n",
    "        return\n",
    "    \n",
    "    # 2b. else, make the split according to the best decision\n",
    "    else:\n",
    "        data_left, data_right = ???\n",
    "        print(f\"{'>'*(depth+1)}Produces {data_left.shape[0]} True data points and {data_right.shape[0]} False data points\")\n",
    "        \n",
    "        # and exclude this feature at future levels of the tree\n",
    "        exclude_features.append(???)\n",
    "    \n",
    "    # 3. continue recursively on each of the resulting two nodes\n",
    "    build_decision_tree(data_left, class_var, depth + 1, exclude_features)\n",
    "    build_decision_tree(???)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block for testing your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_decision_tree(dog_pd, \"Good For Novice Owners\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Can you describe in words how the tree splits the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Thoughts Here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees in `sklearn`\n",
    "\n",
    "Now that we have some basic intuition about decision trees, let's build a decision tree classifier using `sklearn`. As with other machine learning tools in `sklearn`, we will first specify our decision tree model, and then fit the decision tree model to some data—similar to other `sklearn` classifiers we used before. \n",
    "\n",
    "Since the `sklearn` decision tree implementation can form decisions on non-binary features, let's import our full dog breed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import full dataset\n",
    "dog_full = pd.read_csv(\"lab16data.csv\", sep = \",\", index_col = \"Breed Name\")\n",
    "dog_data = dog_full.to_numpy(dtype = np.float16)\n",
    "\n",
    "print(dog_full.shape)\n",
    "\n",
    "dog_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_data[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into the input variables and the target classes\n",
    "in_dog = dog_data[:,:???]\n",
    "out_class = dog_data[:,???]\n",
    "\n",
    "# Get the variable names \n",
    "var_names = list(dog_full.columns)[:???]\n",
    "print(var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify our model\n",
    "# note: the default for the `criterion` parameter is 'gini' - but 'entropy' is also an option\n",
    "dt = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our model to the data\n",
    "dt.fit(in_dog, out_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Umm... this is not easy to \"look\" at. Referring to [this post](https://towardsdatascience.com/decision-tree-in-python-b433ae57fb93), we adjust their code to create a visual output of our decision tree (double-click the image to zoom in): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "\n",
    "export_graphviz(dt, out_file=dot_data, feature_names=var_names)\n",
    "(dt_vis, ) = graph_from_dot_data(dot_data.getvalue())\n",
    "\n",
    "Image(dt_vis.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** What do you notice about this image? What is encoded? What is left out? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Thoughts Here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To wrap up, we'll predict whether the first five dogs in the dataset are good for novice owners. (This is bad practice to predict from our training data—don't use training data like this when you validate your models.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for the first five dogs\n",
    "dt.predict(in_dog[:5,:???])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next time\n",
    "\n",
    "Next time, we will look at ensemble methods which aggregates several methods together. The specific example that we will explore are **random forests.** Random forests are based on a collection of decision trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Thoughts\n",
    "\n",
    "In reality, every dog is unique and may not conform to the description of its breed. While breed descriptions may be useful guidelines for those hoping to adopt a new dog, it is best for each family to meet the individual dog to get a sense of whether the dog will be a good fit.\n",
    "\n",
    "And remember, regardless of fit for an individual family, every dog is a [good dog](https://www.inverse.com/science/59234-can-dogs-really-truly-understand-us). \n",
    "\n",
    "For this lab, you have two choices for how you finish up this lab. To get credit, you **only** need to do one of the following choices: \n",
    "\n",
    "**Choice 1 - Final Questions:**\n",
    "1. Save the image of your `sklearn` decision tree. \n",
    "2. Choose a dog breed in the dataset.\n",
    "3. Using your `sklearn` image of your decision tree, decide if the breed is good for novice owners. \n",
    "4. Compare your result to the result from the classifier. Are they the same?\n",
    "5. Compare your result and the classifier's result to the original classification in the dataset. Are they correct?\n",
    "6. (Optional) If you have knowledge of this dog breed, would you agree with the result of the classification? Is this dog good for novice owners?\n",
    "\n",
    "**Choice 2 - Final Questions:**\n",
    "1. Explain at least one scenario in which use of this classifier could be potentially harmful.\n",
    "2. Name at least one other scenario where forming decisions via a classifier could be potentially harmful and/or unethical. Explain why.\n",
    "3. What are some considerations you would want to keep in mind when deciding whether use of a classifier is appropriate in a given scenario?\n",
    "\n",
    "Share your thoughts in a post on the **#lab16_submission** channel on slack.\n",
    "\n",
    "If your have questions from this lab, post them to **#lab_questions** with the same preamble (i.e. starting with **Lab16**). If you have the same question, please use one of the emoji's to upvote the question. If you would like to answer someone's question, please use the thread function. This will tie your answer to their question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources consulted \n",
    "\n",
    "0. [Data Source - Dogs Breeds on Kaggle](https://www.kaggle.com/datasets/yonkotoshiro/dogs-breeds)\n",
    "1. [Decision Tree Algorithm in Python From Scratch](https://towardsdatascience.com/decision-tree-algorithm-in-python-from-scratch-8c43f0e40173)\n",
    "1. [SCIKIT-LEARN : DECISION TREE LEARNING I - ENTROPY, GINI, AND INFORMATION GAIN](https://www.bogotobogo.com/python/scikit-learn/scikt_machine_learning_Decision_Tree_Learning_Informatioin_Gain_IG_Impurity_Entropy_Gini_Classification_Error.php)\n",
    "1. [`sklearn` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "1. [`sklearn` further tree information](https://scikit-learn.org/stable/modules/tree.html)\n",
    "1. [Google Developers: Let’s Write a Decision Tree Classifier from Scratch - Machine Learning Recipes #8 (YouTube)](https://www.youtube.com/watch?v=LDRbO9a6XPU)\n",
    "1. [StatQuest: Decision Trees (YouTube)](https://www.youtube.com/watch?v=7VeUPuFGJHk)\n",
    "1. [ISLR](https://hastie.su.domains/ISLR2/ISLRv2_website.pdf)\n",
    "1. [SDS 293 Notes by R. Jordan Crouser](http://www.science.smith.edu/~jcrouser/SDS293/)\n",
    "1. [Decision Tree In Python](https://towardsdatascience.com/decision-tree-in-python-b433ae57fb93)\n",
    "1. [Decision Tree Algorithm With Hands On Example](https://medium.com/datadriveninvestor/decision-tree-algorithm-with-hands-on-example-e6c2afb40d38)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab was created by Kathleen R. Hablutzel, Smith College '23J, as part of a Pedagogical Partnership with Professor Katherine M. Kinnaird in Spring 2022. Parts of the introduction and conclusion to the lab are borrowed from previous versions of Lab 16."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
