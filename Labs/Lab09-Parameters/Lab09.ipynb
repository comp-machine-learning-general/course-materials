{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9\n",
    "\n",
    "Today is a second day of vocabularies. Today we will examine _parameters_ and _hyperparameters_ and acquaint ourselves with the idea of storing parameters as vectors. Today's goals are:\n",
    "\n",
    "0. Compare and contrast parameters and hyper-parameters\n",
    "1. Explore how parameters can be viewed and interacted with as vectors \n",
    "2. Discuss grid search\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we start\n",
    "\n",
    "The first part of this lab revisits all the algorithms and methods that we have explored so far. Let's start by making a list of each method so far, noting:   \n",
    "1) the kind (supervised vs. unsupervised),   \n",
    "2) the inputs needed to use the method (other than data), and   \n",
    "3) any other numbers that are involved or set during that method. \n",
    "\n",
    "We'll use this listing in our following discussions. I'll do the first two:  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Method/Algorithm</th>\n",
    "    <th>Type</th>\n",
    "    <th>Inputs</th>\n",
    "    <th>Numbers set/involved</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<th>k-means</th>\n",
    "    <td>unsupervised </td>\n",
    "    <td>$k$ (the number of clusters)</td>\n",
    "    <td>labels, cluster centers</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<th>kNN</th>\n",
    "    <td>supervised</td>\n",
    "    <td>$k$ (number of labeled neighbors consulted)</td>\n",
    "    <td>labels for the rest of the data </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<th>PCA</th>\n",
    "    <td>Type</td>\n",
    "    <td>Inputs</td>\n",
    "    <td>Numbers set/involved</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<th>SVD</th>\n",
    "    <td>Type</td>\n",
    "    <td>Inputs</td>\n",
    "    <td>Numbers set/involved</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<th>Spectral Clustering</th>\n",
    "    <td>Type</td>\n",
    "    <td>Inputs</td>\n",
    "    <td>Numbers set/involved</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<th>Regression</th>\n",
    "    <td>Type</td>\n",
    "    <td>Inputs</td>\n",
    "    <td>Numbers set/involved</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kinds of inputs do we have? What trends do you notice?\n",
    "\n",
    "(Your thoughts here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Hyper-) Parameters\n",
    "\n",
    "In most processes in machine learning, we are working to fit a model or a process to our data. This fitting involves both the data that we fit to and determining the values of various numbers. These numbers are either _parameters_ or _hyperparameters._ \n",
    "\n",
    "### Quick Definitions - \n",
    "\n",
    "* **Hyper-parameters** are numbers that are set by the researcher. These can be set without knowing anything about the data that we are fitting to.    \n",
    "* **Parameters** are numbers whose values are dictated by the data. \n",
    "\n",
    "With these quick definitions, look back at your list from before and determine which numbers are hyper-parameters and which are parameters. Then fill in the below table. Again, I've done the first two: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Method/Algorithm</th>\n",
    "    <th>Hyper-parameters</th>\n",
    "    <th>Parameters</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<th>k-means</th>\n",
    "    <td> k </td>\n",
    "    <td> - </td>\n",
    "\n",
    "</tr>\n",
    "<tr>\n",
    "<th>kNN</th>\n",
    "    <td>k</td>\n",
    "    <td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<th>PCA</th>\n",
    "    <td>Hyper-parameters</td>\n",
    "    <td>Parameters</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<th>SVD</th>\n",
    "    <td>Hyper-parameters</td>\n",
    "    <td>Parameters</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<th>PCA</th>\n",
    "    <td>Hyper-parameters</td>\n",
    "    <td>Parameters</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<th>Spectral Clustering</th>\n",
    "    <td>Hyper-parameters</td>\n",
    "    <td>Parameters</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<th>Regression</th>\n",
    "    <td>Hyper-parameters</td>\n",
    "    <td>Parameters</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Parameters\n",
    "\n",
    "Setting parameters is easier of the two types. Usually you use an algorithm or method (like linear regression) that has a process for setting the parameters (ie. the slope $m$ and the intercept $b$) given the data. Essentially, once you have made your choices regarding method and hyper-parameter values, then the data will finish the determination for the parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Hyper-Parameters\n",
    "\n",
    "Selecting the value for hyper-parameters must come before you can apply your method. For example, you cannot run k-means without a value for $k$. Choosing these values is more of an art because -- unlike setting parameters -- your data can't tell you the best choice. You might be able to make an educated guess by visualizing the data. \n",
    "\n",
    "There is no one right way to choose your hyper-parameters, but you can apply external evaluation or validation techniques to assist you in this choice. So far, we have used elbowology in combination of some external notion of \"goodness.\" For each possible value for the hyper-parameter, we calculated our notion of \"goodness\" and we selected the one that creates the deepest \"elbow\" when we graph the possible values for the hyper-parameter against the resulting \"goodness\" value.\n",
    "\n",
    ">\"Wait, wait, wait. You said the data couldn't tell us what value the hyper-parameter should be.\" -- A natural human reaction \n",
    "\n",
    "This reaction is absolutely appropriate. While we use data to compute the notion of \"goodness\", there is no accepted way to determine where the elbow of your graph of \"goodness\" values is. That decision still requires finesse on the part of the researcher. So no, the data doesn't \"tell\" us what the value should be, but we do certainly use the data to make a reasonable decision about our hyper-parameters.\n",
    "\n",
    "#### An Imperfect metaphor\n",
    "One (imperfect) metaphor is deciding hyper-parameters is akin to choosing the type of clothing - dress, shirt, pants - and the parameters are then set by the tailor fitting the clothing to the person wearing it. The tailor does not fundamentally change the garment, but rather adjusts the lengths of various aspects of the garment to fit the person.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods with both Hyper-Parameters and Parameters\n",
    "\n",
    "Many methods have both hyper-parameters and parameters. This speaks to our intuition that we need to make some decisions before we deploy a method and then have some pieces tuned (or fit more closely) to our particular situation. \n",
    "\n",
    "**Question** - Which of the methods that we have seen have both hyper-parameters and parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing for coding pieces\n",
    "\n",
    "This section is just importing packages and data as well as defining the functions that we need for later in this lab. You just need to run the next 3 blocks. For now, just skip over the fourth block. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import block\n",
    "%matplotlib notebook \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d \n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data for this Lab\n",
    "\n",
    "employ_data = pd.read_csv(\"lab9data.csv\", sep = \",\")\n",
    "\n",
    "## numpy vectors of our inputs\n",
    "neuro = employ_data[[\"neuroticism\"]].to_numpy()\n",
    "perform = employ_data[[\"performance\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for later use\n",
    "\n",
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0, color=\"lightblue\")\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "    \n",
    "def place_parameter(p_vec, col, ax=None):\n",
    "    plt.scatter(p_vec[0],p_vec[1], c=col, marker = \"*\")\n",
    "    \n",
    "def draw_parameter_path(p0,p1, col, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0, color=col)\n",
    "    ax.annotate('', p1, p0, arrowprops=arrowprops)   \n",
    "    \n",
    "def compute_mse(truth_vec, predict_vec):\n",
    "    # MSE is the average of the difference between the\n",
    "    # true values and the predicted values squared\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For function testing \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters as vectors\n",
    "\n",
    "We can think of our *parameters* as a list of values with each value existing on its own numberline that is separate from every other parameters' numberlines. The issue with this view is that we see the parameters as disconnected from each other. Another view is to consider the parameters as vectors. \n",
    "\n",
    "Take our example from last time, we were fitting lines to our data. In this example we have two parameters $m$ (the slope of a line) and $b$ (the intercept or where the line crosses the $x = 0$ line). We can see these two parameters as a vector $[m,b]$ defined for this \"parameter\" space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "# Set axis limits\n",
    "xmin = -5\n",
    "xmax = 10\n",
    "ymin = -5\n",
    "ymax = 10\n",
    "\n",
    "plt.xlim([xmin, xmax])\n",
    "plt.ylim([ymin, ymax])\n",
    "\n",
    "# Build axes\n",
    "draw_vector([xmin,0], [xmax,0])\n",
    "draw_vector([0,ymin], [0,ymax])\n",
    "\n",
    "# Create grid and labels\n",
    "plt.grid(True)\n",
    "plt.xlabel('Value for m --->')\n",
    "plt.ylabel('Value for b --->')\n",
    "plt.title('Parameter Vector space')\n",
    "\n",
    "# Don't forget to \"turn the power off\" on this plot before moving on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pull the $m$ and $b$ values for our first two lines that we made last time and place them in this space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "# Set axis limits\n",
    "xmin = -5\n",
    "xmax = 10\n",
    "ymin = -5\n",
    "ymax = 10\n",
    "\n",
    "plt.xlim([xmin, xmax])\n",
    "plt.ylim([ymin, ymax])\n",
    "\n",
    "# Build axes\n",
    "draw_vector([xmin,0], [xmax,0])\n",
    "draw_vector([0,ymin], [0,ymax])\n",
    "\n",
    "# Add your m's and b's from Lab 08:\n",
    "place_parameter([m2,b2],\"red\")\n",
    "place_parameter([m4,b4],\"green\")\n",
    "\n",
    "# Create grid and labels\n",
    "plt.grid(True)\n",
    "plt.xlabel('Value for m --->')\n",
    "plt.ylabel('Value for b --->')\n",
    "plt.title('Parameter Vector space')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this view and at this point, we have no information about how these parameters relate to the data. Today, we will perform a _grid search_ and build a surface that will connect our parameters to the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "\n",
    "In a grid search, we create a list for each parameter that we want to investigate and then we create a grid that encapsulates all combinations of values from these parameter lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "    <th></th>\n",
    "<th>m = 0</th>\n",
    "    <th>m = 0.5</th>\n",
    "    <th>m = 1.5</th>\n",
    "    <th>m = 2</th>\n",
    "    <th>...</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<th>b = 0</th>\n",
    "    <td>(0,0) </td>\n",
    "    <td>(0,0.5)</td>\n",
    "    <td>(0,1.5) </td>\n",
    "    <td>(0,2)</td>\n",
    "    <td>(0,...) </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<th>b = 1/3</th>\n",
    "    <td>(1/3,0) </td>\n",
    "    <td>(1/3,0.5)</td>\n",
    "    <td>(1/3,1.5) </td>\n",
    "    <td>(1/3,2)</td>\n",
    "    <td>(1/3,...) </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<th>b = 2/3</th>\n",
    "    <td>(2/3,0) </td>\n",
    "    <td>(2/3,0.5)</td>\n",
    "    <td>(2/3,1.5) </td>\n",
    "    <td>(2/3,2)</td>\n",
    "    <td>(2/3,...) </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<th>b = 1</th>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<th>b = 4/3</th>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a grid search, we then replace each entry with some evaluation metric relating to how those parameters do predicting our data. \n",
    "\n",
    "For example, we would use the listed $m$ and $b$ to build a line `perform_guess = m*neuro_score + b`. Then we would compare the guesses for the performance scores to the ground truth. \n",
    "\n",
    "Today we will use the _**mean squared error**_ for this comparison:\n",
    "\n",
    "$MSE(truth, guess) = avg(truth - guess)^2$\n",
    "\n",
    "In the function block above, there is a fourth function that has only `pass` in the body of the function. Fill in the details for taking in a vector of truth and a vector of predictions and returning the average of the squared difference between them. Use the `function testing` block to check your function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating evaluation matrix\n",
    "\n",
    "_Note:_ We are not making the full use of the power of `numpy` in this section. We are doing this the brute force way to better understand what each step is doing in the grid search. \n",
    "\n",
    "To create the evaluation matrix, we need to:\n",
    "1. Create the list of parameter values that we are interested in\n",
    "2. Try all possible combinations of the parameters in our model\n",
    "3. Store how \"well\" those parameters do\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the possible values for our parameters\n",
    "m_vec = np.arange(-3, 10, 0.1)\n",
    "b_vec = np.arange(-2, 60, 0.2)\n",
    "\n",
    "# Create a place to store the values \n",
    "# HINT - Look at the above matrix! \n",
    "eval_mat = np.zeros([len(????),len(????)])\n",
    "\n",
    "# Try (or loop) over all possible combinations\n",
    "# HINT think about how you would traverse this grid\n",
    "for m_inds in range(len(m_vec)):\n",
    "    m = m_vec[???]\n",
    "    for b_inds in range(len(b_vec)):\n",
    "        b = ???\n",
    "        preds = ???*neuro + ???\n",
    "        \n",
    "        goodness = compute_mse(???,???)\n",
    "        eval_mat[???,???] = goodness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the lengths of m_vec and b_vec\n",
    "\n",
    "# Compare these length to the shape of eval_mat\n",
    "\n",
    "# Is the shape of eval_mat what you expect? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is that our MSE is close to zero. In this evaluation matrix, we could simply find the minimum and use those parameters. Let's see what we have: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a peek at your results\n",
    "\n",
    "plt.imshow(eval_mat,cmap='viridis')       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ummmm... What does this mean? \n",
    "\n",
    "Instead let's look at this matrix as a 3D object (also known as a surface) instead. To do that, we'll use some of the tricks from Lab 5 combined with `meshgrid`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "bs,ms = np.meshgrid(b_vec,m_vec)\n",
    "\n",
    "# Create the SCATTER() plot with colors\n",
    "ax.plot_surface(ms,bs, eval_mat, cmap='viridis',edgecolor='none');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting this lab\n",
    "\n",
    "To finish up this lab, post an image of your surface on **#lab_submission** channel on slack and say where you think the lowest point on the surface is based on _**looking**_ at the image.\n",
    "\n",
    "If your have questions from this lab, post them to #lab_questions with the same preamble (i.e. starting with **Lab9**). If you have the same question, please use one of the emoji's to upvote the question. If you would like to answer someone's question, please use the thread function. This will tie your answer to their question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previewing next week\n",
    "\n",
    "Our goal is that our MSE is close to zero. In this evaluation matrix, we could simply find the minimum and use those parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.unravel_index(np.argmin(eval_mat, axis=None), eval_mat.shape)\n",
    "\n",
    "# Trick from example on https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmin.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Minimum of eval_mat is\", eval_mat[ind], \"\\n\")\n",
    "print(\"This min is found at m =\", ms[ind], \"and b=\", bs[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why are we plotting anything at all? Well, grid search is not the most efficient way to search for parameters. So what could be alternative? \n",
    "\n",
    "Notice, we have not actually computed a linear regression. Instead we have the form of the output ($y = m*x +b$) and have been just putting parameters into the $m$ and $b$ places. And we did a lot of computations to find just one minimum. \n",
    "\n",
    "So how could we do this kind of \"blind\" search better? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent....\n",
    "\n",
    "NEXT WEEK! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources consulted \n",
    "\n",
    "1. [Linear Regression using Gradient Descent](https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931)\n",
    "2. [How to Create an Empty Figure with Matplotlib in Python](http://www.learningaboutelectronics.com/Articles/How-to-create-an-empty-figure-with-matplotlib-in-Python.php)\n",
    "3. [Findobj Demo](https://matplotlib.org/3.1.1/gallery/misc/findobj_demo.html#sphx-glr-gallery-misc-findobj-demo-py)\n",
    "4. [3d plots in matplotlib](https://matplotlib.org/mpl_toolkits/mplot3d/tutorial.html)\n",
    "5. [meshgrid in matplotlib](https://docs.scipy.org/doc/numpy/reference/generated/numpy.meshgrid.html)\n",
    "6. [argmin in numpy](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmin.html)\n",
    "7. [Three-Dimensional Plotting in Matplotlib](https://jakevdp.github.io/PythonDataScienceHandbook/04.12-three-dimensional-plotting.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
