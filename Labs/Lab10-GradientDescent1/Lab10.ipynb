{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10\n",
    "\n",
    "This week, we are focusing on gradient descent. Like last week, we wil be using the fake employee dataset with the goal of finding the best parameters for linear regression. The goals for this week are:\n",
    "\n",
    "0. Reviewing grid search and the drawbacks of it\n",
    "1. Motivate the process for gradient descent\n",
    "2. Detail the steps of gradient descent\n",
    "3. Define the learning rate for gradient descent and the impact of it on the speed of the algorithm\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports for Today\n",
    "\n",
    "Let us import the packages that we need for today and the dataset from last time. You may want to simply make a copy of the data in the Lab 10 directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import block\n",
    "%matplotlib notebook \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d \n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for later use\n",
    "\n",
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0, color=\"lightblue\")\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "    \n",
    "def place_parameter(p_vec, col, ax=None):\n",
    "    plt.scatter(p_vec[0],p_vec[1], c=col, marker = \"*\", s = 100)\n",
    "    \n",
    "def draw_parameter_path(p0,p1, col, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0, color=col)\n",
    "    ax.annotate('', p1, p0, arrowprops=arrowprops)   \n",
    "    \n",
    "def compute_mse(truth_vec, predict_vec):\n",
    "    return np.mean((truth_vec - predict_vec)**2)\n",
    "    \n",
    "def compute_m_partial(in_vals, truth_vec, predict_vec):\n",
    "    pass\n",
    "\n",
    "def compute_b_partial(truth_vec, predict_vec):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Data\n",
    "\n",
    "employ_data = pd.read_csv(???, sep = \",\")\n",
    "\n",
    "## numpy vectors of our inputs\n",
    "neuro = employ_data[[\"neuroticism\"]].to_numpy()\n",
    "perform = employ_data[[\"performance\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For function testing \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where are we? \n",
    "\n",
    "The example that we've been carrying around is about building a line that does the best job of prediction. \n",
    "\n",
    "In our specific case, we are working to predict someone's performance score based on a score from a personality test. We are seeking the right $m$ and $b$ for the equation: `performance = m*neuro + b`. \n",
    "\n",
    "So far, we've found the values for these parameters in two ways:\n",
    "1. Using linear regression which uses some linear algebra to state the best values\n",
    "2. Using grid search\n",
    "\n",
    "The first option has the drawback that (in this class) we have not delved into why and how linear regression works. The second also has drawbacks, namely that it can be computationally expensive, which we will discuss in more detail below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewing Grid Search\n",
    "\n",
    "Last time, we introduced grid search, a process of testing all combinations of parameter values by first laying out a _grid_ of the combinations with each parameter on its own axis. For example: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "    <th></th>\n",
    "<th>m = 0</th>\n",
    "    <th>m = 0.5</th>\n",
    "    <th>m = 1.5</th>\n",
    "    <th>m = 2</th>\n",
    "    <th>...</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<th>b = 0</th>\n",
    "    <td>(0,0) </td>\n",
    "    <td>(0,0.5)</td>\n",
    "    <td>(0,1.5) </td>\n",
    "    <td>(0,2)</td>\n",
    "    <td>(0,...) </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<th>b = 1/3</th>\n",
    "    <td>(1/3,0) </td>\n",
    "    <td>(1/3,0.5)</td>\n",
    "    <td>(1/3,1.5) </td>\n",
    "    <td>(1/3,2)</td>\n",
    "    <td>(1/3,...) </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<th>b = 2/3</th>\n",
    "    <td>(2/3,0) </td>\n",
    "    <td>(2/3,0.5)</td>\n",
    "    <td>(2/3,1.5) </td>\n",
    "    <td>(2/3,2)</td>\n",
    "    <td>(2/3,...) </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<th>b = 1</th>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<th>b = 4/3</th>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a grid search, we then replace each entry with some evaluation metric relating to how those parameters do when plugged into a desired algorithmic formulation (say a line $y = m*x+b$) at predicting our data. Last class, we will use the _mean squared error_ for this evaluation:\n",
    "\n",
    "$MSE(truth, guess) = avg(truth - guess)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we have $n_1$ number of possible values for the first parameter and $n_2$ number of possible values for the second parameter, how many times do we compute the MSE? \n",
    "\n",
    "\n",
    "(Add your thoughts here and then fill in the below blank)\n",
    "\n",
    "**Fill in this blank:**\n",
    "We will compute the MSE $_______$ times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search drawback\n",
    "\n",
    "This is a lot of computations! After all these computations, we then have to compute a minimum of all of those results! (which is even more computation)\n",
    "\n",
    "**Gradient descent** allows us to speed this process up and allows us to find the minimum more directly. Let's take moment to see how this could work by reminding ourselves of the surface created by the evaluation matrix last time.\n",
    "\n",
    "(Below is my implementation, just to have as reference. You are welcome to replace the below block with your own implementation. There is more than one way to code a grid search!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the possible values for our parameters\n",
    "m_vec = np.arange(-3, 10, 0.1)\n",
    "b_vec = np.arange(-2, 60, 0.2)\n",
    "\n",
    "# Create a place to store the values \n",
    "# HINT - Look at the above matrix! \n",
    "eval_mat = np.zeros([len(m_vec),len(b_vec)])\n",
    "\n",
    "# Try (or loop over) all possible combinations\n",
    "# HINT think nested\n",
    "for m_inds in range(len(m_vec)):\n",
    "    m = m_vec[m_inds]\n",
    "    for b_inds in range(len(b_vec)):\n",
    "        b = b_vec[b_inds]\n",
    "        preds = m*neuro + b\n",
    "        \n",
    "        goodness = compute_mse(perform,preds)\n",
    "        eval_mat[m_inds,b_inds] = goodness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "bs,ms = np.meshgrid(b_vec,m_vec)\n",
    "\n",
    "# Create the SCATTER() plot with colors\n",
    "ax.plot_surface(ms,bs, eval_mat, cmap='viridis',edgecolor='none');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metaphor for gradient descent\n",
    "\n",
    "Gradient descent is nearly how it sounds: we will *descend* towards the best fitting parameter values (in this case $m$ and $b$) by moving in the direction of greatest negative change, as specified by the *gradient.*\n",
    "\n",
    "To visualize what we will be doing, pick a high place on the surface and with the goal of finding the lowest point on the surface. How would you travel/sled/slide down the surface to arrive at this minimum? \n",
    "\n",
    "(Trace your path with your finger, not code)\n",
    "\n",
    "Does your path always move such that you are parallel to the $m$-axis and/or parallel to the $b$-axis? \n",
    "\n",
    "(Your thoughts)\n",
    "\n",
    "**_Wait here for a group discussion_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Charting a path\n",
    "\n",
    "As you can imagine, charting this path may curve as you move down the surface. Instead of trying to define one direct complete from the beginning, we aim to _descend,_ little by little as dictated by the _gradient._ So what is the gradient exactly?\n",
    "\n",
    "To answer this, let's consider that we begin our search at $m=0$ and $b=0$. How should we change $m$ and $b$ to move towards the minimum fastest? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "\n",
    "The gradient is a version of a multi-variable derivative, whose direction tells us how to move in the steepest direction. If we move in the (positive) gradient direction this yields the greatest increase; while the opposite (or negative) gradient direction is the direction of greatest descent. Effectively, the gradient is the answer to the question _\"How should we change the parameters to move towards the minimum fastest?\"_ \n",
    "\n",
    "The gradient a vector comprised of the partial derivatives with respect to each parameter. When dealing with derivatives, we must first clarify what we are taking a derivative of. In our running example, what are we trying to descend? Justify your choice. \n",
    "\n",
    "\n",
    "**_Wait here for a group discussion_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial derivatives on evaluation metric \n",
    "\n",
    "We are attempting to descend the surface created by the evaluation metric. We need the gradient to tell us which direction to move on the $(m,b)$ parameter space to descend on the evaluation metric surface the quickest. This means that we need to compute:\n",
    "\n",
    "1. The partial derivative of the MSE with respect to $m$, and \n",
    "2. The the partial derivative of the MSE with respect to $b$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With respect to $m$\n",
    "\n",
    "Let's begin with the first one. Recall that we defined MSE as:\n",
    "\n",
    "\\begin{equation}\n",
    "MSE(truth, guess) = avg(truth - guess)^2\n",
    "\\end{equation}\n",
    "\n",
    "For ease of notation, let's refer to \"truth\" or the true performance score as $p_i$ for each $i$ data point. Similarly, let $n_i$ be the neuroticism score. This notation means that the \"guess\" for each $i$ data point is given by $(m*n_i + b)$ for current values of $m$ and $b$. If $D$ is the number of datapoints, we can re-write this MSE as:\n",
    "\n",
    "\\begin{equation}\n",
    "MSE = \\dfrac{1}{D}\\Sigma_{i}(p_i - (m*n_i + b))^2\n",
    "\\end{equation}\n",
    "\n",
    "Now, to take the partial derivative with respect to $m$, we treat every notation -- except $m$ -- in our equation as a **fixed** number (like the number 4) and treat $m$ as something that can vary (or as something that is unknown). \n",
    "\n",
    "Recalling that the derivative of $(f(x))^2$ is $2*f(x)*f'(x)$ (or 2 times the original function times the derivative of the original function) by the chain rule, then the partial derivative of MSE with respect to $m$ (denoted as $d_m(MSE)$) is: \n",
    "\n",
    "\\begin{equation}\n",
    "d_m(MSE) = d_m(\\dfrac{1}{D}\\Sigma_{i}(p_i - (m*n_i + b))^2)\\\\\n",
    "~\\hspace{3cm} = \\dfrac{1}{D}\\Sigma_{i} 2*(p_i - (m*n_i + b))*(-n_i) \\\\\n",
    "~\\hspace{2cm} = \\dfrac{-2}{D}\\Sigma_{i} (n_i)(p_i - (m*n_i + b))\n",
    "\\end{equation}\n",
    "\n",
    "Interpreting this into the original formulation, we have that the partial derivative of MSE with respect to $m$ is: \n",
    "\n",
    "\\begin{equation}\n",
    "d_m(MSE) = -2*avg((neuro)*(truth - (guess)))\n",
    "\\end{equation}\n",
    "\n",
    "Now that we have this partial derivative, we plug in the **current** values for each of these components. The resulting value tells us how far to move in the $m$ direction. \n",
    "\n",
    "In the function block, there is a function `compute_m_partial` that currently just `pass`es. Please complete this formula. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With respect to $b$\n",
    "\n",
    "The second one is much like the first. We begin with the re-written MSE, treating every notation -- except $b$ -- in our equation as a **fixed** number (like the number 4) and treat $b$ as something that can vary (or as something that is unknown): \n",
    "\n",
    "\\begin{equation}\n",
    "d_b(MSE) = d_b(\\dfrac{1}{D}\\Sigma_{i}(p_i - (m*n_i + b))^2)\\\\\n",
    "~\\hspace{3cm} = \\dfrac{1}{D}\\Sigma_{i} 2*(p_i - (m*n_i + b))*(-1) \\\\\n",
    "~\\hspace{2cm} = \\dfrac{-2}{D}\\Sigma_{i} (p_i - (m*n_i + b))\n",
    "\\end{equation}\n",
    "\n",
    "Interpreting this into the original formulation, we have that the partial derivative of MSE with respect to $m$ is: \n",
    "\n",
    "\\begin{equation}\n",
    "d_b(MSE) = -2*avg((truth - (guess)))\n",
    "\\end{equation}\n",
    "\n",
    "Now that we have this partial derivative, we plug in the **current** values for each of these components. The resulting value tells us how far to move in the $b$ direction. \n",
    "\n",
    "In the fuction block, there is a function `compute_b_partial` that currently just `pass`es. Please complete this formula. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the partial derivatives\n",
    "\n",
    "The partial derivatives tell us the steepest direction to move in from our current point. Since we want to move towards the minimum, we choose to move in the direction of the negative gradient. \n",
    "\n",
    "We also want to decide how much we want to move each time. Let's call this value $L$ (for learning rate). We'll discuss the learning rate next time. \n",
    "\n",
    "That is, we update our values for $m$ and $b$ as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "m_{new} = m - L*d_m(MSE) \\\\\n",
    "b_{new} = b - L*d_b(MSE)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each Journey begins with a single step\n",
    "\n",
    "Starting with $m=0$ and $b=0$, compute the gradient and update your values for $m$ and $b$ setting $L = 0.01$. Chart your this first step on the below plot: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set starting point for m and b (called m0 and b0)\n",
    "m0 = \n",
    "b0 = \n",
    "\n",
    "# Computing the gradient\n",
    "preds = \n",
    "d_m = \n",
    "d_b = \n",
    "\n",
    "# Set L\n",
    "L = 0.01\n",
    "\n",
    "# Updating the parameter values \n",
    "m1 = \n",
    "b1 = \n",
    "\n",
    "print(d_b)\n",
    "print(m1,b1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "# Set axis limits\n",
    "xmin = -2\n",
    "xmax = 8\n",
    "ymin = -2\n",
    "ymax = 8\n",
    "\n",
    "plt.xlim([xmin, xmax])\n",
    "plt.ylim([ymin, ymax])\n",
    "\n",
    "# Build axes\n",
    "draw_vector([xmin,0], [xmax,0])\n",
    "draw_vector([0,ymin], [0,ymax])\n",
    "\n",
    "# Starting place \n",
    "place_parameter([0,0],\"red\")\n",
    "\n",
    "# First step\n",
    "draw_parameter_path(???,???,\"green\")\n",
    "\n",
    "# Create grid and labels\n",
    "plt.grid(True)\n",
    "plt.xlabel('Value for m --->')\n",
    "plt.ylabel('Value for b --->')\n",
    "plt.title('Parameter Vector space')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the next step and plot it on your image. What do you notice? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code block for you\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting block for you \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding more steps\n",
    "\n",
    "At its core, **gradient descent** is a series of steps dictated by the _gradient_ _descending_ an evaluation surface. \n",
    "\n",
    "In the below code block, create an iterative process that creates lists of $m$ values, $b$ values, and the MSE associated to it. We should be able to state the number of steps `n_steps` and decend that number of times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the number of steps you wish to take\n",
    "# Set your learning rate\n",
    "n_steps = ???\n",
    "L = 0.01\n",
    "\n",
    "# Initialize starting parameters\n",
    "m = ??\n",
    "b = ??\n",
    "\n",
    "# Create empty lists to store values for m, b, and the associated MSE\n",
    "outm = []\n",
    "outb = ???\n",
    "outmse = ???\n",
    "\n",
    "# Create an iterative process (ie. a loop) that will take N_STEPS\n",
    "#           Note: you likely need to take at least 500 steps\n",
    "for stp in range(??):\n",
    "    # For the current values of m and b: \n",
    "    \n",
    "    # 1. Compute the MSE\n",
    "    preds = ???\n",
    "    errormse = compute_mse(??,??)\n",
    "    \n",
    "    # 2. Store m, b, and the associated MSE in the output lists:\n",
    "    outm.append(m)\n",
    "    outb.append(??)\n",
    "    outmse.??\n",
    "    \n",
    "    # Update m and b by:\n",
    "    \n",
    "    # 1. Computing the gradient\n",
    "    d_m = compute_m_partial(???,???,???)\n",
    "    d_b = ???\n",
    "    \n",
    "    # Update the values for m and b\n",
    "    m = m - (?*d_m)\n",
    "    b = ??\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, plot your path with the parameters selected by the `sklearn` implementation of linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot your results here\n",
    "# plotting block for you \n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "# Set axis limits\n",
    "xmin = -2\n",
    "xmax = max(outm) + 3\n",
    "ymin = ??\n",
    "ymax = ??\n",
    "\n",
    "plt.xlim([xmin, xmax])\n",
    "plt.ylim([??, ??])\n",
    "\n",
    "# Build axes\n",
    "draw_vector([xmin,0], [xmax,0])\n",
    "draw_vector([0,ymin], [0,ymax])\n",
    "\n",
    "# Starting place \n",
    "place_parameter([0,0],\"red\")\n",
    "\n",
    "# plot vectors\n",
    "for stp in range(n_steps - 1):\n",
    "    draw_parameter_path([outm[stp], outb[stp]],[??,??],\"green\")\n",
    "\n",
    "# Create grid and labels\n",
    "plt.grid(True)\n",
    "plt.xlabel('Value for m --->')\n",
    "plt.ylabel('Value for b --->')\n",
    "plt.title('Parameter Vector space')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next time\n",
    "\n",
    "Putting this all together into gradient descent with a proper learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts\n",
    "\n",
    "To finish up this lab, compute the first 50 steps using the gradient. Create a plot with both the first 50 steps AND the best values of $m$ and $b$ according to the linear regression from `sklearn`. \n",
    "Share your plot in a post on **#lab10_submission** channel on slack and share something that you find odd you about your plot. \n",
    "\n",
    "If your have questions from this lab, post them to #lab_questions with the same preamble (i.e. starting with **Lab10**). If you have the same question, please use one of the emoji's to upvote the question. If you would like to answer someone's question, please use the thread function. This will tie your answer to their question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources consulted \n",
    "\n",
    "0. _Doing Data Science: Straight talk from the frontline_ by C. O'Neil & R. Schutt (2014)\n",
    "1. [Linear Regression using Gradient Descent](https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931)\n",
    "2. [Gradient in Wolfram](http://mathworld.wolfram.com/Gradient.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
