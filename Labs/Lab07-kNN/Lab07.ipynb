{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7\n",
    "\n",
    "Today we will start by discussing PCA versus SVD and then move on to **Supervised Learning**. Today's goals are:\n",
    "\n",
    "0. Articulate the differences between _clustering_ and _classification_\n",
    "1. Define _nearest neighbors_\n",
    "2. Build k-nearest neighbor \n",
    "3. Compare kNN to k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# This is a file generated just for this lab \n",
    "import labseven"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA vs SVD\n",
    "\n",
    "PCA and SVD are both dimension reduction algorithms. But are they the _same_? Let us consider a few questions:\n",
    "* What are their restrictions? \n",
    "* Is one always superior? \n",
    "* Is one always preferred?\n",
    "\n",
    "Discuss these questions with your group. Vote on #Lab07-submissions channel with emojis as to whether you think you would reach for PCA or SVD before the other one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "\n",
    "As we mentioned in Lab 3, there are two basic kinds of machine learning: supervised and unsupervised. Up until now, we have worked with unsupervised learning, investigating how to _cluster_ data with k-means and how to reduce data to its most essential information with _dimension reduction_. \n",
    "\n",
    "Today, we move to our first **supervised** machine learning algorithm. Today's lab is circular in nature; instead of directly defining _supervision_ or detailing the steps of today's algorithm, we will seek to take an intuitive journey towards both. \n",
    "\n",
    "\n",
    "### Today's data\n",
    "\n",
    "We will finally be moving on to a new dataset. Please load `lab7data.csv` as `mystery` using `loadtxt` in _numpy_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading our new data\n",
    "mystery = np.loadtxt(\"lab7data.csv\", delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate your data. \n",
    "\n",
    "# What is the size of Mystery? How many data points? How many variables? \n",
    "\n",
    "\n",
    "# Is all the information in Mystery numeric?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a few visualizations of your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What have you learned?\n",
    "\n",
    "Jot down a few first impressions of your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Your notes here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A bit of Information\n",
    "\n",
    "This data falls into two groups. With this added information, we might want to use k-means with $k=2$ to see if we can \"find\" those two groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply k-means (use the sklearn implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block for exploration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Today, we will approach the supervised task of _classification_ or attaching labels to data. \n",
    "\n",
    "\n",
    "With this vague definition, are classification and clustering the same? Are they different? _Justify your answers._\n",
    "\n",
    "**Wait here for a group discussion**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding \"supervision\"\n",
    "\n",
    "In supervised machine learning, we have extra information about some of our observations that is regarded as separate from the input variables. This extra information could be a rating, a label, a grouping, or an outcome, among other things. Colloquially, in this course, we might refer to this extra information as _answers_.   \n",
    "\n",
    "**Note** - Knowing that the data falls into two groups is not the supervision that we are referring to. Rather, it is the having of this kind of extra information, or answers, that is considered to be \"supervision.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a few labels\n",
    "\n",
    "In classification, we often know the labels of a few datapoints. In this example, we have <font color='red'>red</font> points and <font color='blue'>blue</font> labels. \n",
    "\n",
    "Let's look at a few of these labeled points compared to the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2022)\n",
    "# Gather one red and one blue point:\n",
    "\n",
    "# LABSEVEN is a python file created for this lab that \n",
    "#          has two functions: \n",
    "#          * RED_POINTS(num) extracts NUM red points \n",
    "#          * BLUE_POINTS(num) extracts NUM blue points\n",
    "red = labseven.red_points(1)\n",
    "blue = labseven.blue_points(1)\n",
    "\n",
    "plt.scatter(mystery[:,0], mystery[:,1], c=\"k\", alpha = 0.7)\n",
    "plt.scatter(red[:,0], red[:,1], c=\"deeppink\")\n",
    "plt.scatter(blue[:,0], blue[:,1], c=\"cyan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this image, what ideas do you have for using the information from our <font color='red'>red</font> point and <font color='blue'>blue</font> point? \n",
    "\n",
    "\n",
    "**Wait here for a group brainstorming session**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Add your notes from our discussion)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    \n",
    "####  \n",
    "_This space is intentionally left blank_\n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means + labels\n",
    "\n",
    "In k-means, we iterate between assigning clusters and adjusting each cluster's center. This iteration is required because we do not have any clear idea if we are \"right\" or not. In other words, we don't have the \"true\" group labels for each point; so we (iteratively) look for clusters, and once we have clusters, we then assign labels. Recall that in k-means, we don't even know the \"right\" number of clusters!\n",
    "\n",
    "In this lab, we **_know_** that we have two groups and we **_know_** that we have a representative from each group. How can we use that information to assign labels to the rest of the data **_without_** iterating between assigning clusters and updating group centers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each point in mystery to either \"red\" or \"blue\"\n",
    "\n",
    "# Calculate the distance to Red\n",
    "red_dist = distance.cdist(mystery,red, \"euclidean\")\n",
    "\n",
    "# Calculate the distance to Blue\n",
    "blue_dist = distance.cdist(mystery,blue, \"euclidean\")\n",
    "\n",
    "# Figure out which is closer (argmin)\n",
    "check_mat = np.hstack([red_dist,blue_dist])\n",
    "labels = np.argmin(check_mat,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate your results\n",
    "\n",
    "Take a moment to consider your image. Is this what you would expect given the original plot?\n",
    "\n",
    "What would you like to try next? \n",
    "\n",
    "\n",
    "**Wait here for a class brainstorming session**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    \n",
    "####  \n",
    "_This space is intentionally left blank_\n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding more labels\n",
    "\n",
    "Let's add more labeled points to expand this idea of assigning a label to each data point by the nearest labeled neighbor. This time, let's use 5 of each red points and blue points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather five red points and five blue points:\n",
    "red = labseven.red_points(??)\n",
    "blue = labseven.blue_points(??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's plot these labeled points against our whole dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(mystery[:,0], mystery[:,1], c=\"k\", alpha = 0.7)\n",
    "plt.scatter(red[:,0], red[:,1], c=\"deeppink\")\n",
    "plt.scatter(blue[:,0], blue[:,1], c=\"cyan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now assign a point to a group based on the color of the labeled point that it is closest to. We call this closest labeled point the _nearest neighbor._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each point in mystery to either \"red\" or \"blue\"\n",
    "\n",
    "# Calculate the distance to Red\n",
    "red_dist = distance.cdist(???, ???, \"euclidean\")\n",
    "\n",
    "# Calculate the distance to Blue\n",
    "blue_dist = distance.cdist(???, ???, \"euclidean\")\n",
    "\n",
    "# Figure out which is closer (argmin)\n",
    "check_mat = np.hstack([red_dist,blue_dist])\n",
    "labels = np.argmin(check_mat,axis=1)\n",
    "\n",
    "# This case is slightly different: \n",
    "#   We have 10 possible columns in check_mat instead of just two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate your results\n",
    "\n",
    "Take a moment to consider your image. Is this what you would expect given your first attempt classifying attempt and given the original plot (without colors)?\n",
    "\n",
    "What would you like to try next? \n",
    "\n",
    "\n",
    "**Wait here for a class brainstorming session**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    \n",
    "####  \n",
    "_This space is intentionally left blank_\n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using more than one neighbor\n",
    "\n",
    "In this next phase, we will use the same 10 labeled points (5 of each class) from above. But this time, we will label our points based on the three closest labeled neighbors. Before continuing, let us consider the different labeling for a collection of three neighbors, and decide how we would like to handle this:\n",
    "* {<font color='blue'>blue</font>, <font color='blue'>blue</font>, <font color='blue'>blue</font>}\n",
    "* {<font color='blue'>blue</font>, <font color='blue'>blue</font>, <font color='red'>red</font>}\n",
    "* {<font color='blue'>blue</font>, <font color='red'>red</font>, <font color='red'>red</font>}\n",
    "* {<font color='red'>red</font>, <font color='red'>red</font>, <font color='red'>red</font>}\n",
    "\n",
    "(Based on the above, why do you think we skipped over using two nearest neighbors?) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each point in mystery to either \"red\" or \"blue\"\n",
    "#        based on three neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate your results\n",
    "\n",
    "Take a moment to consider your image. Is this what you would expect given your first attempt classifying attempt and given the original plot (without colors)?\n",
    "\n",
    "What would you like to try next? \n",
    "\n",
    "\n",
    "**Wait here for a class brainstorming session**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    \n",
    "####  \n",
    "_This space is intentionally left blank_\n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could keep increasing either the number of labeled points or the number of labeled neighbors we compare to; we could consider doing both! But our ability to do either is often dictated by the data, the context of the data, or another limiting factor. However the general process that we have done is known as _k-nearest neighbor_ or kNN, for short."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbors\n",
    "\n",
    "To apply kNN, we need to have a dataset with a few labeled points. Then we need to decide how many labeled neighbors we want each data point to \"consult\" to determine the labeling. This is the $k$ in kNN. \n",
    "\n",
    "**Question** - Before moving on, is the $k$ in k-means the same as the $k$ in kNN? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN in `sklearn`\n",
    "\n",
    "As you might expect, the implementation of kNN in `sklearn` requires more than one step:\n",
    "1. Define the particulars for our version of kNN\n",
    "2. Fit the kNN to the labled points that we have\n",
    "3. Assign labels to the unlabeled data\n",
    "\n",
    "For this part, let's assume that we have 20 labeled points. To run this in python, we also need a vector encoding those labels. For this example, we say <font color='red'>red</font> points are label 0 and <font color='blue'>blue</font> points are label 1. \n",
    "\n",
    "Here we will use a few of the built in _numpy_ functions to stack our data and the associated labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2022)\n",
    "\n",
    "red = red_points(10)\n",
    "blue = blue_points(10)\n",
    "\n",
    "# Preparing the labeled data\n",
    "label_data = np.vstack([red, blue])\n",
    "\n",
    "labels = np.zeros(20)\n",
    "labels[10:20] = 1\n",
    "\n",
    "# Check the shapes\n",
    "print(label_data.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step one - Define the kNN based on the number of neighbors (ie. the k)\n",
    "kNN_alg = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step two - Fit the kNN to our labeled data\n",
    "kNN_alg.fit(label_data,labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step three - Assign the labels to the unlabeled data\n",
    "all_labels = kNN_alg.predict(mystery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your results\n",
    "\n",
    "plt.scatter(mystery[:,0], mystery[:,1], c=all_labels, cmap = \"PiYG\", alpha = 0.5)\n",
    "plt.scatter(red[:,0], red[:,1], c=\"deeppink\", alpha = 1)\n",
    "plt.scatter(blue[:,0], blue[:,1], c=\"cyan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think of the above results? Add a few notes here - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with values of $k$\n",
    "\n",
    "For the above implementation, we used on the \"nearest\" neighbor to make the assignments. Let's try this process again for a few values of  $k$, say {3, 7, 11, 15, 19}\n",
    "\n",
    "Run each version using the convention of `_k` at the end of each variable, as shown below: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run kNN for k=3\n",
    "kNN_alg_3 = KNeighborsClassifier(n_neighbors=???)\n",
    "kNN_alg_3.fit(label_data,labels)\n",
    "all_labels_3 = kNN_alg.predict(mystery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for k=3\n",
    "plt.scatter(mystery[:,0], mystery[:,1], c=???, cmap = \"PiYG\", alpha = 0.5)\n",
    "plt.scatter(red[:,0], red[:,1], c=\"deeppink\", alpha = 1)\n",
    "plt.scatter(blue[:,0], blue[:,1], c=\"cyan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run kNN for k=7\n",
    "kNN_alg_7 = \n",
    "kNN_alg_7.fit(????,????)\n",
    "all_labels_7 = kNN_alg.predict(????)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for k=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run kNN for k=11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for k=11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run kNN for k=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for k=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run kNN for k=19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for k=19\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the above plots, what do you think the two groupings are? Which is the \"right\" value of $k$?\n",
    "\n",
    "\n",
    "What do you think would happen if you had more labeled points and used the same values of $k$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering vs. Classification\n",
    "\n",
    "Clustering and classification are often used as representative algorithms for unsupervised and supervised learning, respectively. How do clustering and classification differ? Specifically, where is the supervision in classification? \n",
    "\n",
    "Could the student dataset (from previous labs) be used in a classification task? Why or why not? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts\n",
    "\n",
    "To finish up this lab, select one of the values of $k$ and follow the [Nearest Neighbors Classification example for `sklearn`](https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py) to create a plot of the decision boundaries for kNN with your value of $k$. Share your plot in a post on **#lab07_submission** channel on slack and share something that surprised you about your plot. \n",
    "\n",
    "If your have questions from this lab, post them to #lab_questions with the same preamble (i.e. starting with **Lab7**). If you have the same question, please use one of the emoji's to upvote the question. If you would like to answer someone's question, please use the thread function. This will tie your answer to their question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References consulted\n",
    "0. _Doing Data Science: Straight talk from the frontline_ by C. O'Neil & R. Schutt (2014)\n",
    "1. _Python Machine Learning_\n",
    "2. [kNN in `sklearn`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)\n",
    "3. [Nearest Neighbors Classification example for `sklearn`](https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py)\n",
    "4. [seed helpfile in `numpy`](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.seed.html)\n",
    "5. [List of named colors](https://matplotlib.org/3.1.0/gallery/color/named_colors.html)\n",
    "6. [Choosing Colormaps in Matplotlib](https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
