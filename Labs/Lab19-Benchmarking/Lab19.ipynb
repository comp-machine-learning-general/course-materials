{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 19\n",
    "\n",
    "Today, we will dig a bit more into evaluating the efficacy of our code via _benchmarking._ Today'a goals are: \n",
    "\n",
    "0. Define different kinds of benchmarking \n",
    "1. Deploy different kinds of benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning at Scale\n",
    "\n",
    "Part of working on large data and complex code is to explore how long each piece of code is taking. Today, we will return to our implementation of gradient descent and use that as our example. To prepare for that, please import the data from Lab 9 and add to the below functions (from Lab 11). \n",
    "\n",
    "#### Warning! \n",
    "\n",
    "You may get import errors for a few of the new lines. If you do, please stop your `jupyter notebook` and run the following in your **command line**: \n",
    "\n",
    "`conda install line_profiler`   \n",
    "`conda install memory_profiler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import pandas as pd\n",
    "\n",
    "import timeit ## <-- New line!\n",
    "import time\n",
    "\n",
    "import line_profiler ## <-- New line!\n",
    "import memory_profiler ## <-- New line!\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for later use \n",
    "    \n",
    "def compute_mse(truth_vec, predict_vec):\n",
    "    return np.mean((truth_vec - predict_vec)**2)\n",
    "    \n",
    "def compute_m_partial(in_vals, truth_vec, predict_vec):\n",
    "    return -2*np.mean(in_vals*(truth_vec-predict_vec))\n",
    "\n",
    "def compute_b_partial(truth_vec, predict_vec):\n",
    "    return -2*np.mean(truth_vec-predict_vec)\n",
    "\n",
    "def grad_des(input_data, truth_vec, max_steps):\n",
    "    # Add your implementation for gradient descent here\n",
    "    pass\n",
    "\n",
    "def minibatch_gd(input_data, truth_vec, batch_size, max_steps):\n",
    "    # Add your implementation for mini-batch gradient descent here\n",
    "    pass\n",
    "\n",
    "def stochastic_gd(input_data, truth_vec, max_steps):\n",
    "    # Add your implementation for stochastic gradient descent here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data\n",
    "\n",
    "This time, we will use both the data from Labs 9 and 16. Please add a copy of each to this directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Data\n",
    "\n",
    "employ_data = pd.read_csv(\"lab9data.csv\", sep = \",\")\n",
    "\n",
    "## numpy vectors of our inputs\n",
    "neuro = employ_data[[\"neuroticism\"]].to_numpy()\n",
    "perform = employ_data[[\"performance\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Data\n",
    "dog_pd = pd.read_csv(\"lab16data.csv\", sep = \",\", index_col = \"Breed Name\")\n",
    "\n",
    "#Change all the booleans to numbers\n",
    "dog_pd.iloc[:,7:11] = dog_pd.iloc[:,7:11].astype(\"int\")\n",
    "\n",
    "# Export to numpy\n",
    "dog_full_np = dog_pd.to_numpy()\n",
    "\n",
    "# Split into the input variables and the target classes\n",
    "in_dog_data = dog_full_np[:,:-1]\n",
    "out_class = dog_full_np[:,-1]\n",
    "\n",
    "# Get the variable names \n",
    "var_names = list(dog_pd.columns)[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing our implementations with `timeit`\n",
    "\n",
    "Last time, we used the `time` module to time how long it takes an implementation to run. Another option is to use **magic** built in python commands to check individual lines. These commands take the form of `%command` for a single line and `%%command` for a block of code. Today, we will use a few of them starting with `%%timeit`.\n",
    "\n",
    "Noting that there can be small changes in run time, in `timeit`, we run the code several times to find the average run time. Notice how the output between `%%time` and `%%timeit` are different: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "# Specify and fit the model\n",
    "grove = RandomForestClassifier(n_estimators=10, max_features = 3, max_depth=2, random_state=0)\n",
    "grove.fit(in_dog_data, out_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Specify and fit the model\n",
    "grove = RandomForestClassifier(n_estimators=10, max_features = 3, max_depth=2, random_state=0)\n",
    "grove.fit(in_dog_data, out_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What do you notice?\n",
    "\n",
    "Did one of the blocks take longer than the other one? Why do you think this is?  \n",
    "\n",
    "**Your thoughts**\n",
    "\n",
    "Whare are two things that you notice that are similar about these outputs?\n",
    "\n",
    "1. \n",
    "2. \n",
    "\n",
    "What are two things that are different about these outputs? \n",
    "\n",
    "1. \n",
    "2. \n",
    "\n",
    "*Hint* Consult the prose above the code blocks for an explanation as to `time` and `timeit` offer different outputs! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "Last time (and above), we looked at how long a whole algorithm would take to do its work. This is a good first step, but when we _benchmark_ code, we examine how fast each piece of the code is. This means that we need a method that will tell us how each piece runs. We could do this by running a time line for each individual piece of our code, OR we could use a _profiler_ which gives us more nuanced information about the run times without us having to insert additional timing lines. \n",
    "\n",
    "Benchmarking as a whole allows us to determine if we need to edit or change any lines due to _bottlenecking_ (or places where the code slows down).   \n",
    "\n",
    "As a visual, consider a bottle like a glass soda bottle with a long neck. Suppose you filled the bottle with little balls, and then turned the bottle upside down quickly. Would all the balls fall out at once or would there be a sort of traffic jam at one place where the balls would have to wait before falling the rest of the way? This place where they are waiting is the _bottleneck,_ hence the term \"bottlenecking.\"\n",
    "\n",
    "Today, we will look at three kinds of _profilers._ Specifically, we are looking at:   \n",
    "\n",
    "0. Profiling a whole script\n",
    "1. Line by line Profiling\n",
    "2. Memory Profiling \n",
    "\n",
    "For this part, we need to conda install a few things:\n",
    "* `line_profiler`\n",
    "* `memory_profiler`\n",
    "\n",
    "The remainder of this lab follows this [blog post](https://jakevdp.github.io/PythonDataScienceHandbook/01.07-timing-and-profiling.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (First) Profiler\n",
    "\n",
    "The first _magic_ line that we will use is `%prun` which is the profiler command. This is akin to `cProfile` or `profile` in usual [profilers for python scripting](https://docs.python.org/3/library/profile.html). This will tell you every piece (including deep parts of the base python) that are touched by your code. \n",
    "\n",
    "This profiler will give us [various timing information](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-prun). Run the below line and consult the linked helpfile to see what the profiler is telling you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun grove.fit(in_dog_data, out_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does it all mean? \n",
    "\n",
    "This is a lot of information. `%prun` gives us **every** piece and **every** line (even those in deep, deep python). This output can be super overwhelming. \n",
    "\n",
    "Let's look at a few pieces:   \n",
    "* First, let's look at the column names (as described [here](https://stackoverflow.com/questions/7069733/how-do-i-read-the-output-of-the-ipython-prun-profiler-command): \n",
    "  - `ncalls` is the number of classes to that line. \n",
    "  - `tottime` is the total running time per call (without including time calling helper functions)\n",
    "  - `cumtime` is the total running time per call (including time calling helper functions)\n",
    "* What gets called a lot? What gets called less? (Check out the `ncalls` column)\n",
    "* Next isolate the lines are taking up a lot of time in the `tottime` column \n",
    "* Now isolate the lines are taking up a lot of time in the `cumtime` column\n",
    "\n",
    "Are these all the same lines? Does this surprise you? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your turn\n",
    "\n",
    "Run `%prun` on your implementation of `grad_des` and on `minibatch_gd`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run %prun on grad_des"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which lines are taking the most time? \n",
    "\n",
    "**Your thoughts** \n",
    "\n",
    "Which lines are called the most? \n",
    "\n",
    "**Your thoughts** \n",
    "\n",
    "What surprises you about this analysis? \n",
    "\n",
    "**Your thoughts** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run %prun on minibatch_gd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which lines are taking the most time? \n",
    "\n",
    "**Your thoughts** \n",
    "\n",
    "Which lines are called the most? \n",
    "\n",
    "**Your thoughts** \n",
    "\n",
    "What surprises you about this analysis? \n",
    "\n",
    "**Your thoughts** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line Profiler\n",
    "\n",
    "Instead of looking at a full script and every python code that it calls, we might want to look how much time each line (just in our program) takes to execute. This will tells us how long each line we coded takes to run. \n",
    "\n",
    "To start this process, we will must load in the `line_profiler` as \"magic\" functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once loaded as \"magic\" we can use `%lprun` which will run our functions, timing them line by line. Let's do a silly example. Before running the line profiler, which line do you think will take longer: setting up the `grove` (line 1 of `tune_fit`) or fitting grove (line 2 of `tune_fit`)?\n",
    "\n",
    "**Add Your Thoughts _BEFORE_ running the code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_fit(in_vars,classes):\n",
    "    grove = RandomForestClassifier(n_estimators=10, max_features = 3, max_depth=2, random_state=0)\n",
    "    grove.fit(in_vars,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f tune_fit tune_fit(in_dog_data, out_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does it all mean? \n",
    "\n",
    "The output of `%lprun` gives us a line by line timing (including number of times called). Notice that the code is not running for over 30K seconds (which would be more than 8 hours), rather that the `Timer unit` is 0.00000001 of a second (or `1e-06`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your turn\n",
    "\n",
    "Run `%lprun` on your implementation of `grad_des` and on `minibatch_gd`. What is surprising about these two functions? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run %lprun on grad_des"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which lines are taking the most time? \n",
    "\n",
    "**Your thoughts** \n",
    "\n",
    "Which lines are called the most? \n",
    "\n",
    "**Your thoughts** \n",
    "\n",
    "What surprises you about this analysis? \n",
    "\n",
    "**Your thoughts** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run %lprun on minibatch_gd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which lines are taking the most time? \n",
    "\n",
    "**Your thoughts** \n",
    "\n",
    "Which lines are called the most? \n",
    "\n",
    "**Your thoughts** \n",
    "\n",
    "What surprises you about this analysis? \n",
    "\n",
    "**Your thoughts** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Profiler\n",
    "\n",
    "Instead of looking at timing of each line, we might want to look how much memory each line takes to execute as well as the total memory for the function. \n",
    "\n",
    "To start this process, we will must load in the `memory_profiler` as \"magic\" functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it is loaded as \"magic\", we can use two functions `%memit` which gives use the total memory usage and `%mprun` which gives us a line by line assessment of memory. (See bonus section below for `%mprun`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit tune_fit(in_dog_data, out_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your turn\n",
    "\n",
    "Run `%memit` on your implementation of `grad_des` and on `minibatch_gd`. What is surprising about these two functions? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run %memit on grad_des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run %memit on minibatch_gd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your results from above. What was your peak memory for each? Which one uses memory in a more efficient manner? \n",
    "\n",
    "**Your thoughts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(If you would like to see a line by line readout for memory usage, check out `mprun` below the final thoughts section.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we care?\n",
    "\n",
    "In this lab, we've used very small examples. But as we scale up our data, it is important to pay attention to what lines use more memory and take more time. These bottlenecks can make or break our machine learning implementations, and resolving them are critical to scaling up implementations to use on larger and larger datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts\n",
    "\n",
    "To finish up this lab, answer two of the following three questions:    \n",
    "1. **What did you learn by benchmarking two versions of gradient descent? Is there anything you want to re-examine in your code?**  \n",
    "2. **Which is more important to consider when programming a machine learning algorithm: time or memory? Why?**  \n",
    "3. **We have several notions of time: seconds, iterations, and epochs. Which of these do you think is the best to use for comparing the speed of different algorithms? Why that one?**\n",
    "\n",
    "Share your thoughts in a post on **#lab-19_submission** channel on slack with your answer. \n",
    "\n",
    "If your have questions from this lab, post them to #lab_questions with the same preamble (i.e. starting with **Lab19**). If you have the same question, please use one of the emoji's to upvote the question. If you would like to answer someone's question, please use the thread function. This will tie your answer to their question. \n",
    "\n",
    "### Next Time\n",
    "\n",
    "We will start our journey into deep learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus - `mprun` \n",
    "\n",
    "Like `prun` and `lprun`, `mprun` does a line by line read out on memory usage. \n",
    "\n",
    "To run `mprun`, we do have to create a file for our example before we can run it. Again following the [earlier blog post](https://jakevdp.github.io/PythonDataScienceHandbook/01.07-timing-and-profiling.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file mprun_demo.py\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import pandas as pd\n",
    "\n",
    "def tune_fit(in_vars,classes):\n",
    "    grove = RandomForestClassifier(n_estimators=10, max_features = 3, max_depth=2, random_state=0)\n",
    "    grove.fit(in_vars,classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again turn to our a silly example. Before running the memory profiler that we just created, which line do you think will take more memory? Setting up the `grove` (line 1 of `tune_fit`) or fitting grove (line 2 of `tune_fit`)\n",
    "\n",
    "**Your thoughts** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mprun_demo import tune_fit\n",
    "%mprun -f tune_fit tune_fit(in_dog_data, out_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources consulted \n",
    "\n",
    "0. [Benchmarking your code](https://rbspy.github.io/benchmarking-your-code/)\n",
    "1. [IPython Magic Commands](https://jakevdp.github.io/PythonDataScienceHandbook/01.03-magic-commands.html)\n",
    "2. [Profiling and Timing Code](https://jakevdp.github.io/PythonDataScienceHandbook/01.07-timing-and-profiling.html)\n",
    "3. [How do I get time of a Python program's execution?](https://stackoverflow.com/questions/1557571/how-do-i-get-time-of-a-python-programs-execution)\n",
    "4. [Measure Time in Python – time.time() vs time.clock()](https://www.pythoncentral.io/measure-time-in-python-time-time-vs-time-clock/#:~:text=The%20first%20type%20of%20time,called%20elapsed%20or%20running%20time.)  \n",
    "5. I also googled 1e-6 to check for the correct number of 0's after the decimal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
