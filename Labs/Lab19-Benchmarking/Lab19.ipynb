{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 19\n",
    "\n",
    "Today, we will dig a bit more into evaluating the efficacy of our code via _benchmarking._ Today'a goals are: \n",
    "\n",
    "0. Define different kinds of benchmarking \n",
    "1. Deploy different kinds of benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning at Scale\n",
    "\n",
    "Part of working on large data and complex code is to explore how long each piece of code is taking. Today, we will return to our implementation of gradient descent and use that as our example. To prepare for that, please import the data from Lab 9 and add to the below functions (from Lab 11):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import pandas as pd\n",
    "\n",
    "import timeit ## <-- New line!\n",
    "import time\n",
    "\n",
    "# import line_profiler\n",
    "# import memory_profiler\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for later use \n",
    "    \n",
    "def compute_mse(truth_vec, predict_vec):\n",
    "    return np.mean((truth_vec - predict_vec)**2)\n",
    "    \n",
    "def compute_m_partial(in_vals, truth_vec, predict_vec):\n",
    "    return -2*np.mean(in_vals*(truth_vec-predict_vec))\n",
    "\n",
    "def compute_b_partial(truth_vec, predict_vec):\n",
    "    return -2*np.mean(truth_vec-predict_vec)\n",
    "\n",
    "def grad_des(input_data, truth_vec, max_steps):\n",
    "    # Add your implementation for gradient descent here\n",
    "    pass\n",
    "\n",
    "def minibatch_gd(input_data, truth_vec, batch_size, max_steps):\n",
    "    # Add your implementation for mini-batch gradient descent here\n",
    "    pass\n",
    "\n",
    "def stochastic_gd(input_data, truth_vec, max_steps):\n",
    "    # Add your implementation for stochastic gradient descent here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Data\n",
    "\n",
    "employ_data = pd.read_csv(\"../Lab09-Parameters/lab9data.csv\", sep = \",\")\n",
    "\n",
    "## numpy vectors of our inputs\n",
    "neuro = employ_data[[\"neuroticism\"]].to_numpy()\n",
    "perform = employ_data[[\"performance\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Weather Data\n",
    "\n",
    "weather_data = np.genfromtxt(\"../Lab16-DecisionTrees/lab16data.csv\", delimiter=',', skip_header=1)\n",
    "weather_pd = pd.read_csv(\"../Lab16-DecisionTrees/lab16data.csv\", sep = \",\")\n",
    "\n",
    "# Split into the input variables and the target classes\n",
    "in_weather = weather_data[:,:4]\n",
    "out_class = weather_data[:,4]\n",
    "\n",
    "# Get the variable names \n",
    "var_names = list(weather_pd.columns)[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing our implementations with `timeit`\n",
    "\n",
    "Last time, we used the `time` module to time how long it takes an implementation to run. Another option is to use **magic** built in python commands to check individual lines. These commands take the form of `%command` for a single line and `%%command` for a block of code. Today, we will use a few of them starting with `%%timeit`.\n",
    "\n",
    "Noting that there can be small changes in run time, in `timeit`, we run the code several times to find the average run time. Notice how the output between `%%time` and `%%timeit` are different: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "# Specify and fit the model\n",
    "grove = RandomForestClassifier(n_estimators=10, max_features = 3, max_depth=2, random_state=0)\n",
    "grove.fit(in_weather, out_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Specify and fit the model\n",
    "grove = RandomForestClassifier(n_estimators=10, max_features = 3, max_depth=2, random_state=0)\n",
    "grove.fit(in_weather, out_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "Last time (and above), we looked at how long a whole algorithm would take to do its work. This is a good first step, but when we _benchmark_ code, we examine how fast each piece of the code is. This means that we need a system that tells us how each piece runs. We could do this by running a time line for each individual piece of our code, OR we could use a _profiler_ which gives us more nuanced information about the run times without us having to insert additional timing lines. \n",
    "\n",
    "Benchmarking as a whole allows us to determine if we need to edit or change any lines due to _bottlenecking_ (or places where the code slows down). \n",
    "\n",
    "Today, we will look at three kinds of _profilers._ Specifically, we are looking at:\n",
    "0. Profiling a whole script\n",
    "1. Line by line Profiling\n",
    "2. Memory Profiling \n",
    "\n",
    "For this part, we need to conda install a few things:\n",
    "* `line_profiler`\n",
    "* `memory_profiler`\n",
    "\n",
    "The remainder of this lab follows this [blog post](https://jakevdp.github.io/PythonDataScienceHandbook/01.07-timing-and-profiling.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (First) Profiler\n",
    "\n",
    "The first _magic_ line that we will use is `%prun` which is the profiler command. This is akin to `cProfile` or `profile` in usual [profilers for python scripting](https://docs.python.org/3/library/profile.html). This will tell you every piece (including deep parts of the base python) that are touched by your code. \n",
    "\n",
    "This profiler will give us [various timing information](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-prun). Run the below line and consult the linked helpfile to see what the profiler is telling you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun grove.fit(in_weather, out_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your turn\n",
    "\n",
    "Run `%prun` on your implementation of `grad_des` and on `minibatch_gd`. What is surprising about these two functions? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line Profiler\n",
    "\n",
    "Instead of looking at a full script, we might want to look how much time each line takes to execute. This will tells us how long each line we coded takes to run. \n",
    "\n",
    "To start this process, we will must load in the `line_profiler` as \"magic\" functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once loaded as \"magic\" we can use `%lprun` which will run our functions, timing them line by line. Let's do a silly example. Before running the line profiler, which line do you think will take longer? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_fit(in_vars,classes):\n",
    "    grove = RandomForestClassifier(n_estimators=10, max_features = 3, max_depth=2, random_state=0)\n",
    "    grove.fit(in_vars,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f tune_fit tune_fit(in_weather, out_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your turn\n",
    "\n",
    "Run `%lprun` on your implementation of `grad_des` and on `minibatch_gd`. What is surprising about these two functions? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Profiler\n",
    "\n",
    "Instead of looking at timing of each line, we might want to look how much memory each line takes to execute as well as the total memory for the function. \n",
    "\n",
    "To start this process, we will must load in the `memory_profiler` as \"magic\" functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it is loaded as \"magic\", we can use two functions `%memit` which gives use the total memory usage and `%mprun` which gives us a line by line assessment of memory. \n",
    "\n",
    "Let's again turn to our a silly example. Before running the memory profiler, which line do you think will take longer? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit tune_fit(in_weather, out_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your turn\n",
    "\n",
    "Run `%memit` on your implementation of `grad_des` and on `minibatch_gd`. What is surprising about these two functions? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus - `mprun` \n",
    "\n",
    "To run `mprun`, we do have to create a file for our example before we can run it. Again following the earlier blog post:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mprun_demo.py\n"
     ]
    }
   ],
   "source": [
    "%%file mprun_demo.py\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import pandas as pd\n",
    "\n",
    "def tune_fit(in_vars,classes):\n",
    "    grove = RandomForestClassifier(n_estimators=10, max_features = 3, max_depth=2, random_state=0)\n",
    "    grove.fit(in_vars,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%mprun` not found.\n"
     ]
    }
   ],
   "source": [
    "from mprun_demo import tune_fit\n",
    "%mprun -f tune_fit tune_fit(in_weather, out_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts\n",
    "\n",
    "To finish up this lab, answer the question: **what did you learn by benchmarking the three versions of gradient descent?** Share your thoughts in a post on **#lab_submission** channel on slack with your answer. Your post must start with **Lab19** to get credit.  \n",
    "\n",
    "If your have questions from this lab, post them to #lab_questions with the same preamble (i.e. starting with **Lab19**). If you have the same question, please use one of the emoji's to upvote the question. If you would like to answer someone's question, please use the thread function. This will tie your answer to their question. \n",
    "\n",
    "### Next Time\n",
    "\n",
    "We will start our journey into deep learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources consulted \n",
    "\n",
    "0. [Benchmarking your code](https://rbspy.github.io/benchmarking-your-code/)\n",
    "1. [IPython Magic Commands](https://jakevdp.github.io/PythonDataScienceHandbook/01.03-magic-commands.html)\n",
    "2. [Profiling and Timing Code](https://jakevdp.github.io/PythonDataScienceHandbook/01.07-timing-and-profiling.html)\n",
    "3. [How do I get time of a Python program's execution?](https://stackoverflow.com/questions/1557571/how-do-i-get-time-of-a-python-programs-execution)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
