{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8\n",
    "\n",
    "The next two labs concern developing the language around machine learning as well as an intuition for how these words are used. Today we will examine _prediction_ which, like classification, is a classic supervised learning task. Today's goals are:\n",
    "\n",
    "0. Articulate what prediction is (and how it is different from classification)\n",
    "1. Compare and contrast supervised and unsupervised learning from a procedural perspective\n",
    "2. Explain training vs. testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import block\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "In prediction, we aim to guess the output value given a set of input variables. In fact you've been doing \"supervised\" learning in many classes: linear regression. \n",
    "\n",
    "In its simpliest form, linear regression takes an input value $x$ and returns a value for $y$ using the formula $y = mx + b$, for some value of $m$ and some value of $b$. How do we find the values for $m$ and $b$? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Today's (fake) data\n",
    "\n",
    "For today, we are using another artifically created dataset based on [this example](https://paulvanderlaken.com/2017/09/27/simpsons-paradox-two-hr-examples-with-r-code/). \n",
    "\n",
    "In this example, we are consulting for _FakeData Inc_ and our job is to assist them in better understanding their employees. The CEO read an article that claimed that the higher a person's neuroticism score from the [Big Five personality](https://en.wikipedia.org/wiki/Big_Five_personality_traits) test the better their performance at work is. \n",
    "\n",
    "So the CEO has directed us to develop an algorithm that predicts a person's performance score as well as their salary based on their neuroticism score. \n",
    "\n",
    "\n",
    "Please import the data and check the shape of the data, but do **not** plot it _yet._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data \n",
    "employee = pd.read_csv(\"lab8data.csv\", delimiter = \",\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting with Two points\n",
    "\n",
    "Let's start with two people that a member of the C-suite at _FakeData Inc_ says are representative of many employees.\n",
    "\n",
    "Based on this information, find the values for $m$ and $b$ for the simple linear regression. Then plot these two points with the associated line for the create a line for these points. \n",
    "\n",
    "\n",
    "### Simple Linear regression in `sklearn`\n",
    "\n",
    "Unlike our usual procedure of building from scratch, we're going to just use the off the shelf `sklearn` implemenation for the linear regression. If you would like to better understand the theory for how and why this works, check out SDS 291 for all the details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting two employees\n",
    "inds2 = [413,791] \n",
    "justtwo = employee.iloc[inds2,:]\n",
    "input2 = justtwo[[\"neuroticism\"]].to_numpy()\n",
    "output2 = justtwo[[\"performance\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the particulars for the linear regression\n",
    "lm2 = linear_model.LinearRegression()\n",
    "\n",
    "# Fit the linear regression to the data\n",
    "model2 = lm2.fit(input2,output2)\n",
    "\n",
    "# Extract the coefficients\n",
    "m2 = lm2.coef_[0]\n",
    "b2 = lm2.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the points \n",
    "\n",
    "plt.scatter(input2, output2, s = 50)\n",
    "\n",
    "# Plotting your particular model/guessing function\n",
    "x = np.linspace(np.min(input2)-1, np.max(input2)+1, 1000)\n",
    "plt.plot(x, m2*x+b2, linestyle='dashed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have another person who has a neuroticism score of 6. Looking just at the plot above, what do you expect their performance score to be? \n",
    "\n",
    "**Your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the *predicted* performance score for this person:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_person_2score = m2*6 + b2\n",
    "print(new_person_2score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have their *actual* perfomance score; it is 75. Does this surprise you?   \n",
    "\n",
    "**Your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting with 4 points\n",
    "\n",
    "Let's add a few more employees to our dataset and recompute the simple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting four employees\n",
    "# This time select at random using random choice\n",
    "np.random.seed(2021)\n",
    "inds4 = np.random.choice(range(1000),4)\n",
    "\n",
    "justfour = employee.iloc[inds4,:]\n",
    "input4 = justfour[[\"neuroticism\"]].???\n",
    "output4 = justtwo[???].???\n",
    "\n",
    "# Set up the particulars for the linear regression\n",
    "lm4 = linear_model.LinearRegression()\n",
    "\n",
    "# Fit the linear regression to the data\n",
    "model4 = lm4.fit(???,???)\n",
    "\n",
    "# Extract the coefficients\n",
    "m4 = lm4.coef_[0]\n",
    "b4 = ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the points \n",
    "\n",
    "plt.scatter(???,???, s = 50)\n",
    "\n",
    "# Plotting your particular model/guessing function\n",
    "x = np.linspace(np.min(input2)-1, np.max(input2)+1, 1000)\n",
    "\n",
    "plt.plot(???, ???*x+???, linestyle='dashed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to our employee from before with a neuroticism score score of 6. Looking just at the plot above, what do you expect their performance score to be? \n",
    "\n",
    "**Your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the *predicted* performance score for this person:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_person_4score = ??*6 + ??\n",
    "print(new_person_4score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have their perfomance score; it is 75. Does this surprise you? \n",
    "\n",
    "**Your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing plots:\n",
    "\n",
    "Before moving on, let's quickly compare these two lines resulting from two different subsets of employees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the points \n",
    "plt.scatter(input4, output4, s = 50)\n",
    "plt.scatter(input2,output2, s=50, c = \"b\")\n",
    "\n",
    "# Plotting your particular model/guessing functions\n",
    "x = np.linspace(0, 7.5, 1000)\n",
    "plt.plot(x, m2*x+b2, linestyle='dashed')\n",
    "plt.plot(x, m4*x+b4, linestyle='dashed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about these two lines? Does one make more \"sense\" than the other?   \n",
    "\n",
    "Jot down a few notes. (Spend no more than 90 seconds thinking about this. We'll return to these ideas later.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning as a procedure\n",
    "\n",
    "This sequence of steps follows the general process of **supervised** learning:\n",
    "0. Decide what task we are doing (classification or prediction) and selet the algorithm/model we want to use\n",
    "1. Using part of the available data with their given outputs, fit algorithm/model\n",
    "2. Check the model against \"new\" data (ie. the data that we did **not** use in the fitting). Use the \"truth\" to evaluate the \"goodness\" of the model. \n",
    "\n",
    "In the example in this lab, we did these as follows:\n",
    "0. Decided to do _prediction_ using _simple linear regression_\n",
    "1. Used (input score, performance score) of two points to determine the $m$ and the $b$ for the simple linear regression. \n",
    "3. Made a prediction for a new person using only their input value and checked it against the true performance score. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervision\n",
    "\n",
    "The _supervision_ occurs in the Step 1, where we use the \"answers\" to help us set where the line should be. Without these answers/output/response variables, we could place a line in an number of places, but those placements would have nearly nothing to do with our goal of _predicting_ the performance score from the _whatever_ score. In prediction, we are seeking a model that is directly tied to _answer_ and as such we must use it as part of fitting the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Supervised and Unsupervised Learning (in steps) \n",
    "How does this sequence compare or contrast to _unsupervised learning_ where there is no supervision. Below are two outlines, one for general and one for a particular $k$-means example. Be sure to note where the supervision is _lacking_ in the general case, and then pick one such example to make the list of general steps more concrete. \n",
    "\n",
    "This sequence of steps follows the general process of supervised learning:\n",
    "0. Decide what task we are doing (**clustering/grouping** or **dimension reduction**) and select the algorithm/model we want to use\n",
    "1. (no supervision)\n",
    "2. Use **all** the data as input, and fit algorithm/model\n",
    "3. **Evaluating step** - we use something beyond our data, a metric of some kind\n",
    "\n",
    "In our $k$-means examples, we did these steps as follows:\n",
    "0. Specific Step 0\n",
    "1. Specific Step 1\n",
    "2. Specific Step 2   \n",
    "3. Specific Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare and contrast the steps for *supervised* and *unsupervised* machine learning\n",
    "\n",
    "**Wait here for a group discussion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Prediction Model\n",
    "\n",
    "Let's get a better sense of what we are working with. Make one plot of all the data with the neuroticism scores on one axis and the performance scores on the other axis. Place both of your models on this figure. \n",
    "\n",
    "Which is better? Why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all the data with both lines drawn on top of them\n",
    "\n",
    "# Plotting the data \n",
    "plt.scatter(???,???, s = 50)\n",
    "\n",
    "\n",
    "# Plotting your particular model/guessing functions\n",
    "x = np.linspace(0, 7.5, 1000)\n",
    "plt.plot(x, ??*x + ??, linestyle='dashed')\n",
    "plt.plot(???, ????, linestyle='dashed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Spot for your notes here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `predict()` to evaluate the model\n",
    "\n",
    "It would be nice if we could have a number of how \"good\" or \"bad\" our predictions are. \n",
    "\n",
    "Let's see what predictions our first model `lm2` would make for all employees (except the two used to build the model). \n",
    "\n",
    "We begin by creating a list of all the employees, except the two selected to build the model. We call this `emp_testset`. Then we use `predict()` for `lm2` on this subset to predict performance scores for all employees in `emp_testset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the indices associated with the two employees use to \n",
    "# create lm2 from our list of all employees\n",
    "test_inds = list(set(list(range(1000))) - set(inds2))\n",
    "\n",
    "# Create the subset of all employees except those two\n",
    "emp_testset = employee.iloc[test_inds,:]\n",
    "emp_testset = emp_testset[[\"neuroticism\",\"performance\"]].to_numpy()\n",
    "\n",
    "# Separate the data into the \"inputs\" (neuro) and output (performance)\n",
    "neuro = employee[[\"neuroticism\"]].to_numpy()\n",
    "perform = employee[[\"performance\"]].to_numpy()\n",
    "\n",
    "# Find the predicted values for performance\n",
    "predict2 = lm2.predict(neuro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of our results, create a scatter plot of the true performance scores against the predicted scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(perform, predict2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What a mess! This plot does not feel so infromative. \n",
    "\n",
    "\n",
    "Instead let's make a plot of the neuroticism scores with the ground truth scores as `x` and add a second plot on top of the neuroticism scores with the predictions in orange. This should give us a better visual for how well our line predicts the true values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original data\n",
    "plt.scatter(neuro, perform, marker = \"x\")\n",
    "\n",
    "# Plot predictions from lm2\n",
    "plt.scatter(neuro, predict2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks really off! But by how much? One method is to get a notion of how off each guess is from the ground truth on average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by computing the error for each individual person: \n",
    "error2_by_emp = np.abs(predict2 - perform)\n",
    "\n",
    "# Now let's add up all the errors for a notion of the total error: \n",
    "total_error2 = np.sum(error2_by_emp)\n",
    "\n",
    "# Find the average of how off our predictions are:\n",
    "average_mistake2 = total_error2/(1000-2)\n",
    "\n",
    "\n",
    "# Alternatively, we can directly find the average error using MEAN:\n",
    "# average_mistake2 = np.mean(np.abs(predict2 - perform))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the above process for our second model. Is this a better model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find predictions using lm4\n",
    "\n",
    "# Remove the indices associated with the two employees use to \n",
    "# create lm4 from our list of all employees\n",
    "test_inds = \n",
    "\n",
    "# Create the subset of all employees except those four\n",
    "emp_testset = \n",
    "emp_testset = \n",
    "\n",
    "# Separate the data into the \"inputs\" (neuro) and output (performance)\n",
    "neuro = \n",
    "perform = \n",
    "\n",
    "# Find the predicted values for performance\n",
    "predict4 = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot comparing neuroticism scores with the ground truth scores \n",
    "# and the neuroticism scores with the predictions. \n",
    "\n",
    "# Plot original data\n",
    "\n",
    "\n",
    "# Plot predictions from lm4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using whatever method you prefer, compute the average \n",
    "# error for the predicted values for perform\n",
    "\n",
    "\n",
    "average_mistake4 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training vs. Testing\n",
    "\n",
    "Notice that when we did our predictions, we only tested on the data points that we didn't use in our creation of the model. This is because the goal of supervised learning is to build a model that will work generally for your data and your context based on a few data points with \"answers\" included. The phase that uses just a few data points is called the **training** phase and the phase that compares the model's predictions to the ground truth is called the **testing** phase. \n",
    "\n",
    "In fact, we compute two kinds errors for each model: one in training and one in testing. We will talk more about training and testing later in the semester, and how specifically these training and testing computations impact the choices we make regarding data. \n",
    "\n",
    "Compute the **training error** for both models (i.e. the average difference between the predictions for the training data and the ground truth). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use predict() to get the predicted values for the input \n",
    "# values for the training set, ie. the employees that were\n",
    "# part of creating the model\n",
    "pred2 = lm2.predict(input2)\n",
    "\n",
    "# Compute the average of the difference between the \n",
    "# predictions and the real output (ie. from the data)\n",
    "te2 = np.mean(np.abs(pred2 - output2))\n",
    "\n",
    "# Repeat the above process for lm4:\n",
    "pred4 = \n",
    "te4 = \n",
    "\n",
    "print(\"training error for lm2\", te2, \"\\n\")\n",
    "print(\"training error for lm4\", te4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your 2 training computations and your two testing computations. Which model is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts\n",
    "\n",
    "To finish up this lab, create a plot for the data with three lines:\n",
    "1. The first should represent the direction and position of the first principal component\n",
    "2. The second is the linear regression (using all the data for training) with performance as the output\n",
    "3. The third is the linear regression (using all the data for training) with neuroticism as the output\n",
    "\n",
    "Share your plot in a post on **#lab08_submission** channel on slack and share something that you find odd you about your plot. Your post must start with **Lab8** to get credit. \n",
    "\n",
    "If your have questions from this lab, post them to #lab_questions with the same preamble (i.e. starting with **Lab8**). If you have the same question, please use one of the emoji's to upvote the question. If you would like to answer someone's question, please use the thread function. This will tie your answer to their question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References consulted\n",
    "0. _Doing Data Science: Straight talk from the frontline_ by C. O'Neil & R. Schutt (2014)\n",
    "1. _Python Machine Learning_\n",
    "2. [Simple and Multiple Linear Regression in Python](https://towardsdatascience.com/simple-and-multiple-linear-regression-in-python-c928425168f9)\n",
    "3. [Simple Line Plots](https://jakevdp.github.io/PythonDataScienceHandbook/04.01-simple-line-plots.html)\n",
    "3. [Simpson’s Paradox: Two HR examples with R code.](https://paulvanderlaken.com/2017/09/27/simpsons-paradox-two-hr-examples-with-r-code/)\n",
    "4. [Simpson’s Paradox data](https://itsalocke.com/datasaurus/reference/simpsons_paradox)\n",
    "5. [Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing](https://www.autodeskresearch.com/publications/samestats) <= making dino data\n",
    "6. [Simpson’s Paradox and Interpreting Data](https://towardsdatascience.com/simpsons-paradox-and-interpreting-data-6a0443516765)\n",
    "7. [list difference](https://stackoverflow.com/questions/6486450/python-compute-list-difference)\n",
    "8. [Visually differentiating PCA and Linear Regression](https://shankarmsy.github.io/posts/pca-vs-lr.html)\n",
    "\n",
    "\n",
    "##### Note\n",
    "There are two other notebooks in this directory. One shows you how I created the fake data and the second is my scratch work for the second half of the lab. I am including both here so that you can see what happens \"behind the scenes\" if you choose. Neither notebook are as well annotated as this one, nor are they ones that I would call \"final drafts.\" They are mostly notes that I used to create the notebook that you use for the lab. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
