{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4\n",
    "\n",
    "Last time, we worked towards a fully functional k-means. Today, we are going to look at a few issues concerning k-means:\n",
    "0. Discuss what _Stopping Conditions_ are and how to use them\n",
    "1. How to use the `sklearn` implementation for k-means\n",
    "2. How do we decide if we have \"good\" clusters?\n",
    "\n",
    "Today we will continue working with our Smith Students data. Please create a copy and place it in this Lab04 folder. Then create the `justtwo` dataframe with just _coffee_ and _sleep_ variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package Import block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your data\n",
    "students = \n",
    "\n",
    "\n",
    "# Create your subset\n",
    "justtwo = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping Conditions\n",
    "\n",
    "As a warm-up, let's start with a few questions about stopping conditions:\n",
    "   1. What are stopping conditions? \n",
    "   2. Why do we care about them?\n",
    "   3. Why are stopping conditions necessary for k-means?\n",
    "   \n",
    "* Write down at least 3 thoughts/ideas/questions about stopping conditions\n",
    "* Pick at least 1 to share with the class\n",
    "\n",
    "**Wait here for the group**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(For HW2, you will be asked to share your full implementation of k-means, with an explanation of your stopping conditions.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `sklearn`\n",
    "\n",
    "There are several important python packages for doing machine learning, but `sklearn` (or _scikit-learn_) is one of the most powerful. Unlike `numpy`, `pandas`, and `scipy` which serve to be used generally for numerical computations, data wrangling, and scientific computing, respectively, the `sklearn` package is specifically for applying machine learning algorithms to data. The implementations in `sklearn` seek to be optimized and this direct usage is considered to be \"off the shelf\". \n",
    "\n",
    "In this course, we will first program our own version of each algorithm and then switch to using the optimized versions. In a sense `sklearn` becomes our \"check\" after we have a deeper understanding of each method. \n",
    "\n",
    "_Note_ The phrase \"off the shelf\" refers to the analogy of going to a store and buying something that is designed to just work with little tinkering from the customer. When someone says they used \"off the shelf\" _method_ they mean that they did not modify the _method_ implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means in `sklearn`\n",
    "\n",
    "To use k-means in `sklearn`, there are two steps:\n",
    "1. Setting up how the k-means will function, including:\n",
    "   * How many clusters (i.e. the $k$).\n",
    "   * How you would like the clusters to be selected\n",
    "   * The starting random state\n",
    "   * The maximum number of iterations through loop of assigning clusters and finding the centers\n",
    "2. Applying the k-means you set up to your data\n",
    "\n",
    "This might seem odd, and perhaps you are wondering why _scikit-learn_ can't do this in one step. I would argue that in fact, this two-step procedure mirrors how we as humans approach coding k-means cold. In fact, note that during the last class, 1) we developed our k-means algorithm and then 2) we applied that code to our dataset. \n",
    "\n",
    "In the below code block, we have one possible setting of `KMeans()` from `sklearn`. \n",
    "* What _type_ is the output and what information is contained within `km_alg`? \n",
    "* What are the various inputs (called _parameters_ in the helpfiles) doing? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step one: Set up the k-means\n",
    "km_alg = KMeans(n_clusters=2, init=\"random\",random_state = 1, max_iter = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code block for further discovery\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our k-means set up, we _fit_ it to our data. There are a few ways to [fit data using k-means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.fit). In this course, we will use `.fit()` and `.predict()` most often.\n",
    "\n",
    "#### Using `.fit()`\n",
    "\n",
    "This first fitting applies k-means to the data. What _type_ is the output and what information is contained within `fit1`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Apply the set k-means to the data using .fit()\n",
    "fit1 = km_alg.fit(justtwo_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code block for further discovery\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not immediately obvious how to access the 1) cluster assignments, nor 2) the centers for each cluster. The result of `.fit()` wraps this information inside a class style object. We can use `.labels_` and `.cluster_centers_` to access this information: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Labels\\n\", fit1.labels_)\n",
    "\n",
    "print(\"\\n Cluster Centers \\n\",fit1.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `.predict()`\n",
    "\n",
    "If we discover more data that we want to compute the cluster label for **after** applying k-means, we can use `.predict()` on that point. For example, say your friend drinks an average of 1 cup of coffee and sleeps for 8 hours per night, which cluster do they belong to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_friend_label = km_alg.predict(np.array([[1,2]]))\n",
    "print(new_friend_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting our results\n",
    "\n",
    "Each time we apply a machine learning method, we should evaluate the results. One of the fastest and more visceral ways to do this is by plotting our results. Adapting the visualizations from [this example](https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html), we proceed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = fit1.labels_\n",
    "centers = fit1.cluster_centers_\n",
    "\n",
    "# Plot and color the points according to their label\n",
    "plt.scatter(justtwo_np[:,0], justtwo_np[:,1], c=labels, s=50, cmap=\"spring\")\n",
    "# Add the cluster centers on top\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', marker=\"x\", s=200, alpha=0.75)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is this what we expect?**\n",
    "\n",
    "In class, we discussed how we expected to see different clusters. We are likely seeing this due to the scale of the data. So you may decide that you need to scale the different variables so that they are on the same size of axis. \n",
    "\n",
    "### Scaling variables\n",
    "\n",
    "There are two big commonly used ways to scale variables: [normalizing and standardizing](https://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc). For this lab, we will _normalize_ all our variables which will place them within between 0 and 1. \n",
    "\n",
    "To _normalize_ variables, we need to find the minimum and maximum values for each one. Then we use them as follows:\n",
    "\n",
    "$Var_{norm} = \\dfrac{Var - Var_{min}}{Var_{max} - Var_{min}}$\n",
    "\n",
    "Instead of looping over all the rows for each variable, we can do this computation without a `for` loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing Coffee variable\n",
    "\n",
    "coffee = justtwo_np[:,0]\n",
    "mx = np.max(coffee)\n",
    "mn = np.min(coffee)\n",
    "\n",
    "coffee_norm = (coffee - mn)/(mx - mn)\n",
    "coffee_norm = np.around(coffee_norm, decimals = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the sleep variable \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your work with a scatter plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are happy with your plot, then create a new numpy array justtwo_norm using `np.stack()`\n",
    "\n",
    "justtwo_norm = np.stack((coffee_norm, sleep_norm),axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Aside_ - Here we have to use `stack()` because our arrays are 1-D arrays and not 2-D arrays. (If you run `.shape()` on them, only one number comes out.) If we had 2-D arrays, we would use `concatenate` with syntax like:\n",
    "\n",
    "`out_np = np.concatenate((twoD_array1, twoD_array2),axis=1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means + scaled variables\n",
    "\n",
    "The next \"check\" is whether our scaling did what we expect. Using `sklearn`, deploy kmeans on the normalized data and plot your results. Did you get what you expected?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run kmeans on your normalized data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Good\" Clusters\n",
    "\n",
    "When we have grouped our data into clusters, we would like to know if these are \"good\" clusters. We might want to look at the:\n",
    "* The spread of the cluster \n",
    "* How spread clusters are from each other \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Within Cluster Sum of Squares\n",
    "\n",
    "A common measure for clusters is the _within cluster sum of squares_ which is the total distance between each data point and the cluster center. Using `cdist` and `np.sum` within a `for` loop, we can compute this total. Below is just a bit of code to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within cluster sum of squares\n",
    "\n",
    "# Compute the following for each cluster:\n",
    "\n",
    "    cluster_spread = distance.cdist(cluster_points, cluster_center, 'euclidean')\n",
    "    cluster_total = np.sum(cluster_spread)\n",
    "    \n",
    "    \n",
    "# Add all the cluster_totals together\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** - As $k$ increases, what happens to the _within cluster sum of squares?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster separation \n",
    "\n",
    "In cluster separation, we seek to understand the average distance of points from one cluster to the nearest cluster. Again, we use `cdist` but this time we are comparing the points within one cluster to another cluster. We have to compute an average and repeat this process for each cluster. This computation may involve a nested `for` loop. Again, the below code block has a bit of code to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster separation\n",
    "\n",
    "# Compute the following for each cluster:\n",
    "cluster_sep_mat = np.zeros((num_clusts, num_clusts))\n",
    "for :\n",
    "    for :\n",
    "        cluster_ij = distance.cdist(cluster_points, ???, 'euclidean')\n",
    "        cluster_sep_mat[i,j] = np.sum(cluster_ij)/???\n",
    "    \n",
    "    # For each cluster determine which is the closest average\n",
    "\n",
    "# Determine either the average or total of the closest averages\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** - As $k$ increases, what happens to _cluster separation?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which way is \"right\"?\n",
    "\n",
    "In machine learning in general, but in unsupervised in particular, there is often not one right, perfect, or even universally accepted way to evaluate an algorithm. Here a careful eye on 1) your data, 2) your goals, and 3) the overall context of your data and goals is collectively important. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts\n",
    "\n",
    "To finish up this lab, play with different values for $k$ and decide what the correct value is for this dataset is. Create a post to **#lab_submission** channel on slack stating what you think the correct value for $k$ is with 1) your thinking and 2) a plot of the resulting clustering. Your post must start with **Lab4** to get credit. \n",
    "\n",
    "If your have questions from this lab, post them to #lab_questions with the same preamble (i.e. starting with **Lab4**). If you have the same question, please use one of the emoji's to upvote the question. If you would like to answer someone's question, please use the thread function. This will tie your answer to their question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources Consulted\n",
    "\n",
    "0. _Python Machine Learning_\n",
    "1. [k-means helpfile in sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "2. [In Depth: k-Means Clustering](https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html)\n",
    "3. [Colormaps in matplotlib (ie. `cmap`)](https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html)\n",
    "4. [Scatter Helpfile](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.scatter.html)\n",
    "5. [Standardize or Normalize? â€” Examples in Python](https://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc)\n",
    "6. [Concatenation of 2 1D `numpy` Arrays Along 2nd Axis](https://stackoverflow.com/questions/35401041/concatenation-of-2-1d-numpy-arrays-along-2nd-axis)\n",
    "7. [`stack()` helpfile](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.stack.html)\n",
    "8. [`concate()` helpfile](https://docs.scipy.org/doc/numpy/reference/generated/numpy.concatenate.html#numpy.concatenate)\n",
    "9. [Interpret all statistics and graphs for Cluster K-Means](https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/multivariate/how-to/cluster-k-means/interpret-the-results/all-statistics-and-graphs/#average-distance-from-centroid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
