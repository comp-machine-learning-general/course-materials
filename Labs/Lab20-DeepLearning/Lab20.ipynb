{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 20\n",
    "\n",
    "Today, we begin our journey into deep learning, by gathering the pieces of a neural net. Today'a goals are: \n",
    "\n",
    "0. Understand the components of a perceptron\n",
    "1. Code a threshold function\n",
    "2. Employ stochastic gradient descent to update information inside our perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports for today "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import pandas as pd\n",
    "\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the lab 14 data to be an example\n",
    "\n",
    "new_data = pd.read_csv(\"lab14data.csv\", sep = \",\")\n",
    "new_data[\"group\"] = -1*new_data[\"group\"]\n",
    "new_data_np = new_data.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neurons\n",
    "\n",
    "The idea behind neural networks is to build a system that works similar to how our brains function. Brains are a complex network of [_neurons_](https://en.wikipedia.org/wiki/Axon#/media/File:Neuron.svg) which -- thanks to neuroscience -- we understand to work as follows:\n",
    "\n",
    "0. Take in information via their [dendrites](https://en.wikipedia.org/wiki/Dendrite)\n",
    "1. Synthsize this information via the nucleus \n",
    "2. Activate (or not) and send this sythesis down the axon \n",
    "\n",
    "Artificial neurons work very much in this manner. Beginning with the simplest example, a classification perceptron works as follows: \n",
    "\n",
    "0. The information that is taken is the _input_ variables that we feed it. \n",
    "1. To synthesize this information, we use a weighted sum. The weights are determined through training\n",
    "2. Finally, we use a threshold function to determine the positive or negative class assignment. This is the _output._\n",
    "\n",
    "Like a brain neuron that is constantly updating and learning, our preceptrons also update constantly by comparing the guessed classes to the true classes. Today we will focus on coding just one perceptron before we consider larger examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Perceptron\n",
    "\n",
    "Training a perceptron is an iterative training process. In this process, we are aiming to set:\n",
    "\n",
    "* The weights for the inputs\n",
    "* The **bias** unit\n",
    "\n",
    "### Weighting function\n",
    "\n",
    "_Note:_ I find the first image in [this blog post](https://towardsdatascience.com/perceptron-learning-algorithm-d5db0deab975) to be particularly helpful for visualizing what is going on in this part. \n",
    "\n",
    "The role of the weighting function is to determine how much attention we should pay to each input to build a successful classifier. This means that for each input, we have a weight $w$. The output of the weighting function is a sum of the weights times their respective input. For a given data point $d$ with $n$ variables represented as $(d_1, d_2 \\ldots d_n)$, we write \n",
    "\n",
    "\\begin{equation}\n",
    "\\widetilde{WF}(d) = (w_1*d_1) + (w_2*d_2) + \\cdots + (w_n*d_n)\\\\\n",
    "\\hspace{-3 cm} = \\Sigma_{i} (w_i*d_i) \n",
    "\\end{equation}\n",
    "\n",
    "The result is a weighted sum of each variable of the input. The higher a weight, the more attention that we pay to the corresponding input in our perceptron. \n",
    "\n",
    "In the below code block, create a function `weight_function` that takes in weights and **one** data point (as a row) and computes the weighted sum for that data point. Note that the number of weights is the is the same as the number of variables in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your weight_function here\n",
    "def weight_fuction(weights, datapoint):\n",
    "    \"\"\"Compute the weighted sum\"\"\"\n",
    "    pass #return weighted_sum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thresholding \n",
    "\n",
    "So far we have completed the \"sensing\" and \"synthesizing\" steps of a neuron, with our inputs then collated through the weighting function. Our next step is to decide whether to send a \"positive\" or \"negative\" class assignment. This is determined via a _thresholding function._ Simply put, if our weighted sum is above a value $\\theta$, then we assign the positive class and if it is less than $\\theta$, then we assign a negative class. \n",
    "\n",
    "In the below code block, write a function `thresh` that takes in one weighted sum and one singular value for $\\theta$. The output should be either `-1` and `1` where the negative class is assigned when the weighted sum is less than $\\theta$ and where the positive class is assigned otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your thresh function here\n",
    "def thresh(weighted_sum, thresh):\n",
    "    pass #return assigned_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias unit\n",
    "\n",
    "To do this thresholding, we have to have a value in mind for $\\theta$. When we consider that we are setting both the weights for our weighted sum as well as the value that they will be compared against, this all feels a bit circular. \n",
    "\n",
    "Also, instead of thresholding based on some tuned value, it would easier if we could just have the positive class relate to when the weighted sum is positive and the negative class when the weighted sum is negative. In symbols, instead of $WF(d) \\geq \\theta$, we want to make the comparison: $WF(d) - \\theta \\geq 0$. \n",
    "\n",
    "Here we use a trick to give us an adjusted weighting function to make this more desirable comparison possible. For each data point, _pad_ it with a 1. So a given data point $d = (d_1, d_2 \\ldots d_n)$ becomes $(1, d_1, d_2 \\ldots d_n)$. This padded vector allows us to adjust our weighted sum as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "WF(d) = (-\\theta*d_0) + \\Sigma_{i = 1}^n (w_i*d_i) \n",
    "\\end{equation}\n",
    "\n",
    "If we view $(-\\theta)$ as the zeroth weight, then we have our weighting and our thresholding in the same step. Although, we can view $(-\\theta)$ as a weight, this goes by another name: the **bias unit.** \n",
    "\n",
    "Create a function `weighted_sums` that takes in weights (including the bias value) and **one** data point (as a row) It should pad the data vector and compute the expanded weighted sum for that data point. \n",
    "\n",
    "Then create an updated `threshold` function that assigns a positive or negative class based on the value of the weighted sum. \n",
    "\n",
    "Typically $WF(d)$ is plainly referred to as $z$ and the thresholded value is written as $\\phi(z)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your weighted_sums here\n",
    "\n",
    "def weight_fuction(weights, datapoint):\n",
    "    \"\"\"Compute the weighted sum\"\"\"\n",
    "    pass #return weighted_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your threshold function here\n",
    "\n",
    "def thresh(weighted_sum, thresh):\n",
    "    pass #return assigned_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating our weights and bias\n",
    "\n",
    "At this point, we have functions that allow us to feed weights and data in and get back classifications. But we have yet to _learn_ the weights and bias. To do this, we engage with a familiar idea: stochastic gradient descent (or SGD). Using SGD, we start with some guesses for our weights and find the optimal ones by using one datapoint at a time. \n",
    "\n",
    "Critical to any gradient descent (including SGD) is the _update step._ The updates for preceptron weights (and bias) happen in the usual manner with $W_\\textrm{new} = W_\\textrm{old} + \\Delta W$. The change for each weight is governed by the [**perceptron learning rule**](http://hagan.okstate.edu/4_Perceptron.pdf) given by: \n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta W_j = \\eta * (truth - guess)*(j^{th} \\textrm( input))\n",
    "\\end{equation}\n",
    "\n",
    "With $\\eta$ as the learning rate and the $j^{th} \\textrm( input)$ referring to the $j^{th}$ variable of a datapoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examing Truth and Guess\n",
    "\n",
    "In the above set-up, we assume that we are feeding our perceptron one data point at a time. Before coding this, let's explore the potential cases and if there is a change for our weights (ie. is $\\Delta W_j \\neq 0$):\n",
    "* True class is 1, guess is 1 $\\rightarrow$ $\\Delta W_j$ changes/does not change\n",
    "* True class is 1, guess is -1 $\\rightarrow$ $\\Delta W_j$ changes/does not change\n",
    "* True class is -1, guess is 1 $\\rightarrow$ $\\Delta W_j$ changes/does not change\n",
    "* True class is -1, guess is -1 $\\rightarrow$ $\\Delta W_j$ changes/does not change\n",
    "\n",
    "Putting the above all together: Will the weights change for each data point?   \n",
    "**Your thoughts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding changes in the weights\n",
    "\n",
    "In the next two code blocks, we will code the `change_w_i` function that will adjust the $i^{\\textrm{th}}$ weight. Then we will code `change_w` to update all the weights in your perceptron. \n",
    "\n",
    "As a reminder, we eventually be tuning our perceptron using SGD. This means that updating our weights will use exactly one datapoint at a time and **not** the whole dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your change_w_i here\n",
    "def change_w_i(data_true_cls, data_guess_cls, w_i):\n",
    "    pass #return new_wi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your change_w here\n",
    "def change_w(data_true_cls, data_guess_cls, all_weights):\n",
    "    pass #return new_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "Keeping the idea stochastic gradient descent in mind, code `preceptron` that takes in: \n",
    "\n",
    "0. A dataset\n",
    "1. The max number of epochs   \n",
    "\n",
    "This function should cycle through each epoch selecting a random datapoint each time (but not reusing one until the next epoch), and eventually outputing the weights and the bias term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your perceptron here\n",
    "def perceptron(data, max_epochs):\n",
    "    # Set your learning rate\n",
    "    \n",
    "    # Initial starting parameters\n",
    "    \n",
    "    # Create an epoch loop\n",
    "    for something in something: \n",
    "        # Randomly shuffle your data\n",
    "        \n",
    "        # Loop over shuffled data\n",
    "        for thatthing in thisset: \n",
    "            \n",
    "            # Compute a guess class for your current datapoint\n",
    "            # given the weights that you have \n",
    "            \n",
    "            # Update your weights with this one data point\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts\n",
    "\n",
    "To finish up this lab, answer the question: **Do you agree the perceptrons are kind of like brain neurons?** Share your thoughts in a post on **#lab-20-submission** channel on slack with your answer.  \n",
    "\n",
    "If your have questions from this lab, post them to #lab_questions. If you have the same question, please use one of the emoji's to upvote the question. If you would like to answer someone's question, please use the thread function. This will tie your answer to their question. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources consulted \n",
    "\n",
    "0. _Python Machine Learning_\n",
    "1. [An Introduction to Python Machine Learning with Perceptrons](https://www.codementor.io/mcorr/an-introduction-to-python-machine-learning-with-perceptrons-k7pn85vfi)\n",
    "2. [Neural Network Design, Chapter 4](http://hagan.okstate.edu/4_Perceptron.pdf)\n",
    "3. [Perceptron Learning Algorithm: A Graphical Explanation Of Why It Works](https://towardsdatascience.com/perceptron-learning-algorithm-d5db0deab975)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
