{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 15\n",
    "\n",
    "Today, we will continue our discussion about **_Support Vector Machines_** and discuss kernel methods. Today's goals are: \n",
    "\n",
    "0. Define support vectors\n",
    "1. Operationalize \n",
    "2. Use `sklearn` build an SVM\n",
    "3. Define kernels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last time - Motivating SVM \n",
    "\n",
    "Last lab, we looked at Maximal Margins for nicely (or linearly) separated data. Then we explored how to \"relax\" our desire to have a \"hard\" boundary. This softening is called the _soft-margin classification._\n",
    "\n",
    "Today we will add a bit more terminology to our discussion and use the `sklearn` implementation for SVM to classify our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports for Today\n",
    "\n",
    "Today we are working with two (constructed) datasets from Lab 14. \n",
    "\n",
    "We begin as usual, importing the packages and data that we need. Plot the first dataset `new_data` and note at least 2 observations about the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import block\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import pandas as pd\n",
    "\n",
    "import random \n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for later use\n",
    "\n",
    "def plot_svc_decision_function(SVM, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # Create grid to evaluate SVM model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = SVM.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # Plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # Plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(SVM.support_vectors_[:, 0],\n",
    "                   SVM.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=1, facecolors='none', edgecolors='k');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "# Adapted from https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html\n",
    "\n",
    "def plot_supports(SVM, ax=None):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # Plot support vectors\n",
    "    ax.scatter(SVM.support_vectors_[:, 0], \n",
    "               SVM.support_vectors_[:, 1],\n",
    "               s=300, linewidth=1, facecolors='none', edgecolors='k');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For function testing \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Data\n",
    "\n",
    "new_data = pd.read_csv(\"lab14data.csv\", sep = \",\")\n",
    "new_data[\"group\"] = -1*new_data[\"group\"]\n",
    "new_data_np = new_data.to_numpy()\n",
    "\n",
    "extra_data = pd.read_csv(\"extra_lab14data.csv\", sep = \",\")\n",
    "extra_data[\"group\"] = -1*extra_data[\"group\"]\n",
    "extra_data_np = extra_data.to_numpy()\n",
    "\n",
    "data_all = np.vstack([new_data_np,extra_data_np])\n",
    "\n",
    "## x-values for plotting\n",
    "x = np.linspace(-3.1,3.15,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot new_data\n",
    "#\n",
    "# What do you notice? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inflating the green boundary:\n",
    "inflate_width = 0.8\n",
    "\n",
    "plt.scatter(new_data[\"myst\"],new_data[\"mysty\"])\n",
    "plt.plot(x,x,'-k')\n",
    "plt.fill_between(x, x - inflate_width, x + inflate_width, \n",
    "                 edgecolor='none', color='#AAAAAA', alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions \n",
    "\n",
    "Last time, we played with existing boundary options and added a margin around it. Typically in SVM, we find the points that are closest to the line (plane or hyperplane) that will separate the two groups. These points are called the _support vectors._ \n",
    "\n",
    "One of the margins is the positive margin, assigning all data points on its side the class `1` (which means one margin width above the bounary). The other margin is the negative margin assigning all all data points on its side the class `-1` (which means one margin width below the boundary). The central boundary line is called the _decision boundary._\n",
    "\n",
    "For each class, there will be support vectors for class `1` and support vectors for class `-1`. They will be sitting on the edge of their respective side of the margin. Running equally between them will be the decision boundary. \n",
    "\n",
    "Any points within the the boundary are called _slack variables_ which fall into two groups:\n",
    "* Those on the wrong \"side\" of the decision boundary\n",
    "* Those on the  right \"side\" of the line BUT are within the margin\n",
    "\n",
    "Slack variables accumulate penalties (or costs) denoted as $\\xi_i$ (pronounced \"ex-cee\") as follows:\n",
    "* Those on the wrong \"side\" of the decision boundary -- **greater than 1**\n",
    "* Those on the  right \"side\" of the line BUT are within the margin -- **greater than 0, but less than 1**\n",
    "\n",
    "In fact, we can extend these costs (or $\\xi_i$) to all points: \n",
    "* Data points exactly on the margin (ie. the support vectors) -- **there is no penalty**\n",
    "* Data points on the \"right\" side of the line and not on the margin -- **there is no penalty**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penalty terms \n",
    "\n",
    "In SVM we are trying to balance two desires:\n",
    "* Have the biggest possible margin, ie. $margin~size$\n",
    "* Limit the costs of the slack variables ie. $\\sum \\xi_i$\n",
    "\n",
    "The latter is what is known as the _penalty term._ \n",
    "\n",
    "In our balancing, we may want to limit (or increase) the effect of the penalty term. We control this with a parameter $C>0$, rewriting our penalty term as $C \\sum \\xi_i$. \n",
    "* If $C=0$, how much impact does the penalty term have?\n",
    "* If $C$ is large, how much impact does the penalty term have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(A plce for your notes on the above questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM output\n",
    "\n",
    "The goal of SVM is to find the decision boundary that maximizes the margin subject to the penalty term. So we need a way to encode the distance from points to the decision boundary, which the dot product between the coefficients of a line with the associated components of the data points gives us. This means that (after some serious multivariable calculus) we can define the classifier resuling from linear SVM as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{classifier}(x) = \\beta_0 + \\sum_{i} \\alpha_i (x\\cdot x_i)\n",
    "\\end{equation}\n",
    "\n",
    "To estimate the values for all the $\\alpha_i$'s we need to take the dot product between our new point and each of the data points. However, SVM only relies on the support vectors, which means that $\\alpha_i = 0$ unless the $i^{\\textrm{th}}$ data point is a support vector. So the SVM classifier can be written as: \n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{classifier}(x) = \\beta_0 + \\sum_{s, \\textrm{support vectors}} \\alpha_s (x\\cdot x_i)\n",
    "\\end{equation}\n",
    "\n",
    "_Note:_ This markdown block is based heavily on page 351 in _ISLR._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM in  `sklearn` \n",
    "\n",
    "Let's step back from the terminology and see what we actually get with SVM. We begin with the nicely separated data from teh first part of Lab 14. In the later part of this section, we will look the full dataset (with the points that seem to mix). \n",
    "\n",
    "Just with our previous algorithms in `sklearn` there is a phase of determining the details of our models and then fitting that model to the actual data. \n",
    "\n",
    "**Note:** SVM is can be called `SVC` for _support vector classifier._\n",
    "\n",
    "_note:_ this may take a second to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate our data into inputs and class labels\n",
    "inputs = new_data_np[:,:2]\n",
    "classes = new_data_np[:,2]\n",
    "\n",
    "# Create the details of the model\n",
    "# Note that C is very large here! \n",
    "first_SVM = SVC(kernel=\"linear\", C=1E10)\n",
    "\n",
    "# Fit the model to our data\n",
    "first_SVM.fit(inputs, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputs of SVM\n",
    "\n",
    "We've talked about how SVM relies on the support vectors. Let's see which of our data points are the support vectors. First we will print the numerical information and then plot our data with the support vectors denoted with circles around the points: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_SVM.support_vectors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(inputs[:, 0], inputs[:, 1], c=classes, s=50, cmap='autumn')\n",
    "plot_supports(first_SVM);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the above plot surprise you? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the decision boundary with its margins (and the support vectors still marked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(inputs[:, 0], inputs[:, 1], c=classes, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(first_SVM, plot_support=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick explanation about this picture \n",
    "\n",
    "One way to think about support vectors is that they are the smallest number of points that uniquely define the boundary. The smallest number that you can possibly have is 3: two for one class and one for the other class. \n",
    "\n",
    "Suppose that you only had two support vectors: one per class. There would be infinitely many lines that would be halfway between these two points. The addition of the third support vector effectly chooses the tilt (or slope) of the boundary that is halfway between the two support vectors in one class and the remaining support vector. \n",
    "\n",
    "Notice in this picture that the line defined by the two red support vectors is parallel to the boundary and the other margin. The existence of the two red support vectors is determining that tilt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your turn\n",
    "\n",
    "So far we've only used the nicely separated data with one value for $C$. Repeat this process with $C=0$ and with two other values for $C$. Note your observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the details of the model\n",
    "# Set C = 0 \n",
    "second_SVM = SVC(kernel=\"linear\", C=1E-14)\n",
    "\n",
    "# Fit the model to our data\n",
    "second_SVM.fit(inputs, classes)\n",
    "\n",
    "# Plot the decision boundary with the supports labeled \n",
    "plt.scatter(inputs[:, 0], inputs[:, 1], c=classes, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(second_SVM, plot_support=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the details of the model\n",
    "# Set C equal to another positive value \n",
    "third_SVM = SVC(kernel=\"linear\", C=???)\n",
    "\n",
    "# Fit the model to our data\n",
    "third_SVM.fit(???,???)\n",
    "\n",
    "# Plot the decision boundary with the supports labeled \n",
    "plt.scatter(inputs[:, 0], inputs[:, 1], c=???, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(????, plot_support=???);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the details of the model\n",
    "# Set C equal to another positive value \n",
    "fourth_SVM = ???\n",
    "\n",
    "# Fit the model to our data\n",
    "\n",
    "\n",
    "# Plot the decision boundary with the supports labeled \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next using the `data_all` variable, repeat this process with two of the values of $C$ from above. What happens? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate our data into inputs and class labels\n",
    "inputs = data_all[:,:2]\n",
    "classes = data_all[:,2]\n",
    "\n",
    "# Create the details of the model\n",
    "# Note that C is very large here! \n",
    "SVM1 = SVC(kernel=\"linear\", C=1E10)\n",
    "\n",
    "# Fit the model to our data\n",
    "SVM1.fit(inputs, classes)\n",
    "\n",
    "# Plot the decision boundary with the supports labeled \n",
    "plt.scatter(inputs[:, 0], inputs[:, 1], c=classes, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(SVM1, plot_support=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the details of the model\n",
    "# Set C = 0 \n",
    "SVM2 = SVC(kernel=\"linear\", C=???)\n",
    "\n",
    "# Fit the model to our data\n",
    "SVM2.fit(??,???)\n",
    "\n",
    "# Plot the decision boundary with the supports labeled \n",
    "plt.scatter(inputs[:, 0], inputs[:, 1], c=???, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(SVM1, plot_support=???);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the details of the model\n",
    "# Set C equal to another positive value\n",
    "SVM3 = SVC(kernel=\"linear\", C=???)\n",
    "\n",
    "# Fit the model to our data\n",
    "SVM3.????\n",
    "\n",
    "# Plot the decision boundary with the supports labeled \n",
    "plt.scatter(???)\n",
    "plot_svc_decision_function(???, ???);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the details of the model\n",
    "# Set C equal to another positive value\n",
    "SVM4 = ???\n",
    "\n",
    "# Fit the model to our data\n",
    "\n",
    "\n",
    "# Plot the decision boundary with the supports labeled \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code block for you "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels\n",
    "\n",
    "Recall that the SVM classifier can be written as: \n",
    "\n",
    "\\begin{equation}\n",
    "classifier(x) = \\beta_0 + \\sum_{s, \\textrm{support vectors}} \\alpha_s (x\\cdot x_i)\n",
    "\\end{equation}\n",
    "\n",
    "The dot product is an example of a kernel function, in that it quantifies the similarity between two data points. If two data points are very similar their dot product will be close to the lengths of those two data points multiplied together. If they are completely different, then their dot product will be close to 0. \n",
    "* Another way to think of this is as vectors. If two vectors go in the same direction, their dot product is high. If they go in perpendicular directions, their dot product is 0. \n",
    "\n",
    "Anything that is a kernel can be written as $K(x,y)$ where the function $K$ tells us how to compute the similarity between two datapoints $x$ and $y$. This means that we can rewrite the SVM classifier as: \n",
    "\n",
    "\\begin{equation}\n",
    "classifier(x) = \\beta_0 + \\sum_{s, \\textrm{support vectors}} \\alpha_s K(x, x_i)\n",
    "\\end{equation}\n",
    "\n",
    "The kernel just defines how we think about similarity between two points. \n",
    "\n",
    "The **kernel trick** is where we transform our data into a higher dimension based on the kernel that we've chosen. One popular one is the _radial basis function_ which effectively pulls data into a higher dimension such that the height that a data point is at is based on its distance from the origin. The _radial basis function_ is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "rbf(x) = e^{-(x\\cdot x)}\n",
    "\\end{equation}\n",
    "\n",
    "Let's see this kernel trick in practice.\n",
    "\n",
    "(Following example from [In-Depth: Support Vector Machines](https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a circular dataset\n",
    "circ_data, circ_class = make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "# Plot our new data\n",
    "plt.scatter(circ_data[:, 0], circ_data[:, 1], c=circ_class, s=50, cmap='autumn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM\n",
    "\n",
    "First let's apply a linear SVM to this dataset. In other words, let's try to separate this data with a straight line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the details of the model\n",
    "line_SVM = SVC(kernel=\"linear\", C=1E6)\n",
    "\n",
    "# Fit the model to our data\n",
    "line_SVM.????\n",
    "\n",
    "# Plot the decision boundary with the supports labeled \n",
    "plt.scatter(???)\n",
    "plot_svc_decision_function(???, ???);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do you notice? \n",
    "\n",
    "Is using a linear SVM a good idea? Why or why not?\n",
    "\n",
    "*Your thoughts*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel SVM\n",
    "\n",
    "Now let's try applying the radial basis function. Recall that this kernel is equivalent to:\n",
    "\n",
    "1. stretching our data into a higher dimension such that the points closer to the origin will pull away from those further away, and then...    \n",
    "2. Computing linear SVM on the points in this higher dimensional space, and then ...    \n",
    "3. Pushing the data back to the original space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', C=1E6)\n",
    "clf.fit(circ_data, circ_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(circ_data[:, 0], circ_data[:, 1], c=circ_class, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(clf, plot_support=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next time\n",
    "\n",
    "Next time, we will look at another kind of classification algorithm called **decision trees**. Later in the week, we will explore _ensemble methods._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts\n",
    "\n",
    "To finish up this lab, using the `data_all` variable, explore polynomial kernels from degree 2 to 5 using the sequence of three flags: `kernel='poly', degree=#, gamma='auto'`:\n",
    "\n",
    "`pd_SVM = SVC(kernel='poly', degree = 2, gamma = 'auto')`\n",
    "\n",
    "\n",
    "Do you think these polynomial kernels do a better job? Why or why not? Plot the one that you think is best for the overlapping data. Share your plot and thoughts in a post on **#lab15_submission** channel on slack with your answer. Your post must start with **Lab15** to get credit.  \n",
    "\n",
    "If your have questions from this lab, post them to #lab_questions with the same preamble (i.e. starting with **Lab15**). If you have the same question, please use one of the emoji's to upvote the question. If you would like to answer someone's question, please use the thread function. This will tie your answer to their question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources consulted \n",
    "\n",
    "0. [ISLR](https://hastie.su.domains/ISLR2/ISLRv2_website.pdf)\n",
    "1. [SDS 293 Notes by R. Jordan Crouser](http://www.science.smith.edu/~jcrouser/SDS293/)\n",
    "2. [In-Depth: Support Vector Machines](https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html)\n",
    "3. [SVM in `sklearn`](https://scikit-learn.org/stable/modules/svm.html)\n",
    "4. [Plot different SVM classifiers in the iris dataset](https://scikit-learn.org/stable/auto_examples/svm/plot_iris_svc.html)\n",
    "5. _Pattern Recognition and Machine Learning_ C. Bishop (2006)\n",
    "6. [Stackoverflow regarding `samples_generator`](https://stackoverflow.com/questions/65898399/no-module-named-sklearn-datasets-samples-generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
