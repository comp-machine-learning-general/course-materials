{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6\n",
    "\n",
    "Today we continue exploring **dimension reduction** to help us get a handle on large dimensional data. Today's goals are:\n",
    "\n",
    "0. Understand how to use the `sklearn` implementation for PCA\n",
    "1. Determine what the \"right\" lower dimension is\n",
    "2. Introduce _Singular Value Decomposition_\n",
    "3. Compare and contrast PCA and SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import block\n",
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easier comparisons, we will continue with the ever exciting `students_info.csv` file. Please import this in `pandas` and then create a `numpy` array with only the numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "students = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create justnum with only the numerical data\n",
    "justnum = students[[\"coffee\", \"sleep\", \"gym\", \"gpa\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "\n",
    "Recall that for PCA, we need to have normalized data. Before continuing, create `justnum_std` that _standardizes_ each variable. (Hint: you might want to grab this from another lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing our variables:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your creation of justnum_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may want to create a few plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another block for exploration \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA in `sklearn`\n",
    "\n",
    "Like kmeans, PCA is implemented in `sklearn` and it works much the way that kmeans did: there's a set-up phase and then an application phase. We first set up how many components we are looking for with PCA. We then apply that particular PCA that we have crafted to our data. \n",
    "\n",
    "As we did with kmeans, we will take each step individually, exploring the output that we generate in each stage. \n",
    "\n",
    "In the below code block, we have one possible setting of `PCA()` in `sklearn`.\n",
    "* What _type_ is the output and what information is contained within `PCA`? \n",
    "* What are the various parameters doing? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step one: Set up PCA by defining the number of components \n",
    "pca_alg = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code block for further discovery\n",
    "type(????)\n",
    "\n",
    "\n",
    "# Try displaying the result from above. \n",
    "#       What can you read easily and what is hidden? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `dir` for further exploration\n",
    "\n",
    "We can use `dir()` to actually peek inside what kinds information is contained within `pca_alg`. The output from `dir()` will just list the attributes and names within the listed input. \n",
    "\n",
    "**WARNING:** `dir` works differently in the **terminal** (or command prompt or anaconda prompt) that you use to access your notebooks. \n",
    "\n",
    "Let's see this in operation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(pca_alg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with kmeans, now that we have set up our PCA, we can _fit_ it to our data. We can `fit` our data and then `transform` it; or if we prefer, we can do both using `fit_transform`. In the next section, we will explore these three functions:   \n",
    "* `fit()`    \n",
    "* `transform()`    \n",
    "* `fit_transform()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `.fit()`\n",
    "\n",
    "This first fitting applies our PCA to the data. What _type_ is the output and what information is contained within `pfit`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfit = pca_alg.fit(justnum_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code block for further discovery\n",
    "type(????)\n",
    "\n",
    "\n",
    "# Try displaying the result from above. \n",
    "#       What can you read easily and what is hidden? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not immediately obvious how to access the principal components. The result of `.fit()` wraps this information inside a class style object. We can use `.components_` to access the principal components: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the resulting components\\n\", pfit.components_.shape)\n",
    "\n",
    "print(\"\\n Actual components \\n\",pfit.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this what we expect to see? Why or why not? \n",
    "\n",
    "**Take a minute to explain this to a neighbor.** Then working with your fellow machine learning practitioner, try to condense your explanation into something you could post on social media (like a tweet or a meme). \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA `.fit` result\n",
    "\n",
    "The output of `fit()` is a _transition_ matrix. This is the matrix that \"carries\" our data from a higher dimension down to the lower one. What `.fit` does **not** do is carry out this tranformation. For that step, we need to use `.transform()`. \n",
    "\n",
    "\n",
    "#### Using `.transform()`\n",
    "\n",
    "So far, we have set-up our PCA and applied it to our data to get our transition matrix. To actually send our data to the lower dimensional space, we _transform_ our data using `transform()`. \n",
    "\n",
    "\n",
    "_Note_ - Both `.predict` for kmeans and `transform` for PCA in `sklearn` are similar in the sense of _extending_ the common applications of their algorithms respectively. However, in the case of PCA, simply stopping at the _transition_ matrix feels a bit odd as the _dimension reduction_ has not yet occurred. To complete the dimension reduction, we use `transform()`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before moving on:** Write in **your** own words:  \n",
    "1. What is `.fit()`?   \n",
    "2. What is `.transform()`?  \n",
    "3. How are they different? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justnum_intwo = pca_alg.transform(justnum_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the new lower dimensional result. - \n",
    "#     What is the type and shape of our output? \n",
    "type(????)\n",
    "\n",
    "\n",
    "# Try displaying the result from above. \n",
    "#       What can you read easily and what is hidden? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2D plot of the new lower dimensional result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this what you expect given what you know about this data? Why or why not? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `fit_transform()` \n",
    "\n",
    "We can fit our PCA to our data and do the dimension reduction in one step using `fit_transform()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justnum_intwo_onestep = pca_alg.fit_transform(justnum_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a layer to compare the previous result to this one. \n",
    "# Your goal with this plot is to confirm that you have the same result\n",
    "\n",
    "# Create the plot from above with larger but fainter circles: \n",
    "plt.scatter(justnum_intwo[????], justnum_intwo[????], s = 100, alpha = 0.5)\n",
    "\n",
    "# Layer with the new variable represented with little x markers\n",
    "plt.scatter(????, ????, marker = \"x\", c = 'k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aside - `transform` is just matrix multiplication\n",
    "\n",
    "All that `.transform` is doing is _right_ multiplying our data with the transition matrix. In fact, if we define `trans_mat` as follows, we can "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_mat = pfit.components_\n",
    "justnum_intwo_mult = np.dot(justnum_std,trans_mat.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the shape is what we expect\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a layer to compare the previous result to this one. \n",
    "# Your goal with this plot is to confirm that you have \n",
    "#      the same result using multiplication\n",
    "# Hint - It should look exactly like the above one, but \n",
    "#        with a different variable in the last line\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrapping up PCA \n",
    "\n",
    "So far, we have applied PCA to a dataset of two-dimensions. This seems a bit odd since most computers can easily visualize data in two-dimensions. So this kind of dimension reduction feels unnecessary. And you're totally right! It is unnecessary from a machine learning perspective. **But** we did this dimension reduction precisely because we can see each part. We can visualize the starting point in 2-D, see the result in 1-D, and compare the two. \n",
    "\n",
    "Before moving on, apply PCA to `justnum_std` for 1, 3, and 4 components. That is, create a 1-D starting point (or a dataset with just one column) and do PCA. Then repeat this process for a 3-D starting point (or a dataset with three columns) and for a 4-D starting point. \n",
    "\n",
    "Then create visualizations for 1 and 3 dimensions. Considering the two dimensional ones that we created, what is the right number of components? **Justify** your choice. \n",
    "\n",
    "Below are a few code blocks for your experiments and plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to justnum_std for 1 component\n",
    "\n",
    "# Set up PCA with 1 component \n",
    "pca_alg1 = PCA(n_components=??)\n",
    "\n",
    "# Fit PCA to the data\n",
    "pfit = pca_alg.fit(???)\n",
    "\n",
    "# Project the data onto the resulting components\n",
    "justnum_inone = pca_alg.???(???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize result of PCA with 1 component applied justnum_std\n",
    "plt.scatter(????,np.zeros(???))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to justnum_std for 3 components\n",
    "\n",
    "# Set up PCA with 3 components\n",
    "pca_alg1 = PCA(n_components=??)\n",
    "\n",
    "# Fit PCA to the data and transform it in one step\n",
    "justnum_inthree = pca_alg.???(???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize result of PCA with 3 components applied justnum_std\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "# Create the SCATTER() plot with colors\n",
    "ax.scatter(???,???,???);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to justnum_std for 4 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why is there no visulization block for this one? \n",
    "# your thoughts - \n",
    "\n",
    "\n",
    "\n",
    "# Yes this is intentionally a code block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about your data? What is easier to see in 1, 2, or 3 D projections? \n",
    "\n",
    "**Wait here for a group discussion** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the _lower_ dimension\n",
    "\n",
    "The goal of PCA is a dimension reduction. The question becomes how far should we reduce? What is the \"right\" lower dimension? After our adventure with k-means, it should come as no surprise that there is no universally accepted \"answer\" for choosing the right number for the lower dimension. If your primary reason for reducing the dimension is for data visualization, then many times the \"answer\" is 2 for the _target_ dimension. \n",
    "\n",
    "If your goal is to reduce your data to speed up algorithms or to simply remove redundant information, then you might look at how much your reduction _explains_ of the original data. Encapsulated in the `explained_variance_ratio_` is a notion of explanation. \n",
    "\n",
    "### Total Explained Variation \n",
    "\n",
    "Most data is spread out. Data that is more spread from each other with varying notions of \"near\" and \"far\", we call data of _high variance_ and by contrast data with _low variance_ is data that tends to be very tightly clustered together. The idea of _variance_ is to give us a notion of how spread (or not) our data is. It can also be thought of as a notion of the variation in between our data points. \n",
    "\n",
    "PCA seeks to be a summary of our original data. We want it to be a decent summary that gives us close approximation of the data. To give us an idea of how well our PCA works, we can compute how much of the original data's variation is _explained_ by each principal component. In `sklearn` this information is captured in `explained_variance_ratio_` or the ratio of the **total** variance that is explained by a particular component. \n",
    "\n",
    "The `explained_variance_ratio_` is an attribute of our fitted PCA. Compute PCA with four components and plot the `explained_variance_ratio_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_alg = PCA(n_components=???)\n",
    "pfit4 = pca_alg.fit(justnum_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pfit4.explained_variance_ratio_, marker='*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the number of components matches the dimension of the data, the `sum` of the `explained_variance_ratio_` should equal one. Check that this is true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your check here: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note_ - If yours is not exactly one, check that the difference between your answer and 1 is below _machine tolerance_ or about $10^{-14}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mach_tol = 10e-14\n",
    "print(np.abs(np.sum(pfit4.explained_variance_ratio_)-1) < mach_tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a minute to unpack what is in the second check. What does each piece do?     \n",
    "* `np.sum(pfit4.explained_variance_ratio_)` is doing _____    \n",
    "* `np.abs(???-1)` is doing _____\n",
    "* `???? < mach_tol` is doing _____\n",
    "\n",
    "**Wait here for a group discussion** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `explained_variance_ratio_` to choose the \"right\" dimension\n",
    "\n",
    "In `explained_variance_ratio_` we have how much each principal component explains. As we can see above, the added benefit of each one goes down. This is related to the fact that the computation of `explained_variance_ratio_` has to do with the eigenvalues associated to each principal component (which by construction are in decreasing order). \n",
    "\n",
    "It would be much more helpful if we could see how much the cumulative contribution of the first component, the second component with the first, the third component with the first and the second, and so on. Now, while we could write a `for` loop, let's take a page out of our text _Python Machine Learning_ and use the numpy command `cumsum`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_variance = np.cumsum(pfit4.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `cumsum()` takes cumulative sums in progressing order. To illustrate what this means run the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cumsum = np.cumsum([1,2,3,4,5])\n",
    "print(test_cumsum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that that the first entry in the output is the value of the first index. The second entry is the sum of the the values associated to the first two indices, and so on. \n",
    "\n",
    "Returning to our cumulative variance, let's now plot this output against the individual contributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.step(range(1,5),cum_variance)\n",
    "plt.bar(range(1,5),pfit4.explained_variance_ratio_)\n",
    "\n",
    "# Plot based on image on Page 148 in _Python Machine Learning_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we should see in this plot is how much each piece contributes. While this is not the most exciting example with our original data's dimension being only 4, we can see concretely how much each component explains and how much they consecutive components together.  \n",
    "\n",
    "Now the question still looms: how do we decide what the appropriate target dimension (ie. the lower dimension) should be? Again, if not desiring to make a two-dimensional visualization of our data, we would examine a plot similar to the above one and use a process similar to elbowology to determine the \"right\" target dimension. \n",
    "\n",
    "What do you think the \"right\" target dimension is for this data? **Justify** your choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition (SVD)\n",
    "\n",
    "SVD is linear algebra at its finest (though I promise not to explore that tangent). There are several parallels to PCA (which we explore later, but in this section, we will focus on 1) what SVD is and 2) how to compute it. \n",
    "\n",
    "SVD is based on the fact that for any matrix $D$, we can find a _decomposition_ comprised of three matrices -- $U$, $S$, and $V$ -- such that $D = USV^t$. In other words, we can find three matrices -- $U$, $S$, and $V$ -- where $D$ is equal to the matrix multiplications of $U$, $S$, and $V$ transposed. (Recall that all that _transpose_ means is \"flip\" the rows to be columns and the columns to be rows.)\n",
    "\n",
    "For our data matrix $D$, suppose we have $m$ observations and $n$ variables. Then the matrices in this decomposition have a particular structure:\n",
    "* $U$ is an $m \\times m$ matrix where the **columns** are each perpendicular to each other (like axes). These are called the _left singular vectors_.\n",
    "* $S$ is an $m \\times n$ matrix where nearly all the entries are 0. Only the diagonal entries (ie. those in the $i,i$ locations are non-zero. Like the eigenvalues in PCA, we the entries of $S$ decrease as we step down the diagonal. These are called the _singular values_.\n",
    "* $V$ is an $n \\times n$ matrix where the **rows** are each perpendicular to each other (like axes). These are called the _right singular vectors_.\n",
    "\n",
    "The goal of SVD is to find the matrices $U$, $S$, and $V$, such that $D = USV^t$. \n",
    "\n",
    "\n",
    "### SVD in python\n",
    "\n",
    "SVD is _just_ a decomposition of a matrix. It's all linear algebra, and as such, it is implemented directly in _numpy_ in the `linalg` sub-module: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U,S,V = np.linalg.svd(justnum,full_matrices=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the shape of the things that we have now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code block for shape checks\n",
    "S.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the shape of $S$ only has one value. This means that numpy is regarding this as an one-dimensional array. If we want to check that our matrix decomposition is close to the original data, we use the following trick from the [svd helpfile](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umax = S.shape[0]\n",
    "np.allclose(justnum, np.dot(U[:, :umax] * S, V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in this \"trick\" we are using `np.allclose`. This method compares all the elements between two matrices and returns `True` if the entries in the same position are all close to each other. Put another way, if we want to compare $A$ and $B$, then `np.allclose(A,B) = True` if (for all values of $i$ and $j$) $A_{i,j}$ and $B_{i,j}$ are nearly the same value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD for dimension reduction\n",
    "\n",
    "Like with PCA, we can use pieces of our matrice decomposition to perform a dimension reduction. Borrowing from the trick above, we can push our data into a lower dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justnum_svd2 = U[:, :2] * S[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot of the resulting dimension reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resulting dimension reduction\n",
    "\n",
    "Notice that here, while we have a two dimensional representation, we do not have nice axes that we can easily interpret. Part of this is due to not normalizing nor standardizing our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SVD on the standardized data\n",
    "U_std,S_std,V_std = np.linalg.svd(????,full_matrices=True)\n",
    "\n",
    "justnum_std_svd2 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2D plot of the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `full_matrices` flag\n",
    "\n",
    "You will notice that we used a flag in our SVD. This flag sets the output shape of our matrices, due to a technical theorectical point:\n",
    "* The maximum number of singular values is the minimum of $m$ and $n$. \n",
    "\n",
    "So while we can create a matrix $S$ that is of shape $m \\times n$, so much of that matrix will have 0 entries. And just like with single numbers, when we have whole rows or columns of zeros in a matrix, multiplying with that matrix will yield rows or columns of zeros. This means that there are parts of either the $U$ or $V$ matrices that will never have an effect in our decomposition. \n",
    "\n",
    "The `full_matrices` flag allows us to create a decomposition with only rows and columns that will be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_small,S_small,V_small = np.linalg.svd(justnum,full_matrices=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lots of kinds of SVD\n",
    "\n",
    "Similar to using the `full_matrices` flag, there are many versions of SVD that can be deployed in various circumstances. One version is called the [truncated SVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) where we decide in advance what dimension we want our data to be viewed in. \n",
    "\n",
    "As the [truncated SVD helpfile in sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) states, truncated SVD is a good alternative for _sparse_ data. So what is \"sparse\" data? \n",
    "\n",
    "#### Sparse data\n",
    "\n",
    "Sparse data is data where most of each observation's variables are equal to zero. Typically, when someone says \"sparse\", they mean that only about 10% of the total dataframe is non-zero. Although, you may hear of other specifications where someone says that their data has only 5% non-zero values. \n",
    "\n",
    "Sparsity happens more often than you think. Consider a dataset about restaurants. We might have a variable for the type of food (ie. American, fusion, etc), another for whether they are open for breakfast (ie. yes or no), another for open for brunch, another for lunch, and another for dinner. We could have indicator variables (ie. ones with either \"yes\" or \"no\" for choices) for hundred options. For many of these options, restaurants may only have \"yes\" for a handful of them, and thus the data is likely _sparse_. \n",
    "\n",
    "#### SVD on sparse data\n",
    "To learn more about SVD on sparse data in python, check out [this blog post](https://jakevdp.github.io/blog/2012/12/19/sparse-svds-in-python/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA & SVD - A Comparison\n",
    "\n",
    "PCA and SVD are closely related (and with a bit of linear algebra, you can show this to be true). Both can be used for dimension reduction in different situations. Both also have their restrictions:\n",
    "* PCA requires the data to be centered around 0\n",
    "* If you want each variable to have even weight, then you also need to standardize your data\n",
    "* SVD does not require the data to be normalized, but this imbues a sense of importance on each variable based on the span of the variable (think GPA vs hours slept)\n",
    "* SVD can be performed on sparse data, while PCA cannot always be used\n",
    "\n",
    "Both SVD and PCA are very powerful tools. Like choosing the target dimension, we have to choose whether to use PCA or SVD based on the situation that we have. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts\n",
    "\n",
    "To finish up this lab, create **_one_** plot comparing PCA and SVD on the standardized data. Share your plot in a post on **#lab06_submission** channel on slack saying whether you prefer PCA or SVD and **why**. Cheer on your fellow Machine Learning practitioners as the votes in the epic discussion of PCA vs. SVD roll in! \n",
    "\n",
    "If your have questions from this lab, post them to #lab_questions with the same preamble (i.e. starting with **Lab6**). If you have the same question, please use one of the emoji's to upvote the question. If you would like to answer someone's question, please use the thread function. This will tie your answer to their question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References consulted\n",
    "0. _Doing Data Science: Straight talk from the frontline_ by C. O'Neil & R. Schutt (2014)\n",
    "1. _Python Machine Learning_\n",
    "2. [PCA `sklearn` helpfile](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA)\n",
    "3. [Truncated SVD `sklearn` helpfile](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD)\n",
    "4. [SVD `numpy` helpfile](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html)\n",
    "5. [What is principal component analysis? from \"Bits of DNA\"](https://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/)\n",
    "6. [Explained variance in PCA](https://ro-che.info/articles/2017-12-11-pca-explained-variance)\n",
    "7. [What is the intuitive relationship between SVD and PCA?](https://math.stackexchange.com/questions/3869/what-is-the-intuitive-relationship-between-svd-and-pca)\n",
    "8. [Reduced SVDs on Wikipedia](https://en.wikipedia.org/wiki/Singular_value_decomposition#Reduced_SVDs)\n",
    "9. [Sparse SVDs in Python](https://jakevdp.github.io/blog/2012/12/19/sparse-svds-in-python/)\n",
    "10. [PCA and SVD explained with numpy](https://towardsdatascience.com/pca-and-svd-explained-with-numpy-5d13b0d2a4d8)\n",
    "11. [`dir()` helpfile](https://docs.python.org/3/library/functions.html#dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
