{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 13\n",
    "\n",
    "In the last lab, we focused on the process of supervised learning, specifically spending time with the training and testing phases. Today, we examine the limitations of how a single iteration of train and test, and introduce the idea of cross-validation for addressing some of these weaknesses.\n",
    "\n",
    "0. Detail limitations of \"vanilla\" train/test paradigm\n",
    "1. Define and apply cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports for Today\n",
    "\n",
    "Today we are working with the data from last time. You can either import the data from the Lab 12 directory directly or by copying it into this directory. \n",
    "\n",
    "We begin as usual, importing the packages and data that we need. Still do **not** plot the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import block\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import pandas as pd\n",
    "\n",
    "import random \n",
    "\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for later use\n",
    "\n",
    "def compute_mse(truth_vec, predict_vec):\n",
    "    return np.mean((truth_vec - predict_vec)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For function testing \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Data\n",
    "#  Move a copy of the data into this folder before running this block\n",
    "\n",
    "mystery_data = pd.read_csv(\"lab12data.csv\", sep = \",\")\n",
    "mystery_np = np.genfromtxt(\"lab12data.csv\", delimiter=',', skip_header=1)\n",
    "\n",
    "\n",
    "# DO NOT PLOT ANYTHING! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General x-values for fitting and plotting\n",
    "x = np.linspace(np.min(mystery_np[:,1])-0.1, np.max(mystery_np[:,1])+0.1, 1000)\n",
    "x = np.array([x]).T\n",
    "\n",
    "x_mat = np.hstack([x,x**2,x**3,x**4,x**5,x**6,x**7,x**8,x**9,x**10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing: What is the \"right\" approach?\n",
    "\n",
    "In the last lab, we started to play with the intuition behind each of the train and test phases. You generated images of the training and testing errors for various proportions for the train and test sets. \n",
    "\n",
    "#### Warm-Up Part 1\n",
    "\n",
    "Let's begin with a quick activity. Let's divide the data into 5 equal parts: \n",
    "* Set A - Rows 0:59\n",
    "* Set B - Rows 60:119\n",
    "* Set C - Rows 120:179\n",
    "* Set D - Rows 180:239\n",
    "* Set E - Rows 240:299\n",
    "\n",
    "Assume the data follows a 2-degree polynomial, compute the training and testing errors where each set acts as the _testing_ set with the other four combined as the training set. Plot the training and test errors. What do you notice? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing the data into the five sets\n",
    "A_set = mystery_np[:60,:]\n",
    "B_set = mystery_np[60:120,:]\n",
    "C_set = mystery_np[120:180,:]\n",
    "D_set = mystery_np[180:240,:]\n",
    "E_set = mystery_np[240:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing train and test errors for where\n",
    "#    each set acts as the testing data \n",
    "#    for a 2-degree polynomial\n",
    "\n",
    "# Initial Train and test error lists\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "\n",
    "## =-=-=-=-=\n",
    "## Set A is testing set \n",
    "## =-=-=-=-=\n",
    "#  Everything else is in the training set:\n",
    "trainA = np.vstack((B_set, C_set, D_set, E_set))\n",
    "testA = A_set\n",
    "\n",
    "# Fit model to training dataset\n",
    "lm_A = linear_model.LinearRegression()\n",
    "mod_A = lm_A.fit(trainA[:,1:3], trainA[:,0])\n",
    "\n",
    "# Compute the training error \n",
    "train_predsA = mod_A.predict(trainA[:,1:3])\n",
    "train_errorA = compute_mse(train_predsA, trainA[:,0])\n",
    "train_errors.append(train_errorA)\n",
    "\n",
    "# Compute the testing error\n",
    "test_predsA = mod_A.predict(testA[:,1:3])\n",
    "test_errorA = compute_mse(test_predsA, trainA[:,0])\n",
    "test_errors.append(test_errorA)\n",
    "\n",
    "\n",
    "## =-=-=-=-=\n",
    "## Set B is testing set:\n",
    "## =-=-=-=-=\n",
    "trainB = np.vstack((A_set, C_set, D_set, E_set))\n",
    "testB = B_set\n",
    "\n",
    "# Fit model to training dataset\n",
    "lm_B = linear_model.LinearRegression()\n",
    "mod_B = lm_B.fit(trainB[:,1:3], trainB[:,0])\n",
    "\n",
    "# Compute the training error \n",
    "train_predsB = mod_B.predict(trainB[:,1:3])\n",
    "train_errorB = compute_mse(???, trainB[:,0])\n",
    "train_errors.append(train_errorB)\n",
    "\n",
    "# Compute the testing error\n",
    "test_predsB = mod_B.predict(????)\n",
    "test_errorB = compute_mse(???,???)\n",
    "test_errors.append(????)\n",
    "\n",
    "## =-=-=-=-=\n",
    "## Set C is testing set:\n",
    "## =-=-=-=-=\n",
    "trainC = np.vstack((A_set, B_set, ???,???))\n",
    "testC = ???\n",
    "\n",
    "# Fit model to training dataset\n",
    "lm_C = linear_model.LinearRegression()\n",
    "mod_C = lm_C.fit(trainC[:,1:3], trainC[:,0])\n",
    "\n",
    "# Compute the training error \n",
    "train_predsC = mod_C.predict(????)\n",
    "train_errorC = compute_mse(???, ???)\n",
    "train_errors.append(train_errorC)\n",
    "\n",
    "# Compute the testing error\n",
    "test_predsC = mod_C.predict(????)\n",
    "test_errorC = compute_mse(???,???)\n",
    "test_errors.append(????)\n",
    "\n",
    "## =-=-=-=-=\n",
    "## Set D is testing set:\n",
    "## =-=-=-=-=\n",
    "trainD = \n",
    "testD = \n",
    "\n",
    "# Fit model to training dataset\n",
    "lm_D = \n",
    "mod_D = \n",
    "\n",
    "# Compute the training error \n",
    "train_predsD = \n",
    "train_errorD = \n",
    "\n",
    "\n",
    "# Compute the testing error\n",
    "test_predsD = \n",
    "test_errorD = \n",
    "\n",
    "\n",
    "## =-=-=-=-=\n",
    "## Set E is testing set:\n",
    "## =-=-=-=-=\n",
    "\n",
    "\n",
    "# Fit model to training dataset\n",
    "\n",
    "\n",
    "# Compute the training error \n",
    "\n",
    "\n",
    "# Compute the testing error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and testing errors in order \n",
    "# of the Sets A, B, C, D, E as the test set\n",
    "\n",
    "plt.plot(list(range(1,6)), train_errors, c=\"b\")\n",
    "plt.plot(list(range(1,6)), test_errors, c=\"r\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Given these plots, what statements/questions/concerns do you have about how we split data into train and test? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Spot for your notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Warm-Up Part 2 \n",
    "\n",
    "Repeat part 1, but for a 3-degree polynomial and for a 4-degree polynomial. I would strongly recommend that you copy and paste the code below, instead of just rerunning the above blocks. Again, below each plot, answer the question from part 1 (copied below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing train and test errors for where\n",
    "#    each set acts as the testing data \n",
    "#    for a 3-degree polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and testing errors in order \n",
    "# of the Sets A, B, C, D, E as the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Given these plots, what statements/questions/concerns do you have about how we split data into train and test? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Spot for your notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing train and test errors for where\n",
    "#    each set acts as the testing data \n",
    "#    for a 4-degree polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and testing errors in order \n",
    "# of the Sets A, B, C, D, E as the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Given these plots, what statements/questions/concerns do you have about how we split data into train and test? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Spot for your notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Warm-up Part 3 \n",
    "Now, make one plot with three lines for the testing errors in order of the Sets A, B, C, D, E as the test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three testing error lines on one plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing answer a few questions:    \n",
    "**Q1:** What surprises you about this plot?   \n",
    "**A1:** \n",
    "\n",
    "**Q2:** Which training set is the right one? Why?     \n",
    "**A2:** \n",
    "\n",
    "**Q3:** Of these three polynomial degrees (2,3,4), which is the best one for this data? Why?     \n",
    "**A3:** \n",
    "\n",
    "Check in with your group. Where do you agree? Where do you disagree? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations for the train and test paradigm\n",
    "\n",
    "It can be very surprising that the training and testing errors can vary so wildly depending on what data is in the training set and what data is in the testing set. This is because of the _variance_ that is inherent in any model about data, due to the variations in data. Recalling that the goal of any machine learning algorithm is to uncover some underlying truth about our data, in seeking this truth, we can't rely on having the \"right\" training data. Instead, we need to have methods that are robust to variations in our data; in other words, we want to feel confident that our discoveries are good regardless of the data that we train on. \n",
    "\n",
    "Our goal with the `mystery_data` is to uncover the polynomial relationship between `mysty` and `myst`. In Lab 12, we divided our data into train and test. Then for degrees between 1 and 10, we used the training data to find a polynomial relationship, and used the testing set to see how this found relationship might generalize to new data. We noticed a few things:\n",
    "* The training error went down as we increased the degree of the polynomial\n",
    "* The test error dropped for a bit and then increased, as we increased the degree of the polynomial\n",
    "\n",
    "In Lab 12, we also experimented with the proportion of the data that we classified as the training set. Then in the first part of this lab, we investigated which part of the data is the \"right\" training data. With the goal of trying to uncover the \"right\" polynomial relationship for `mysty` and `myst`, we have felt less than satisfied with our choices. (Note, this was intentional.) \n",
    "\n",
    "With the goal of figuring out the \"right\" polynomial for `mysty` and `myst`, we have to determine the correct degree (or shape) for our polynomial model. \n",
    "\n",
    "_Side Question:_ what is the degree of the polynomial: parameter or hyperparameter? \n",
    "\n",
    "If we rely solely on training error or choose the wrong training set, we risk overfitting to our training data. We need something that will punish us for adding degrees for the sake of adding degrees, but also reward us for adding the right number of degrees. So what are we to do? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "**Cross-Validation** is an iterative version of the train/test paradigm where each data point has a turn in the training set. There are a few varieties of _cross-validation_ (also colloquially called cross-val), but they all have the same basic structure:\n",
    "\n",
    "0. Choose the model that you want to validate (ie. see how it would generalize to new data)\n",
    "1. Divide your data into $k$ equal pieces (Note that there are many ways to do this!)\n",
    "2. Do the following $k$ times: \n",
    "   * Select the $i^{\\textrm{th}}$ piece of the dataset to be the test set\n",
    "   * The rest of the pieces are your training set\n",
    "   * Fit your model on the training set\n",
    "   * Compute the test error (ie. MSE) and call this the $i^{\\textrm{th}}$ test error\n",
    "   * Add the $i^{\\textrm{th}}$ test error to a list of test errors\n",
    "3. Compute the average of the test errors, which is called the **cross-validated error.** \n",
    "\n",
    "When $k$ does not equal the number of data points, we call this **$k$-fold cross-validation.** The _folds_ are the pieces that serve as the test data. \n",
    "\n",
    "When $k$ equals the number of data points, we call this **leave one out cross validation** (or LOOCV). In this set-up, each iteration of the cross-validation holds out exactly one datapoint as the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing $k$-fold Cross-Validation\n",
    "\n",
    "Returning to the warm-up for this lab, notice that we have already done all but the last step for a 5-fold cross-validation. \n",
    "\n",
    "Compute the _cross-validated_ error for degree 2, 3, and 4 degree polynomials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average for the testing errors for the \n",
    "# degree 2, 3, and 4 degree polynomials from the warm up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Leave One Out Cross-Validation\n",
    "\n",
    "Referring to the warm up code, implement LOOCV and compute the _cross-validated_ error for degree 2, 3, and 4 degree polynomials. In other words, loop over all the datapoints allowing each one to act as the only point in the test set, meaning that all the other points are the training points. \n",
    "\n",
    "_Hint:_ You may find lab 12 helpful for code that excludes some indices from consideration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOOCV implementation \n",
    "\n",
    "# Find the number of data points in your data\n",
    "n_data = mystery_np.shape[0]\n",
    "\n",
    "# Initialize the list of errors\n",
    "test_errors = []\n",
    "\n",
    "\n",
    "# Loop over all points in the data set, letting each act as the test set\n",
    "for i in range(n_data):\n",
    "    \n",
    "    # Split data into train and test\n",
    "    test_data = \n",
    "    train_data = \n",
    "    \n",
    "    # Create and train a model\n",
    "    lm= linear_model.LinearRegression()\n",
    "    mod = lm.fit(train_data[:,1:3], train_data[:,0])\n",
    "    \n",
    "    # Compute the testing error and add it to the list of testing errors\n",
    "    preds = mod.predict(test_data[:,1:3])\n",
    "    t_error = compute_mse(test_data[:,0],preds)\n",
    "    \n",
    "    test_errors.append(t_error)\n",
    "    \n",
    "\n",
    "# Compute the cross-val error\n",
    "np.mean(test_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation fun facts\n",
    "\n",
    "Notice that the cross-validated error is computed as the average of the testing errors. This means the cross-validated error will go down as we add useful degrees (or input variables) and sharply increase when we add a \"junk\" degree (or input variable). \n",
    "\n",
    "Additionally by relying on all of the possible testing errors, the cross-validated error is less sensitive to the selection of the training and testing sets, then any single train/test split. This means that the cross-validated error gives us a more robust value for how our model will generalize to new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next time\n",
    "\n",
    "Next time, we will _finally_ get away from linear regression kinds of things, and start exploring **Support Vector Machines**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts\n",
    "\n",
    "To finish up this lab, share what you think the right polynomial degree for our mystery data is. Share your answer in a post on **#lab-13-submission** channel on slack with your answer. After you post, look at the notebook in the Lab12 folder that created the data. Compare your result to how the data was created. Using the thread function on slack, post a reaction to seeing the \"answer.\" \n",
    "\n",
    "If your have questions from this lab, post them to #lab_questions with the same preamble (i.e. starting with **Lab13**). If you have the same question, please use one of the emoji's to upvote the question. If you would like to answer someone's question, please use the thread function. This will tie your answer to their question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources consulted \n",
    "\n",
    "0. [ISLR](http://faculty.marshall.usc.edu/gareth-james/ISL/)\n",
    "1. [SDS 293 Notes by R. Jordan Crouser](http://www.science.smith.edu/~jcrouser/SDS293/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
