{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 12\n",
    "\n",
    "This week, we are going to focus on the process of supervised learning. Specifically, our goals today are:\n",
    "\n",
    "0. Understand the role of each phase\n",
    "1. Compare and contrast training and testing\n",
    "2. Discuss how to select training and testing data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports for Today\n",
    "\n",
    "Today we are working with a new data. We begin as usual, importing the packages and data that we need. Do **not** plot the data. You can peek at the top of the data using `head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import block\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import pandas as pd\n",
    "\n",
    "import random \n",
    "\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for later use\n",
    "\n",
    "def compute_mse(truth_vec, predict_vec):\n",
    "    return np.mean((truth_vec - predict_vec)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For function testing \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Data\n",
    "\n",
    "mystery_data = pd.read_csv(\"lab12data.csv\", sep = \",\")\n",
    "mystery_np = np.genfromtxt(\"lab12data.csv\", delimiter=',', skip_header=1)\n",
    "\n",
    "\n",
    "# DO NOT PLOT ANYTHING! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on the Mystery data\n",
    "\n",
    "The mystery data is laid out with the output values `mysty` in the first column. This output depends on `myst` (which is the second column) in some polynomial fashion, ie. $mysty = b_0 + b_1\\cdot myst + b_2\\cdot myst^2 + b_3\\cdot myst^3 + ...$. To assist our investigation, there are 9 \"input\" variables denoted `myst#` where the number refers to the number of powers `myst` has been raised to. Concretely `myst2` = `myst`$^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning - Where are we? \n",
    "\n",
    "So far our journey with supervised learning, we've seen the process of _training_ our algorithms and _testing_ our trained algorithms. We've discussed the differences between parameters and hyperparameters, and how to use gradient descent to set parameters. \n",
    "\n",
    "In all our discussions, we have carefully side-stepped the _**why**_ behind the two phases in supervised learning. Today aims to confront these _whys_ directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning - Broad Goal\n",
    "\n",
    "The goal of any machine learning algorithm is to uncover some underlying truth about our data. For example, consider our student information dataset. We might want to know does average amount of sleep relate or impact one's GPA? To answer this question, we have (fake) data from a number of students that are individual glimpses into the **true** relationship between average amount of sleep and a person's GPA. In an imperfect metaphor, just as a doctor can witness a person's symptoms for an unknown illness, we can view the data that we have as the \"symptoms\" for the true relationship that we cannot see. \n",
    "\n",
    "Our broad goal in machine learning is to find the _true relationship_ for our data based on the data that we have. The idea is that if we find the true relationship, then we can accurately predict, classify, and cluster any new data (or symptoms or examples) that we are given. \n",
    "\n",
    "The process of supervised learning divides this goal into the two pieces. First the train phase works to find something close to the true relationship and the test phase gives a notion of how well the output of the train phase would do on new data. In other words, the test phase gives us a notion of how close our guess (from the training phase) is to the true relationship. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing for Train and Test\n",
    "\n",
    "In order to implement a supervised learning algorithm, we have to divide the available data into _train_ and _test_ data. In this section, we will investigate this division using our the numpy import of our `mystery_np` data. \n",
    "\n",
    "Noting that our mystery data has 300 data points, we begin with a small training set of 15 randomly selected data points. We then put the rest in the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2022)\n",
    "\n",
    "# Number of data points\n",
    "n_data = mystery_np.shape[0]\n",
    "\n",
    "# Splitting into train and test, with 15 points in the training set\n",
    "train_inds = random.sample(list(range(n_data)),15)\n",
    "train_data = mystery_np[train_inds,:]\n",
    "\n",
    "test_inds = list(set(range(n_data)).difference(set(train_inds)))\n",
    "test_data = mystery_np[test_inds,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot these points and make a guess of how `mysty` is constructed from `myst`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_data[:,1],train_data[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing all the possible polynomials\n",
    "\n",
    "We have been told that `mysty` and `myst` have a polynomial relationship and our broad goal is to uncover the true polynomial relationship. To do this, we will build polynomials of degrees 1 through 10. For each one, we will plot the resulting function, and compute the training error and test error.\n",
    "\n",
    "Since we know that `mysty` and `myst` have a polynomial relationship, and given our layout of our data, we can actually just use linear regression in `sklearn` to output various polynomial relationships. Below is an example of this process for a 2-degree polynomial, ie $mysty = b_0 + b_1\\cdot myst + b_2\\cdot myst^2$.\n",
    "\n",
    "I've done the first two, but with your group divide and conquer the rest of this part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General x-values for fitting and plotting\n",
    "x = np.linspace(np.min(train_data[:,1])-0.1, np.max(train_data[:,1])+0.1, 1000)\n",
    "x = np.array([x]).T\n",
    "\n",
    "x_mat = np.hstack([x,x**2,x**3,x**4,x**5,x**6,x**7,x**8,x**9,x**10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree 1\n",
    "\n",
    "# Note weirdness with due to the one-d arrayness here!\n",
    "\n",
    "# Create polynomial\n",
    "lm1 = linear_model.LinearRegression()\n",
    "model1 = lm1.fit(train_data[:,1].reshape(-1, 1),train_data[:,0])\n",
    "\n",
    "# Create plot of training data with the polynomial\n",
    "plt.scatter(train_data[:,1].reshape(-1, 1),train_data[:,0])\n",
    "\n",
    "preds1 = model1.predict(x_mat[:,:1].reshape(-1, 1))\n",
    "plt.plot(x_mat[:,0],preds1, c=\"r\")\n",
    "\n",
    "# Compute the training error\n",
    "train_preds1 = model1.predict(train_data[:,1].reshape(-1, 1))\n",
    "train_error1 = compute_mse(train_preds1, train_data[:,0].reshape(-1, 1))\n",
    "\n",
    "# Compute the testing error\n",
    "test_preds1 = model1.predict(test_data[:,1].reshape(-1, 1))\n",
    "test_error1 = compute_mse(test_preds1, test_data[:,0].reshape(-1, 1))\n",
    "\n",
    "print(\"The training error is\", round(train_error1,3),\n",
    "     \" and the testing error is\", round(test_error1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree 2\n",
    "\n",
    "# Create polynomial\n",
    "lm2 = linear_model.LinearRegression()\n",
    "model2 = lm2.fit(train_data[:,1:3],train_data[:,0])\n",
    "\n",
    "# Create plot of training data with the polynomial \n",
    "plt.scatter(train_data[:,1],train_data[:,0])\n",
    "\n",
    "preds2 = model2.predict(x_mat[:,:2])\n",
    "plt.plot(x_mat[:,0],preds2, c=\"r\")\n",
    "\n",
    "# Compute the training error\n",
    "train_preds2 = model2.predict(train_data[:,1:3])\n",
    "train_error2 = compute_mse(train_preds2, train_data[:,0])\n",
    "\n",
    "# Compute the testing error\n",
    "test_preds2 = model2.predict(test_data[:,1:3])\n",
    "test_error2 = compute_mse(test_preds2, test_data[:,0])\n",
    "\n",
    "print(\"The training error is\", round(train_error2,3), \n",
    "      \" and the testing error is\", round(test_error2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree 9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the \"right\" one?\n",
    "\n",
    "Based on these plots, which one is the \"right\" polynomial? \n",
    "\n",
    "Chat with your group about which one \"looks\" like the right one to you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfitting and Overfitting\n",
    "\n",
    "There are two terms concerning fit that often used _underfitting_ and _overfitting_. Before we define those, let's plot the training errors and the testing errors both against the number of degrees. After the plot, write down a few things that you notice about this plot: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate the training and testing errors \n",
    "# Hint: You can do this directly by just creating a list with the variable names\n",
    "\n",
    "train_errors = [???]\n",
    "test_errors = [???]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training and testing errors\n",
    "\n",
    "plt.plot(list(range(1,11)), train_errors, c=\"b\")\n",
    "plt.plot(list(range(1,11)), test_errors, c=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Notes about the plot here!)\n",
    "*  \n",
    "*  \n",
    "*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Underfitting** happens when we have not adequately fit our model to the training data. This happens when our training error is high. \n",
    "\n",
    "**Overfitting** happens when we have fit our model to perfectly to the training data and have made it less useful for making predictions to new data. This happens when the training error is low and the testing error is high. \n",
    "\n",
    "Underfitting and overfitting have to do with the _Bias/Variance trade off._ Our models are attempting to explain some generalization about our data and the underlying mechanics of the data. We want our model to be a good summary of the data in that it explains enough of the data's variations without being too sensitive to that particular data. \n",
    "* Every simplication of data will have _bias;_ the more simplistic the model, the more _bias_. \n",
    "* Every model also has _variance;_ the more complicated the model, the slightest change in the data will cause huge changes in the model.  \n",
    "\n",
    "For an imperfect metaphor, say you want to buy a new blazer for work. You buy one in town. It's generic to a person of your approximate size and shape. There is _bias_ in creating this garmet to fit as many people as possible. You have the jacket tailored to your specific body. Now your friend wants to borrow it. You are both about the same size, but she says the jacket doesn't fit her. This is because it is overfit to you. In order for it to fit her, you will have to have it retailored and possibly many measurements will change. This is because the model of a jacket (with its many specific lengths and measurements) has high _variance._\n",
    "\n",
    "Our goal in building models is to find a balance between bias and variance, which we exemplify as trying to minimize **both** training and testing errors. To achieve this balance, we want to have as much training and testing data as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proportion of Train and Test\n",
    "\n",
    "In our first exploration today of train and test, we used about 5% of the data for the training data and the rest for testing. \n",
    "\n",
    "Now we will repeat parts of these experiments with 50-50 train/test split and a 95-5 train/test split, that is 50% of the data in train and then 95% of our data in train, respectively. We will make a make a training and testing error plot for each split and compare your results. \n",
    "\n",
    "_Note:_ Due to the coding oddities for degree one polynomial, we will skip that one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 50-50 train/test split\n",
    "n_data = mystery_np.shape[0]\n",
    "\n",
    "# Initialize lists to store the train and test errors\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "# Split for train and test\n",
    "n_train = round(0.5*???)\n",
    "\n",
    "train_inds = random.sample(list(range(n_data)),???)\n",
    "train_data = mystery_np[train_inds,:]\n",
    "\n",
    "test_inds = list(set(range(n_data)).???)\n",
    "test_data = mystery_np[test_inds,:]\n",
    "\n",
    "# Loop over polynomials with degree 2 through 10: \n",
    "for p in range(2,11):\n",
    "    lm = linear_model.LinearRegression()\n",
    "    model = lm.fit(train_data[:,1:(p+1)],train_data[:,0])\n",
    "\n",
    "    # Compute the training error and add to list of training errors\n",
    "    train_preds = model.predict(train_data[:,1:(p+1)])\n",
    "    train_error_p = compute_mse(train_preds, train_data[:,0])\n",
    "    train_errors.append(train_error_p)\n",
    "\n",
    "    # Compute the training error and add to list of testing errors\n",
    "    test_preds = model.predict(test_data[:,1:??])\n",
    "    test_error_p = compute_mse(???, test_data[:,0])\n",
    "    test_errors.append(???)\n",
    "\n",
    "# Plot your errors against the degree of the polynomials\n",
    "plt.plot(list(range(2,11)), train_errors, c=\"b\")\n",
    "plt.plot(list(range(2,11)), test_errors, c=\"r\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 95-5 train/test split\n",
    "n_data = mystery_np.shape[0]\n",
    "\n",
    "# Initialize lists to store the train and test errors\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "# Split for train and test\n",
    "n_train = \n",
    "\n",
    "train_inds = \n",
    "train_data = \n",
    "\n",
    "test_inds = \n",
    "test_data = \n",
    "\n",
    "# Loop over polynomials with degree 2 through 10: \n",
    "for p in range(2,11):\n",
    "    lm = linear_model.LinearRegression()\n",
    "    model = lm.fit(???,???)\n",
    "\n",
    "    # Compute the training error and add to list of training errors\n",
    "    train_preds = model.predict(train_data[:,1:p])\n",
    "    train_error_p = compute_mse(train_preds, train_data[:,0])\n",
    "    train_errors.append(train_error_p)\n",
    "\n",
    "    # Compute the training error and add to list of testing errors\n",
    "    test_preds = \n",
    "    test_error_p = \n",
    "    test_errors\n",
    "\n",
    "# Plot your errors against the degree of the polynomials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now given these two experiments, what degree of polynomial do you think is best? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next time\n",
    "\n",
    "Next time, we will continue exploring dividing our data into train and test and how the process of **cross-validation** helps us choose the \"right\" shape for our models. In this example the shape of our model is given by the degree of the polynomial. Cross-validation can help us figure out what degree is most appropriate for "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts\n",
    "\n",
    "To finish up this lab, post your second two training and testing plots and state what you think the right balance of training and testing data is. In other words, what percentage of your data should be in training? Justify your answer in context of the goals for the training and testing phases. Share your plots in a post on **#lab-12-submission** channel on slack with your answer. \n",
    "\n",
    "If your have questions from this lab, post them to #lab_questions with the same preamble (i.e. starting with **Lab12**). If you have the same question, please use one of the emoji's to upvote the question. If you would like to answer someone's question, please use the thread function. This will tie your answer to their question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources consulted \n",
    "\n",
    "0. [Random selection in python](https://pynative.com/python-random-choice/)\n",
    "1. [Setting random seed](https://pynative.com/python-random-seed/)\n",
    "2. [Set difference](https://www.geeksforgeeks.org/python-set-difference/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
