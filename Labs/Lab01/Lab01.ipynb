{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1\n",
    "\n",
    "After Lab 0, your computer should be all set up for this course. Today we dive into the first concepts that will be the underpinning to everything that we do for the rest of term. \n",
    "\n",
    "This lab's goals are:\n",
    "* Create a `.gitignore` file\n",
    "* Be able to import data in both `numpy` and `pandas`\n",
    "* Articulate the shape of data, specifying 'variables' and 'observations'\n",
    "* Understand the role of unit testing in our course and in programming development more broadly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before starting...\n",
    "\n",
    "Make sure that you have recently `pull`ed `course-materials`. We will need a data file from the `Data` folder for this lab.  \n",
    "\n",
    "Also to avoid causing conflicts between the course materials on the master directory and the work you do in your labs, make a copy of this file and put it in a **sub-directory** under `course-materials` called `student-labs`. The `.gitignore` file has been told to ignore any work that you have in that folder, meaning that anything in that folder will not create conflicts with the master directory. \n",
    "\n",
    "**Note** that if a lab requires more than one file, it will be in its own folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating `.gitignore`\n",
    "\n",
    "The `.gitignore` file is a [hidden](https://en.wikipedia.org/wiki/Hidden_file_and_hidden_directory) file that tells `git` which files to ignore. In general, you should be creating one for each of your repositories. A few things that you will want to ignore each time include your `.ipynb` checkpoints.    \n",
    "\n",
    "Refering to the [help file](https://help.github.com/en/articles/ignoring-files), the process for creating `.gitignore` file is as follows: \n",
    "1. From your command line, navigate to the directory for your **Lab 00** repository on your local machine\n",
    "2. Next we use the `touch` command (which is similar to `cd` and `m`) to create the file.   \n",
    "   Type `touch .gitignore`. This creates an empty file with that name\n",
    "3. Now we need to add the files (or rather the file types) that we do not want to commit to git. To do this, we will use `vim`. Refering to this [Simple Vim Workflow Example](https://www.keycdn.com/blog/vim-commands#simple-vim-workflow-example), we proceed as follows:\n",
    "   * Type `vim .gitignore`   \n",
    "     Now you will see an empty file. If you try to type right now, nothing will happen until you hit the **i** key. This is confusing at first. But **i** is a special key in _vim_ as we will discuss next. \n",
    "   * Enter **insert mode** by typing `i`. Now you can type whatever you want. Since, we want to ignore our _.ipynb_ checkpoints, add `.ipynb_checkpoints` to your _.gitignore_ file. \n",
    "   * Exiting a file in _vim_ is not as simple as closing the window. First, we need to get out of _edit/insert_ mode and back to command mode. To do this hit `Esc`. \n",
    "   * Now you are in _command_ mode. To **save and exit** your file, type `:wq` and then `Enter`.  \n",
    "   \n",
    "_Unsolicited Advice_ - You will be doing this procedure a lot. I recommend either 1) creating a section in your machine learning notebook for **Repeated Procedures** and adding notes there about how to do this or 2) writing them on the inside cover of your notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary packages\n",
    "\n",
    "The first coding component of any script or Jupyter Notebook is the list of imported packages. There are a few reasons for this:\n",
    "1. **Programming reason** - Python executes in order of the given lines. This means that you need to import a package before you use an element from it\n",
    "2. **Human reason** - Before running a notebook or a script, a new user would want to know immediately if they can run the script or not. Putting the import statements at the top of the file allows your user to check if they can run or not run your file. \n",
    "\n",
    "**Note**: Failure to put your import statments at as the first lines of non-commented code will result in an _automatic loss of half of the assignment's total points_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you just throw an error? You might not be in the correct `conda` enviroment. Remember that in Lab 0, we only installed our required packages into one conda environment. So you may need to shut down the `jupyter` kernal, activate the correct environment, and then relaunch your `jupyter notebook`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A few notes on imports\n",
    "\n",
    "You might be wondering why I imported these two packages this way. Partially this is due to what I've seen others do, but a better reason is the one articulated in 'Python for Data Analysis' on page 90:\n",
    "> ... throughout the book, I use the standard NumPy convention of always using `import numpy as np`. You are, of course, welcome to put `from numpy import *` in your code to avoid having to write `np.`, but I advise against making a habit of this. The `numpy` namespace is large and contains a number of functions whose names conflict with built-in Python functions (like `min` and `max`).\n",
    "\n",
    "You will even notice that the help files for [numpy](https://docs.scipy.org/doc/numpy/reference/arrays.ndarray.html) and [pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#basics) use these conventions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data\n",
    "\n",
    "When working with data, we need to first import that data. One can do this either using `pandas` or using `numpy`. In this lab, we will work with both methods for sake of completeness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing with `numpy`\n",
    "\n",
    "Importing with `numpy` will bring in your data as a `numpy array`. The most straightforward way to do this is using `genfromtxt()`. To learn more about this function, read its [help file](https://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffire_np = np.genfromtxt(\"forestfires.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking this apart piece by piece, let's look at what we just did:\n",
    "1. `ffire_np` is a variable\n",
    "2. `np.` tells us to reach into the `numpy` library\n",
    "3. `genfromtxt` is the specific method that we want to call\n",
    "4. The first argument is the name of the file, which we have placed within the same directory. \n",
    "5. The second argument `delimiter=` tells us what gaps to look for between data information. It is safe to use a comma as our separation since `csv` stands for `c`omma `s`eparated `v`aribles. \n",
    "\n",
    "Let's take a peek at what our variable looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ffire_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this what we expect to see? Open the datafile using your favorite spreadsheet viewer. \n",
    "\n",
    "What do you see? Does it match the above? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a third argument that we can use with `genfromtxt` to ignore the column headers: `skip_header=1`. In the below code block use `genfromtxt` with three arguments to re-import the `forestfires` data and print the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import forestfire data here\n",
    "ffire_np = \n",
    "\n",
    "# Take a look at the result here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, is this what you expect to see? Compare this with the open datafile in your favorite spreadsheet viewer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations in `numpy`\n",
    "\n",
    "`numpy` can have a hard time with non-numerical data. When `numpy` doesn't know what to do with cells that are not numbers, it replaces those cells with `nan` or `n`ot `a` `n`umber. \n",
    "\n",
    "Take a look at both of your outputs, and comparing them again to the open datafile in your favorite spreadsheet viewer. Which rows and colums have `nan`s? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What? Why?\n",
    "\n",
    "This doesn't feel particularly useful given that so much data contains information that is non-numerical. However, a closer examination of what `numpy` stands for makes it a bit clearer why `numpy` does this. With each reference to `numpy`, I've used the `code` formatting, but the name of this package is **NumPy** or **Num**erical **Py**thon. This package is for the fast processing of numerical data leveraging tricks from linear algebra. \n",
    "\n",
    "`pandas` can offer us a bit more flexibility. Let's try importing data with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing with `pandas`\n",
    "\n",
    "`pandas` has more functionality when it comes to data with non-numerical values than `numpy`. While our book doesn't delve into the root of the name `panda`, according to [this site](https://towardsdatascience.com/a-quick-introduction-to-the-pandas-python-library-f1b678f34673), `pandas` is short for `P`ython `Da`ta `An`aly`s`is. (Clearly I'm not sure how they got 'pandas' from those three words, so if you would like to instead just imagine a group of cuddly pandas, that's fine with me.) \n",
    "\n",
    "Returning to how we can import using `pandas`, the most common method that we will use is `read_csv()`.  To learn more about this function, read its [help file](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffire_pd = pd.read_csv(\"forestfires.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking this apart piece by piece, let's look at what we just did:\n",
    "1. `ffire_pd` is again just a variable\n",
    "2. `pd.` tells us to reach into the `pandas` library\n",
    "3. `read_csv` is the specific method that we want to call\n",
    "4. The first argument is the name of the file (just like in `genfromtxt()` from above).   \n",
    "5. The second argument `sep=` is the pandas version of `delimiter=` telling us what gaps to look for between data information. (Why are we using a comma here for denoting the gaps?)\n",
    "\n",
    "Let's take a peek at what our variable looks like. Note that we're going to do this both using and not using the `print()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffire_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ffire_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before we go on:\n",
    "1. What do you see in both of these views of the output `ffire_pd`? \n",
    "2. What are the differences between these views?\n",
    "3. How does `ffire_pd` compare to the datafile in your favorite spreadsheet viewer?\n",
    "4. How do `ffire_pd` and `ffire_np` differ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Frames and Series\n",
    "\n",
    "Data types are something that we pay a lot of attention to in computer science. So before moving much further,we should ask ourselves: what kinds of objects did we just create? (How do we check the **type** of an object in python?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the data types for ffire_np and ffire_pd here \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll turn our attention to the `pandas` version: `ffire_pd`. This variable is a `pandas` **_DataFrame_**, which are comprised of **_Series_**. DataFrames look like spreadsheets that we are used to seeing, and they can contain data with both numerical and non-numerical values. Each row of the DataFrame is a Series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape of Data\n",
    "\n",
    "Data, like spreadsheets, have a shape. Most data that we will work with in this class will have a shape that can be described by the number of rows and the number of columns. When refering to data, we think of **_Observations_** and _**Variables**_. \n",
    "* Each observation is a row. Think of the observations as an object, person, or item that we have a set of information on. \n",
    "* The variables are stored as columns. The variables details the kind of information that we have stored on each observation. \n",
    "\n",
    "Looking at your dataframe and refering to the data's [code book](http://archive.ics.uci.edu/ml/datasets/Forest+Fires), what are the observations in this dataset? What are the variables? _(Note: A code book is a document that provides details on a dataset.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the shape of your data\n",
    "\n",
    "Once you have the data loaded, you can quickly generate a number of facts about your data:\n",
    "1. The dimensions of your data (i.e. the number of observations and variables) using `.shape`\n",
    "2. The names of the variables using `.columns`\n",
    "3. Create a quick glance of your data using `.head`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Using `.shape` on ffire_pd\n",
    "print(ffire_pd.shape)\n",
    "\n",
    "#2. Using `.columns` on ffire_pd\n",
    "var_names = ffire_pd.columns\n",
    "print(var_names)\n",
    "\n",
    "# 3. Using `.head` on ffire_pd\n",
    "glance = ffire_pd.head()\n",
    "print(glance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken these pieces apart: \n",
    "1. `.shape` tells us the number of observations followed by the number of variables. We call data _tall_ if there are substantially more observations than variables. We say data is _wide_ if there are substantially more variables than observations. \n",
    "2. `.columns` gives us the list of _variables_ which `pandas` considers as an index. \n",
    "3. `.head` gives us a snippet of the data. The default is 5 rows, but we can actually specify the number of rows that we want to see by supplying a number between the parentheses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Turn\n",
    "\n",
    "Check out [fivethirtyeight's data repository on GitHub](https://github.com/fivethirtyeight/data) and pick a dataset of interest. Use python to import the data and describe key features about the data set. Practice your machine learning communication skills and write a short paragraph about your data based on the code that you ran. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Block for you!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Markdown block for you!)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Powerhouse packages\n",
    "\n",
    "As `python` and data science have grown and evolved, several packages -- including `numpy`, `pandas`, `scipy`, `matplotlib` -- as critical to the practice of machine learning. In this course, we will use these packages, but with an eye towards deep understand of each method that we employ. \n",
    "\n",
    "This course has twin themes: **carpentry** and **creativity**. Most of the assignments in the course focus on the former in service of the latter. Think of this course as a kind of cookng class where we will spend a considerable amount of time on each ingredient and the most basic of recipes, with the ultimate goal of creating a glorious meal mixing and blending the ingredients in unexpected ways with each other and with new ingredients. \n",
    "\n",
    "**Carpentry**: We are going to focus on the interior elements of classic machine learning algorithms, building each one from scratch. We will compare our results to already optimized versions in packages like `scikit-learn`. The goal of carpentry is _deep_ understanding of each algorithm. The course _**Homework**_ and _**Labs**_ are designed to build your toolkit. \n",
    "\n",
    "**Creativity**: While the majority of class time and homework assignments are focused on carpentry, we are building towards you asking \"what if I applied X to Y in this way?\" and \"what happens if we remove this assumption from Z?\" The goal of creativity is applying these algorithms to problems and data that you care about in _new_ and _exciting_ ways. The course **_Projects_** are designed to engage you creatively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Testing: When are we right? \n",
    "\n",
    "We will be using unit testing in our course a lot. In fact, each homework assignment will have unit tests that you can use (or not). Quoting [McFee and Kell](https://bmcfee.github.io/ismir2018-oss-tutorial/tutorial/2018/08/12/part-5.html): \n",
    "\n",
    "> Tests make your life easier and give you confidence in your code. Software is complicated, and tests allow you to be sure that you’ve not broken your project when you change something.\n",
    ">\n",
    ">Writing tests slows you down in the short term, but speeds things up in the long term. Further, as someone who is writing scientific code, tests make sure that your work can be easily reproduced.\n",
    ">\n",
    ">Philosophically, each test should test one thing. This makes it easier to ensure that we can see what has broken when a test breaks. \n",
    "\n",
    "In the next few blocks, we will go over:\n",
    "1. Using tests\n",
    "2. Writing tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit tests in our course\n",
    "\n",
    "For many of our course's homework assignments, you will be given tests. You can use these tests to check your code and see if it is working how I expect it to. In the case of projects, you may be asked to design your own tests. Even if you are not explicitly asked to design a test, it is strongly recommended that you get in the habit of building tests. \n",
    "\n",
    "A few notes: \n",
    "1. When grading your homework, The first thing I will do is check to see if the tests pass. \n",
    "2. I personally am just getting in the habit of writing and using unit-tests. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using tests\n",
    "\n",
    "We will use `pytest` to test our work. In this course, the convention for these files will to have a file name that begins with `test`. In fact, according to [this post](https://semaphoreci.com/community/tutorials/testing-python-applications-with-pytest), `pytest` expects our test files to either use this preamble or end with `_test.py`. In this section, I am again editing from [Part 5](https://bmcfee.github.io/ismir2018-oss-tutorial/tutorial/2018/08/12/part-5.html) of McFee and Kell's tutorial: \n",
    "\n",
    "To run the tests, from your command line, type:\n",
    "`pytest -v`\n",
    "\n",
    "Note:\n",
    "1. My test files will assume that you have followed all directions in the homework assignment, including and especially naming files correctly. Please pay careful attention to these details. \n",
    "2. You cannot test functions inside a `jupyter notebook`. You will need to create a `.py` file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your turn\n",
    "\n",
    "I have written a set of 3 tests for this lab in a file called `test_Lab1.py`. Open this file in your favorite code editor and then run the tests from the command line. What happens?   \n",
    "   \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "_This space is intentionally left blank_\n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "####    \n",
    "   \n",
    "They failed?! Oh no! <sub>This was intentional. Tests fail a lot. It's ok.</sub>   \n",
    "\n",
    "Let's figure out what is going on. (Remember that **I** wrote these tests. You have done nothing wrong.)   \n",
    "\n",
    "Read the error message that you got. What seems to be the issue?    \n",
    "\n",
    "Test your theory by opening `labone.py`. Make **2** minor changes to this file and run the tests again. (When they pass, take a moment to celebrate. Happy dances are encouraged.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note about imports in testing\n",
    "\n",
    "You will notice that we have three odd lines in the test file:\n",
    "1. `import os`\n",
    "2. `path, _ = os.path.split(os.path.abspath(__file__))`\n",
    "3. `fname = os.path.join(path,\"forestfires.csv\")`\n",
    "\n",
    "When testing our files, Travis is building a virtual machine to test our files. We need the paths to be such that Travis can build them. The first line imports the `os` package that allows us to do path manipulations. The second line extracts the path for the test file and splits it such that the file name (in this case `test_Lab1.py`) from the directory name (i.e. where the test file lives). Then the third line connects our directory with the data file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing tests\n",
    "\n",
    "To write your own tests, you need to \n",
    "1. Create a python file whose name starts with `test`\n",
    "2. `import` the python file that you are testing \n",
    "3. Write out what you expect to happen\n",
    "4. Write an assert statement to test the results of your code against your expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your turn\n",
    "\n",
    "Edit `test_importdata` to test the function `importdata` on importing your dataset from earlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts\n",
    "\n",
    "To finish up this lab, create a post to **#lab_submission** channel on slack sharing the green box of all your tests passing on travis. Your post must start with **Lab1** to get credit. \n",
    "\n",
    "If your have questions from this lab, post them to #lab_questions with the same preamble (i.e. starting with **Lab1**). If you have the same question, please use one of the emoji's to upvote the question. If you would like to answer someone's question, please use the thread function. This will tie your answer to their question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources consulted to build this lab:\n",
    "\n",
    "1. _Python for Data Analysis_ \n",
    "2. [Open Source and Reproducible MIR Research](https://bmcfee.github.io/ismir2018-oss-tutorial/)\n",
    "3. [NumPy Tutorial: Data analysis with Python](https://www.dataquest.io/blog/numpy-tutorial-python/) on Dataquest\n",
    "4. [A Quick Introduction to the “Pandas” Python Library](https://towardsdatascience.com/a-quick-introduction-to-the-pandas-python-library-f1b678f34673)\n",
    "5. [Testing Python Applications with Pytest](https://semaphoreci.com/community/tutorials/testing-python-applications-with-pytest)\n",
    "6. [Find current directory and file's directory [duplicate]](https://stackoverflow.com/questions/5137497/find-current-directory-and-files-directory)\n",
    "7. [Travis CI tests (Python 3.5, 3.6) failing on reproducibility tests](https://github.com/soft-matter/trackpy/issues/459)\n",
    "8. [Ignoring Files and Directories in Git with .gitignore](https://www.jamescoyle.net/how-to/1094-ignoring-files-in-git-with-gitignore)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
