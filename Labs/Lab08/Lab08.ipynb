{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8\n",
    "\n",
    "The next two labs concern developing the language around machine learning as well as an intuition for how these words are used. Today we will examine _prediction_ which, like classification, is a classic supervised learning task. Today's goals are:\n",
    "\n",
    "0. Articulate what prediction is (and how it is different from classification)\n",
    "1. Compare and contrast supervised and unsupervised learning from a procedural perspective\n",
    "2. Goal 2 \n",
    "3. Goal 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import block\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "In prediction, we aim to guess the output value given a set of input variables. In fact you've been doing \"supervised\" learning in many classes: linear regression. \n",
    "\n",
    "In its simpliest form, linear regression takes an input value $x$ and returns a value for $y$ using the formula $y = mx + b$, for some value of $m$ and some value of $b$. How do we find the values for $m$ and $b$? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Today's (fake) data\n",
    "\n",
    "For today, we are using another artifically created dataset based on [this example](https://paulvanderlaken.com/2017/09/27/simpsons-paradox-two-hr-examples-with-r-code/). \n",
    "\n",
    "In this example, we are consulting for _FakeData Inc_ and our job is to assist them in better understanding their employees. The CEO read an article that claimed that the higher a person's neuroticism score from the [Big Five personality](https://en.wikipedia.org/wiki/Big_Five_personality_traits) test the better their performance at work is. \n",
    "\n",
    "So the CEO has directed us to develop an algorithm that predicts a person's performance score as well as their salary based on their neuroticism score. \n",
    "\n",
    "\n",
    "Please import the data and check the shape of the data, but do **not** plot it _yet._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data \n",
    "employee = pd.read_csv(\"lab8data.csv\", delimiter = \",\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting with Two points\n",
    "\n",
    "Let's start with two people that a member of the C-suite at _FakeData Inc_ says are representative of many employees.\n",
    "\n",
    "Based on this information, find the values for $m$ and $b$ for the simple linear regression. Then plot these two points with the associated line for the create a line for these points. \n",
    "\n",
    "\n",
    "### Simple Linear regression in `sklearn`\n",
    "\n",
    "Unlike our usual procedure of building from scratch, we're going to just use the off the shelf `sklearn` implemenation for the linear regression. If you would like to better understand the theory for how and why this works, check out SDS 291 for all the details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting two employees\n",
    "inds2 = [413,791] \n",
    "justtwo = employee.iloc[inds2,:]\n",
    "input2 = justtwo[[\"neuroticism\"]].to_numpy()\n",
    "output2 = justtwo[[\"performance\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the particulars for the linear regression\n",
    "lm2 = linear_model.LinearRegression()\n",
    "\n",
    "# Fit the linear regression to the data\n",
    "model2 = lm2.fit(input2,output2)\n",
    "\n",
    "# Extract the coefficients\n",
    "m2 = lm2.coef_[0]\n",
    "b2 = lm2.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the points \n",
    "\n",
    "plt.scatter(input2, output2, s = 50)\n",
    "\n",
    "# Plotting your particular model/guessing function\n",
    "x = np.linspace(np.min(input2)-1, np.max(input2)+1, 1000)\n",
    "plt.plot(x, m2*x+b2, linestyle='dashed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have another person who has a neuroticism score of 6. Given your line, what do you expect their performance score to be? \n",
    "\n",
    "We have their perfomance score; it is Y. Does this surprise you? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting with 4 points\n",
    "\n",
    "Let's add a few more employees to our dataset and recompute the simple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting four employees\n",
    "# This time select at random using random choice\n",
    "np.random.seed(2019)\n",
    "inds4 = np.random.choice(range(1000),4)\n",
    "\n",
    "justfour = employee.iloc[inds4,:]\n",
    "\n",
    "# Set up the particulars for the linear regression\n",
    "\n",
    "\n",
    "# Fit the linear regression to the data\n",
    "\n",
    "# Extract the coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the points \n",
    "\n",
    "# Plotting your particular model/guessing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to our employee from before with a neuroticism score score of 6. Given your new line, what do you expect their performance score to be? \n",
    "\n",
    "We have their perfomance score; it is Y. Does this surprise you? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning as a procedure\n",
    "\n",
    "This sequence of steps follows the general process of **supervised** learning:\n",
    "0. Decide what task we are doing (classification or prediction) and selet the algorithm/model we want to use\n",
    "1. Using part of the available data with their given outputs, fit algorithm/model\n",
    "2. Check the model against \"new\" data (ie. the data that we did **not** use in the fitting). Use the \"truth\" to evaluate the \"goodness\" of the model. \n",
    "\n",
    "In our example, we did these as follows:\n",
    "0. Decided to do _prediction_ using _simple linear regression_\n",
    "1. Used (input score, performance score) of two points to determine the $m$ and the $b$ for the simple linear regression. \n",
    "3. Made a prediction for a new person using only their input value and checked it against the true performance score. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervision\n",
    "\n",
    "The _supervision_ occurs in the Step 1, where we use the \"answers\" to help us set where the line should be. Without these answers/output/response variables, we could place a line in an number of places, but those placements would have nearly nothing to do with our goal of _predicting_ the performance score from the _whatever_ score. In prediction, we are seeking a model that is directly tied to _answer_ and as such we must use it as part of fitting the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Supervised and Unsupervised Learning (in steps) \n",
    "How does this sequence compare or contrast to _unsupervised learning_ where there is no supervision. Below are two outlines, one for general and one for a particular $k$-means example. Be sure to note where the supervision is _lacking_ in the general case, and then pick one such example to make the list of general steps more concrete. \n",
    "\n",
    "This sequence of steps follows the general process of supervised learning:\n",
    "0. Decide what task we are doing (**option 1** or **option 2**) and selet the algorithm/model we want to use\n",
    "1. **Fitting step**\n",
    "2. **Evaluating step**\n",
    "\n",
    "In our example, we did these as follows:\n",
    "0. Specific Step 0\n",
    "1. Specific Step 1\n",
    "2. Specific Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wait here for a class discussion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Prediction Model\n",
    "\n",
    "Let's get a better sense of what we are working with. Make one plot of all the data with the neuroticism scores on one axis and the performance scores on the other axis. Place both of your models on this figure. \n",
    "\n",
    "Which is better? Why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all the data with both lines drawn on top of them\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Spot for your notes here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `predict()` to evaluate the model\n",
    "\n",
    "It would be nice if we could have a number of how \"good\" or \"bad\" our predictions are. Let's begin by seeing what predictions our first model would make by using `predict()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inds = list(set(list(range(1000))) - set(inds2))\n",
    "\n",
    "emp_test = employee.iloc[test_inds,:]\n",
    "emp_test = emp_test[[\"neuroticism\",\"performance\"]].to_numpy()\n",
    "neuro = employee[[\"neuroticism\"]].to_numpy()\n",
    "perform = employee[[\"performance\"]].to_numpy()\n",
    "predict2 = lm2.predict(neuro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of our results, create a scatter plot of the true performance scores against the predicted scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(perform, predict2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What a mess! Let's instead make a plot of the neuroticism scores with the ground truth scores and add a second plot on top of the neuroticism scores with the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(neuro, perform, marker = \"x\")\n",
    "plt.scatter(neuro, predict2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks really off! But by how much? One method is to get a notion of how off each guess is from the ground truth on average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the average of how off we are: \n",
    "\n",
    "average_mistake2 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the above process for our second model. Is this a better model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot comparing neuroticism scores with the ground truth scores \n",
    "# and the neuroticism scores with the predictions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average of how off we are\n",
    "\n",
    "average_mistake4 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training vs. Testing\n",
    "\n",
    "Notice that when we did our predictions, we only tested on the data points that we didn't use in our creation of the model. This is because the goal of supervised learning is to build a model that will work generally for your data and your context based on a few data points with \"answers\" included. The phase that uses just a few data points is called the **training** phase and the phase that compares the model's predictions to the ground truth is called the **testing** phase. \n",
    "\n",
    "In fact, we compute two kinds errors for each model: one in training and one in testing. We will talk more about training and testing later in the semester, and how specifically these training and testing computations impact the choices we make regarding data. \n",
    "\n",
    "Compute the **training error** for both models (i.e. the average difference between the predictions for the training data and the ground truth). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your 2 training computations and your two testing computations. Which model is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts\n",
    "\n",
    "To finish up this lab, create a plot for the data with three lines:\n",
    "1. The first should represent the direction and position of the first principal component\n",
    "2. The second is the linear regression (using all the data for training) with performance as the output\n",
    "3. The third is the linear regression (using all the data for training) with neuroticism as the output\n",
    "Share your plot in a post on **#lab_submission** channel on slack and share something that you find odd you about your plot. Your post must start with **Lab8** to get credit. \n",
    "\n",
    "If your have questions from this lab, post them to #lab_questions with the same preamble (i.e. starting with **Lab8**). If you have the same question, please use one of the emoji's to upvote the question. If you would like to answer someone's question, please use the thread function. This will tie your answer to their question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References consulted\n",
    "0. _Doing Data Science: Straight talk from the frontline_ by C. O'Neil & R. Schutt (2014)\n",
    "1. _Python Machine Learning_\n",
    "2. [Simple and Multiple Linear Regression in Python](https://towardsdatascience.com/simple-and-multiple-linear-regression-in-python-c928425168f9)\n",
    "3. [Simple Line Plots](https://jakevdp.github.io/PythonDataScienceHandbook/04.01-simple-line-plots.html)\n",
    "3. [Simpson’s Paradox: Two HR examples with R code.](https://paulvanderlaken.com/2017/09/27/simpsons-paradox-two-hr-examples-with-r-code/)\n",
    "4. [Simpson’s Paradox data](https://itsalocke.com/datasaurus/reference/simpsons_paradox)\n",
    "5. [Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing](https://www.autodeskresearch.com/publications/samestats) <= making dino data\n",
    "6. [Simpson’s Paradox and Interpreting Data](https://towardsdatascience.com/simpsons-paradox-and-interpreting-data-6a0443516765)\n",
    "7. [list difference](https://stackoverflow.com/questions/6486450/python-compute-list-difference)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
